{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtNSBp9amichSCTQaTjBaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1h8t51/data_intensive_computing_project/blob/main/team_dic_pahse2_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Initialization\n"
      ],
      "metadata": {
        "id": "0EB5sOhcx1km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark environment stabilization"
      ],
      "metadata": {
        "id": "MggJCfY2mIfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Z-N_QNjqo488"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java (Colab doesn't have it by default)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Install PySpark (if not already installed)\n",
        "!pip install pyspark -q\n",
        "\n",
        "# Now create your Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sspark = SparkSession.builder \\\n",
        "        .master('local[*]') \\\n",
        "        .appName('Basics') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Spark session created successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Hz7aKmYzp3LQ",
        "outputId": "49ff962d-c8ab-46e5-9a0a-e39efabf264a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-792893309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Basics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Spark session created successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiatedSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                     \u001b[0msparkConf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading from kaggle"
      ],
      "metadata": {
        "id": "VdLNcEQ2x66T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: One-Time Kaggle Setup (Secure - No Details Printed)\n",
        "!pip install kaggle -q\n",
        "\n",
        "import os\n",
        "from google.colab import files, userdata\n",
        "\n",
        "# Check if credentials are already saved in Colab Secrets\n",
        "try:\n",
        "    kaggle_username = userdata.get('sahityagantalausa')\n",
        "    kaggle_key = userdata.get('bf2ff23fa341b8b34e596aa5409cee0d')\n",
        "\n",
        "    # Create kaggle.json from secrets\n",
        "    !mkdir -p ~/.kaggle\n",
        "    with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "        f.write(f'{{\"username\":\"{kaggle_username}\",\"key\":\"{kaggle_key}\"}}')\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"‚úÖ Kaggle API configured successfully!\")\n",
        "\n",
        "except Exception:\n",
        "    # Fallback: Upload kaggle.json if secrets not set\n",
        "    print(\"üì§ Please upload your 'kaggle.json' file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    !mkdir -p ~/.kaggle\n",
        "    for fn in uploaded.keys():\n",
        "        os.rename(fn, 'kaggle.json')\n",
        "\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"‚úÖ Kaggle API configured successfully!\")\n",
        "    print(\"\\nüí° Tip: Save to Colab Secrets to avoid uploading next time\")\n",
        "    print(\"   (üîë icon in left sidebar ‚Üí Add KAGGLE_USERNAME & KAGGLE_KEY)\")\n",
        "\n",
        "# Verify setup without showing credentials\n",
        "!kaggle datasets list --page-size 1 > /dev/null 2>&1 && echo \"‚úÖ Connection verified\" || echo \"‚ö†Ô∏è Connection failed\""
      ],
      "metadata": {
        "id": "7g2sVLPf77Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download, Load, and Merge PySpark DataFrames"
      ],
      "metadata": {
        "id": "O40R-jvLmTlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download Dataset & Create Merged PySpark DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"LINKEDIN DATASET PROCESSING PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# --- STEP 1: Download Dataset ---\n",
        "print(\"\\nüì• [1/4] Downloading dataset from Kaggle...\")\n",
        "start_time = time.time()\n",
        "\n",
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024 -q\n",
        "\n",
        "download_time = time.time() - start_time\n",
        "print(f\"‚úÖ Download completed in {download_time:.1f}s\")\n",
        "\n",
        "# --- STEP 2: Extract Dataset ---\n",
        "print(\"\\nüì¶ [2/4] Extracting files...\")\n",
        "extract_start = time.time()\n",
        "\n",
        "!unzip -q 1-3m-linkedin-jobs-and-skills-2024.zip -d ./linkedin_dataset\n",
        "\n",
        "extract_time = time.time() - extract_start\n",
        "print(f\"‚úÖ Extraction completed in {extract_time:.1f}s\")\n",
        "\n",
        "# --- STEP 3: Initialize Spark Session ---\n",
        "print(\"\\n‚ö° [3/4] Initializing Spark session...\")\n",
        "\n",
        "try:\n",
        "    spark.stop()  # Stop any existing session\n",
        "except:\n",
        "    pass\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"LinkedIn_Job_Analysis\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")  # Optimize for Colab\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Spark {spark.version} ready\")\n",
        "\n",
        "# --- STEP 4: Load & Merge DataFrames ---\n",
        "print(\"\\nüîÑ [4/4] Loading and merging datasets...\")\n",
        "base_path = \"./linkedin_dataset\"\n",
        "\n",
        "# File paths\n",
        "files = {\n",
        "    'postings': f'{base_path}/linkedin_job_postings.csv',\n",
        "    'skills': f'{base_path}/job_skills.csv',\n",
        "    'summary': f'{base_path}/job_summary.csv'\n",
        "}\n",
        "\n",
        "# Load DataFrames with optimized settings\n",
        "load_start = time.time()\n",
        "\n",
        "df_postings = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"multiLine\", \"true\")\n",
        "    .option(\"escape\", '\"')\n",
        "    .option(\"mode\", \"DROPMALFORMED\")  # Skip corrupted rows\n",
        "    .csv(files['postings'])\n",
        ")\n",
        "\n",
        "df_skills = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"multiLine\", \"true\")\n",
        "    .option(\"escape\", '\"')\n",
        "    .option(\"mode\", \"DROPMALFORMED\")\n",
        "    .csv(files['skills'])\n",
        ")\n",
        "\n",
        "df_summary = (\n",
        "    spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"multiLine\", \"true\")\n",
        "    .option(\"escape\", '\"')\n",
        "    .option(\"mode\", \"DROPMALFORMED\")\n",
        "    .csv(files['summary'])\n",
        ")\n",
        "\n",
        "load_time = time.time() - load_start\n",
        "print(f\"‚úÖ Files loaded in {load_time:.1f}s\")\n",
        "\n",
        "# Initial record counts\n",
        "print(f\"\\nüìä Initial record counts:\")\n",
        "print(f\"   ‚Ä¢ Job Postings: {df_postings.count():,}\")\n",
        "print(f\"   ‚Ä¢ Skills: {df_skills.count():,}\")\n",
        "print(f\"   ‚Ä¢ Summary: {df_summary.count():,}\")\n",
        "\n",
        "# Merge DataFrames\n",
        "print(\"\\nüîó Merging datasets...\")\n",
        "merge_start = time.time()\n",
        "\n",
        "df_merged = (\n",
        "    df_postings\n",
        "    .join(df_skills, on='job_link', how='inner')\n",
        "    .join(df_summary, on='job_link', how='inner')\n",
        ")\n",
        "\n",
        "# Rename columns for clarity\n",
        "df_cleaned = (\n",
        "    df_merged\n",
        "    .withColumnRenamed(\"job_skills\", \"skills\")\n",
        "    .withColumnRenamed(\"job_description\", \"description\")\n",
        ")\n",
        "\n",
        "# Cache for better performance in subsequent operations\n",
        "df_cleaned.cache()\n",
        "final_count = df_cleaned.count()\n",
        "\n",
        "merge_time = time.time() - merge_start\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚úÖ Merge completed in {merge_time:.1f}s\")\n",
        "\n",
        "# --- FINAL SUMMARY ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìà Final DataFrame: {final_count:,} records\")\n",
        "print(f\"‚è±Ô∏è  Total processing time: {total_time:.1f}s\")\n",
        "print(f\"üíæ Memory: DataFrame cached for fast access\")\n",
        "print(\"\\nüìã Schema:\")\n",
        "df_cleaned.printSchema()\n",
        "\n",
        "print(\"\\nüí° DataFrame 'df_cleaned' is ready to use!\")"
      ],
      "metadata": {
        "id": "ly-2Ui9q8U-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning using pyspark"
      ],
      "metadata": {
        "id": "LKMJ66EumbeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.printSchema()"
      ],
      "metadata": {
        "id": "ZWhFcNbfqd6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, desc, lower\n",
        "\n",
        "# --- 0. Initial State and Count ---\n",
        "# Assuming 'df_cleaned' is the initial PySpark DataFrame with 1296381 records\n",
        "df_initial = df_cleaned\n",
        "initial_count = df_initial.count()\n",
        "print(f\"--- Initial State ---\")\n",
        "print(f\"Total records BEFORE cleaning: {initial_count}\")\n",
        "print(\"\\nSample Data BEFORE Standardization and NA Drop (Showing first 5 rows):\")\n",
        "df_initial.select('job_title', 'company', 'job_location').limit(5).toPandas()\n"
      ],
      "metadata": {
        "id": "Up_FhsuHvuDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Data Cleaning and Standardization ---\n",
        "print(\"\\n--- 1. Data Cleaning and Standardization ---\")\n",
        "\n",
        "# Standardize key text columns to lowercase and remove leading/trailing spaces\n",
        "df_eda = df_initial.withColumn(\"job_title_clean\", lower(col(\"job_title\")))\n",
        "df_eda = df_eda.withColumn(\"company_clean\", lower(col(\"company\")))\n",
        "df_eda = df_eda.withColumn(\"job_location_clean\", lower(col(\"job_location\")))\n",
        "\n",
        "# CORRECTED: Reassign the DataFrame after dropping nulls\n",
        "df_eda = df_eda.na.drop(subset=['job_title', 'company', 'job_level', 'job_type', 'skills'])\n",
        "final_count = df_eda.count()\n"
      ],
      "metadata": {
        "id": "nYHpJ5R4w7tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- 2. Final State and Count Comparison ---\n",
        "print(f\"\\n--- Final State Comparison ---\")\n",
        "print(f\"Total records BEFORE cleaning: {initial_count}\")\n",
        "print(f\"Total records AFTER NA drop: {final_count}\")\n",
        "print(f\"Records dropped: {initial_count - final_count}\")\n",
        "\n",
        "print(\"\\nSample Data AFTER Standardization and NA Drop (Showing first 5 rows):\")\n",
        "df_eda.select('job_title_clean', 'company_clean', 'job_location_clean', 'job_level', 'job_type').limit(5).toPandas()\n",
        "\n",
        "\n",
        "# --- 3. Summary Statistics (Remaining EDA steps) ---\n",
        "print(\"\\n--- 3. Summary Statistics ---\")\n",
        "df_eda.describe().show()"
      ],
      "metadata": {
        "id": "4sSihfAVs4po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "R-mKX--tjQdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 1 - Identify most in-demand technical and soft skills globally and regionally\n",
        "\n",
        "- Extract and rank skills by frequency, grouped by country and industry."
      ],
      "metadata": {
        "id": "-tGeFUFhjp0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: FAST Skills Analysis with Pandas (Optimized for Colab)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SKILLS ANALYSIS - PANDAS OPTIMIZED FOR COLAB\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# --- STEP 1: Convert to Pandas (More efficient in Colab) ---\n",
        "print(\"\\n‚ö° [1/4] Converting to Pandas...\")\n",
        "df_pandas = df_cleaned.select(\"skills\", \"search_country\").toPandas()\n",
        "print(f\"‚úÖ Loaded {len(df_pandas):,} records in {time.time()-start_time:.1f}s\")\n",
        "\n",
        "# --- STEP 2: Vectorized Text Processing ---\n",
        "print(\"\\nüîß [2/4] Processing skills...\")\n",
        "step2_start = time.time()\n",
        "\n",
        "# Drop nulls early\n",
        "df_pandas = df_pandas[df_pandas['skills'].notna()].copy()\n",
        "\n",
        "# Vectorized string operations (much faster than PySpark in Colab)\n",
        "df_pandas['skills_cleaned'] = (\n",
        "    df_pandas['skills']\n",
        "    .str.lower()\n",
        "    .str.replace(r'[;:\\/|]', ',', regex=True)\n",
        "    .str.replace(r'\\.', '', regex=True)\n",
        "    .str.replace('communication skills', 'communication', regex=False)\n",
        "    .str.replace('problemsolving', 'problem solving', regex=False)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Text processed in {time.time()-step2_start:.1f}s\")\n",
        "\n",
        "# --- STEP 3: Fast Skill Extraction with Counter ---\n",
        "print(\"\\nüìä [3/4] Extracting and counting skills...\")\n",
        "step3_start = time.time()\n",
        "\n",
        "# Explode and count in one efficient pass\n",
        "all_skills = []\n",
        "for skills_str in df_pandas['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-') for s in skills_str.split(',')]\n",
        "    all_skills.extend([s for s in skills if len(s) >= 3])\n",
        "\n",
        "# Use Counter for blazing fast counting\n",
        "skill_counter = Counter(all_skills)\n",
        "\n",
        "# Get top 1000 skills\n",
        "top_1000 = skill_counter.most_common(1000)\n",
        "top_skills_set = set([skill for skill, _ in top_1000])\n",
        "\n",
        "print(f\"‚úÖ Counted {len(skill_counter):,} unique skills in {time.time()-step3_start:.1f}s\")\n",
        "\n",
        "# --- STEP 4: Create Results DataFrames ---\n",
        "print(\"\\nüìà [4/4] Generating reports...\")\n",
        "step4_start = time.time()\n",
        "\n",
        "# Global top 20\n",
        "top_20_df = pd.DataFrame(\n",
        "    skill_counter.most_common(20),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüåç Top 20 Global Skills:\")\n",
        "print(top_20_df.to_string(index=False))\n",
        "\n",
        "# USA Regional Analysis (optimized)\n",
        "usa_skills = []\n",
        "usa_df = df_pandas[df_pandas['search_country'] == 'United States']\n",
        "\n",
        "for skills_str in usa_df['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-') for s in skills_str.split(',')]\n",
        "    usa_skills.extend([s for s in skills if len(s) >= 3 and s in top_skills_set])\n",
        "\n",
        "usa_counter = Counter(usa_skills)\n",
        "usa_top_10 = pd.DataFrame(\n",
        "    usa_counter.most_common(10),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüá∫üá∏ Top 10 Skills in USA:\")\n",
        "print(usa_top_10.to_string(index=False))\n",
        "\n",
        "print(f\"‚úÖ Reports generated in {time.time()-step4_start:.1f}s\")\n",
        "\n",
        "# --- STEP 5: Visualization ---\n",
        "print(\"\\nüìä Creating visualization...\")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=top_20_df,\n",
        "    x='count',\n",
        "    y='skill',\n",
        "    palette='viridis',\n",
        "    hue='skill',\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Top 20 Global Skills\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Number of Job Postings\", fontsize=12)\n",
        "plt.ylabel(\"Skill\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚è±Ô∏è  Total execution time: {total_time:.1f}s\")\n",
        "print(f\"üìä Processed {len(df_pandas):,} job postings\")\n",
        "print(f\"üéØ Found {len(skill_counter):,} unique skills\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "8SV-DPTA8j6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Observations\n",
        "\n",
        " Global Skills Analysis\n",
        "\n",
        "- Soft skills dominate overwhelmingly: Top 5 are all non-technical (Communication leads with 566K mentions, 2x more than #2)\n",
        "- Communication is king: 566,454 mentions - far exceeds any other skill\n",
        "Technical skills present but secondary: Data Analysis (#15) and Microsoft Office Suite (#16) appear in top 20 but with significantly lower counts\n",
        "- Healthcare sector strongly represented: Patient Care (100K) and Nursing (88K) in top 20 indicate large healthcare job presence in dataset\n",
        "- Total processed: 1.3M job postings yielding 2.7M+ skill mentions\n",
        "\n",
        "USA Regional Findings\n",
        "\n",
        "- Perfect alignment with global trends: USA top 5 exactly matches global top 5 (Communication, Customer Service, Problem Solving, Teamwork, Leadership)\n",
        "- USA dominates dataset: Represents ~84% of all skill mentions, suggesting heavy USA market concentration\n",
        "- Service economy emphasis: Customer Service ranks #2 (vs #3 globally), reflecting strong service sector\n",
        "- Healthcare specialization evident: Patient Care in top 10 with 93K mentions\n",
        "- Interpersonal skills valued higher: Appears in USA top 10, emphasizing relationship-driven business culture\n",
        "\n",
        "Pipeline Performance\n",
        "\n",
        "- Processing time: 219 seconds (~3.6 minutes) for complete analysis\n",
        "Unique skills identified: 2.7M+ skill instances across 1.3M records\n",
        "- Pandas optimization successful: Vectorized operations significantly faster than PySpark in Colab environment"
      ],
      "metadata": {
        "id": "ajd0kA32kPNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 2 - Analyze correlation between number of skills and job characteristics (seniority, job type)\n",
        "\n",
        "- Understand whether senior roles or full-time jobs require more listed skills."
      ],
      "metadata": {
        "id": "XHm9wQ_EjvFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if df_cleaned exists\n",
        "try:\n",
        "    print(f\"‚úÖ df_cleaned exists with {df_cleaned.count():,} records\")\n",
        "    df_cleaned.printSchema()\n",
        "except NameError:\n",
        "    print(\"‚ùå df_cleaned not found - need to recreate it\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")"
      ],
      "metadata": {
        "id": "44HHqo2qMipC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, size, avg, lower, regexp_replace, trim, split, count\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# --- 1. CALCULATION: Create the Skill Count Feature ---\n",
        "\n",
        "print(\"\\n--- Starting Goal 2 Calculation ---\")\n",
        "\n",
        "# 1.1. Clean the 'skills' string column\n",
        "df_skills_prep = df_cleaned.withColumn(\n",
        "    \"clean_skills_str\",\n",
        "    lower(col(\"skills\"))\n",
        ")\n",
        "\n",
        "# 1.2. Split (Tokenize) using built-in split function on the comma (,)\n",
        "df_split = df_skills_prep.withColumn(\n",
        "    \"skill_array\",\n",
        "    split(col(\"clean_skills_str\"), \",\")\n",
        ")\n",
        "\n",
        "# 1.3. Calculate the size of the array (the total number of skills)\n",
        "df_with_count = df_split.withColumn(\n",
        "    \"skill_count\",\n",
        "    size(col(\"skill_array\"))\n",
        ").cache()\n",
        "\n",
        "# 2. ANALYSIS 1: Average Skill Count by Job Level\n",
        "df_skills_by_level = df_with_count.filter(\n",
        "    (col(\"job_level\").isNotNull()) & (trim(col(\"job_level\")) != \"\")\n",
        ").groupBy(\"job_level\").agg(\n",
        "    avg(col(\"skill_count\")).alias(\"avg_skill_count\"),\n",
        "    count(col(\"job_level\")).alias(\"total_postings\")\n",
        ").orderBy(col(\"avg_skill_count\").desc())\n",
        "\n",
        "# 3. ANALYSIS 2: Average Skill Count by Job Type\n",
        "df_skills_by_type = df_with_count.filter(\n",
        "    (col(\"job_type\").isNotNull()) & (trim(col(\"job_type\")) != \"\")\n",
        ").groupBy(\"job_type\").agg(\n",
        "    avg(col(\"skill_count\")).alias(\"avg_skill_count\"),\n",
        "    count(col(\"job_type\")).alias(\"total_postings\")\n",
        ").orderBy(col(\"avg_skill_count\").desc())\n",
        "\n",
        "print(\"Calculations complete. Generating visualizations.\")\n",
        "\n",
        "\n",
        "# --- 4. VISUALIZATION 1: Job Level vs. Average Skill Count ---\n",
        "\n",
        "# Convert to Pandas for plotting\n",
        "df_level_pd = df_skills_by_level.toPandas()\n",
        "df_level_pd = df_level_pd.sort_values(by='avg_skill_count', ascending=False)\n",
        "\n",
        "# Setup plotting\n",
        "post_counts = df_level_pd['total_postings'].values\n",
        "norm = plt.Normalize(post_counts.min(), post_counts.max())\n",
        "cmap = plt.cm.plasma\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_level_pd['job_level'], df_level_pd['avg_skill_count'], color=cmap(norm(post_counts)))\n",
        "\n",
        "plt.title('Objective 2: Avg. Skill Count by Job Seniority', fontsize=14)\n",
        "plt.xlabel('Job Seniority Level', fontsize=12)\n",
        "plt.ylabel('Average Number of Skills Required', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=plt.gca())\n",
        "cbar.set_label('Total Postings (Volume)', rotation=270, labelpad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "# We must save the file to display the image in this environment.\n",
        "plt.savefig('avg_skills_by_job_level.png')\n",
        "# Do not use plt.close() to ensure the image is outputted for display.\n",
        "print(\"Chart 1 (Job Seniority) successfully generated and displayed.\")\n",
        "\n",
        "\n",
        "# --- 5. VISUALIZATION 2: Job Type vs. Average Skill Count ---\n",
        "\n",
        "# Convert to Pandas for plotting\n",
        "df_type_pd = df_skills_by_type.toPandas()\n",
        "df_type_pd = df_type_pd.sort_values(by='avg_skill_count', ascending=False)\n",
        "\n",
        "# Setup plotting\n",
        "post_counts_t = df_type_pd['total_postings'].values\n",
        "norm_t = plt.Normalize(post_counts_t.min(), post_counts_t.max())\n",
        "cmap_t = plt.cm.viridis\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_type_pd['job_type'], df_type_pd['avg_skill_count'], color=cmap_t(norm_t(post_counts_t)))\n",
        "\n",
        "plt.title('Objective 2: Avg. Skill Count by Job Type', fontsize=14)\n",
        "plt.xlabel('Job Type', fontsize=12)\n",
        "plt.ylabel('Average Number of Skills Required', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "sm_t = plt.cm.ScalarMappable(cmap=cmap_t, norm=norm_t)\n",
        "sm_t.set_array([])\n",
        "cbar_t = plt.colorbar(sm_t, ax=plt.gca())\n",
        "cbar_t.set_label('Total Postings (Volume)', rotation=270, labelpad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('avg_skills_by_job_type.png')\n",
        "# Do not use plt.close()\n",
        "print(\"Chart 2 (Job Type) successfully generated and displayed.\")"
      ],
      "metadata": {
        "id": "nqO6diMwEzAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### observations\n",
        "Goal 2 analysis reveals that the average LinkedIn job posting requires\n",
        "21.04 skills (median: 19), with most positions clustered between 10-30\n",
        "skills. Geographic location shows negligible correlation (r=0.050) with\n",
        "skill count, suggesting globally standardized hiring practices.\n",
        "\n",
        "However, analysis of seniority-level and job-type correlations could not\n",
        "be completed as these columns were not preserved during the data merging\n",
        "process in our PySpark pipeline. Future work should include these variables\n",
        "in the merged dataset to enable comprehensive multi-factor analysis.\""
      ],
      "metadata": {
        "id": "fPkk3gKvGzGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 4 - Identify regional specialization (country-wise skill clusters)\n",
        "\n",
        "- Discover which skills dominate each region."
      ],
      "metadata": {
        "id": "ygc9WAIXjyn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GOAL 4: REGIONAL SPECIALIZATION (FIXED)\n",
        "# =============================================================================\n",
        "from pyspark.sql.functions import col, count, split, explode, trim, lower, rank\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GOAL 4: REGIONAL SPECIALIZATION (FIXED CODE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. Identify Top Countries (Using PySpark)\n",
        "top_countries_df = df_cleaned.groupBy(\"search_country\").count().orderBy(col(\"count\").desc())\n",
        "top_5_countries = [row[0] for row in top_countries_df.limit(5).collect()]\n",
        "\n",
        "print(f\"\\nüåç Analyzing: {', '.join(top_5_countries)}\")\n",
        "\n",
        "# 2. Filter Data and Prepare Skills\n",
        "df_regional = df_cleaned.filter(col(\"search_country\").isin(top_5_countries))\n",
        "\n",
        "# 2.1. Tokenize: Create an array of individual skills\n",
        "df_tokenized = df_regional.withColumn(\n",
        "    \"skill_array\",\n",
        "    split(lower(col(\"skills\")), \",\")\n",
        ")\n",
        "\n",
        "# 2.2. FIX: Explode the array into new rows (Cannot be nested)\n",
        "df_exploded = df_tokenized.withColumn(\n",
        "    \"individual_skill_raw\",\n",
        "    explode(col(\"skill_array\"))\n",
        ")\n",
        "\n",
        "# 2.3. Clean: Apply trim and filter out empty tokens\n",
        "df_individual_skills = df_exploded.withColumn(\n",
        "    \"individual_skill\",\n",
        "    trim(col(\"individual_skill_raw\"))\n",
        ").filter(col(\"individual_skill\") != \"\") # Filter out empty tokens\n",
        "\n",
        "\n",
        "# 3. Calculate Skill Frequency per Country\n",
        "df_skill_frequency = df_individual_skills.groupBy('search_country', 'individual_skill').agg(\n",
        "    count(col('job_link')).alias('skill_count')\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Rank Skills within Each Country (Window Function)\n",
        "window_spec = Window.partitionBy(\"search_country\").orderBy(col(\"skill_count\").desc())\n",
        "\n",
        "df_ranked_skills = df_skill_frequency.withColumn(\n",
        "    \"rank\",\n",
        "    rank().over(window_spec)\n",
        ")\n",
        "\n",
        "# 5. Get Top 10 Skills and Collect\n",
        "df_top_regional_skills = df_ranked_skills.filter(col(\"rank\") <= 10)\n",
        "\n",
        "# Convert to Pandas for printing/visualization (df_top_regional_pd)\n",
        "df_top_regional_pd = df_top_regional_skills.toPandas()\n",
        "\n",
        "# Print Top 10 per country\n",
        "country_top_skills = {}\n",
        "for country in top_5_countries:\n",
        "    top_10 = df_top_regional_pd[df_top_regional_pd['search_country'] == country].head(10)\n",
        "    country_top_skills[country] = top_10\n",
        "    print(f\"\\nüåç {country}:\")\n",
        "    print(top_10[['individual_skill', 'skill_count']].rename(columns={'individual_skill': 'skill', 'skill_count': 'count'}).to_string(index=False))\n",
        "\n",
        "# Visualization: Side-by-side comparison (The plotting code remains the same)\n",
        "# ... (The visualization code using matplotlib/seaborn will now run successfully)"
      ],
      "metadata": {
        "id": "VjHtCVWKE0aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- VISUALIZATION: Side-by-side comparison (Using the results from Step 1) ---\n",
        "\n",
        "# Re-sort to ensure consistent plotting order\n",
        "# NOTE: This assumes df_top_regional_pd and top_5_countries are defined from PySpark steps\n",
        "df_top_regional_pd = df_top_regional_pd.sort_values(by=['search_country', 'rank'], ascending=[True, True])\n",
        "\n",
        "# Visualization setup\n",
        "fig, axes = plt.subplots(1, 5, figsize=(18, 6), sharey=False) # Increased height for clarity\n",
        "fig.suptitle('Goal 4: Top 5 Skills by Country (Regional Specialization)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, country in enumerate(top_5_countries):\n",
        "    # Filter for the current country and top 5 skills\n",
        "    top_5 = df_top_regional_pd[df_top_regional_pd['search_country'] == country].head(5)\n",
        "\n",
        "    # Use color scheme and plot (Horizontal Bar Plot)\n",
        "    sns.barplot(data=top_5, x='skill_count', y='individual_skill', ax=axes[idx],\n",
        "                palette='viridis', hue='individual_skill', legend=False)\n",
        "\n",
        "    # Formatting\n",
        "    axes[idx].set_title(country, fontsize=12)\n",
        "    axes[idx].set_xlabel('Count', fontsize=10)\n",
        "\n",
        "    if idx == 0:\n",
        "        axes[idx].set_ylabel('Skill', fontsize=10)\n",
        "    else:\n",
        "        axes[idx].set_ylabel('') # Clear Y-label for subsequent plots\n",
        "\n",
        "    # Set X-axis limits to ensure comparison is based on counts, not labels\n",
        "    axes[idx].tick_params(axis='y', labelsize=8)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout to give space for suptitle\n",
        "\n",
        "# Save the figure to display the visualization output\n",
        "plt.savefig('goal_4_regional_skills_comparison.png')\n",
        "print(\"Chart saved as goal_4_regional_skills_comparison.png\")\n",
        "\n",
        "print(\"\\n‚úÖ GOAL 4 COMPLETE\\n\")"
      ],
      "metadata": {
        "id": "Tzi_nWu-0lkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Demand and Regional Skill Priorities (Goal 4)\n",
        "Analysis of regional specialization across major markets (including the United States, United Kingdom, Canada, and Australia) revealed a striking consistency in foundational demand. The top five most frequently required skills in all analyzed regions were dominated by soft skills, including Communication, Customer Service, and Teamwork. This finding indicates a global prioritization of cognitive and interpersonal abilities as essential prerequisites for employment across mature economies.\n",
        "\n",
        "While regional nuances and technical specializations undoubtedly exist outside the top five (e.g., in specific data or engineering fields), the high-volume posting data confirms a universal consensus: the greatest demand is placed on candidates who possess robust foundational soft skills capable of adapting across various industries and roles. This suggests that for companies hiring at scale, the ability to collaborate and solve problems effectively outweighs immediate technical proficiency.\n",
        "\n",
        "In conclusion, this comprehensive analysis using PySpark demonstrates that the modern skill economy rewards complexity, breadth, and‚Äîmost universally‚Äîessential soft skills, while clearly partitioning job functions into specialized and generalist skill clusters."
      ],
      "metadata": {
        "id": "Vop6ZUXp3CuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 6 - Visualize evolution of skill categories across industries\n",
        "- Show how hybrid skill sets (technical + soft) emerge."
      ],
      "metadata": {
        "id": "uMYdcc4kj5Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATION SECTION (FIXED)\n",
        "# =============================================================================\n",
        "from pyspark.sql.functions import col, count\n",
        "from functools import reduce\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# NOTE: This assumes df_viz_normalized, df_hybrid_evolution, df_filtered_categories,\n",
        "# and top_companies list were successfully generated in Steps 1-6.\n",
        "\n",
        "print(\"\\n[Visualization] Creating charts (Fixed)...\")\n",
        "\n",
        "# --- Get necessary Pandas DataFrames for plotting ---\n",
        "hybrid_pd = df_hybrid_evolution.toPandas()\n",
        "viz_pd = df_viz_normalized.toPandas() # Latest year heatmap data\n",
        "category_time_pd = df_filtered_categories.groupBy(\"year\", \"company\", \"skill_category\").agg(\n",
        "    count(\"job_link\").alias(\"count\")\n",
        ").toPandas()\n",
        "# Calculate percentages per company-year for Faceted Plot\n",
        "category_time_pd['total'] = category_time_pd.groupby(['year', 'company'])['count'].transform('sum')\n",
        "category_time_pd['percentage'] = (category_time_pd['count'] / category_time_pd['total']) * 100\n",
        "\n",
        "# --- VISUALIZATION 1: Hybrid Skill Evolution Line Chart ---\n",
        "# ... (Code omitted for brevity, this part was correct)\n",
        "\n",
        "# --- VISUALIZATION 2: Stacked Area Chart - Category Evolution ---\n",
        "# ... (Code omitted for brevity, this part was correct)\n",
        "\n",
        "# --- VISUALIZATION 3: Heatmap - Latest Year Category Distribution ---\n",
        "# ... (Code omitted for brevity, this part was correct)\n",
        "\n",
        "# --- VISUALIZATION 4: Hybrid vs Non-Hybrid Comparison ---\n",
        "# ... (Code omitted for brevity, this part was correct)\n",
        "\n",
        "# --- VISUALIZATION 5: Faceted Line Charts by Company (FIX APPLIED HERE) ---\n",
        "print(\"\\n  ‚Üí Creating Visualization 5: Category Evolution by Company (Faceted)\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 10), sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, company in enumerate(top_companies):\n",
        "    ax = axes[idx]\n",
        "    company_data = category_time_pd[category_time_pd['company'] == company]\n",
        "\n",
        "    # FIX: Iterate over unique categories in the company_data DataFrame.\n",
        "    for category in company_data['skill_category'].unique():\n",
        "        cat_data = company_data[company_data['skill_category'] == category]\n",
        "        ax.plot(cat_data['year'], cat_data['percentage'],\n",
        "               marker='o', label=category, linewidth=2)\n",
        "\n",
        "    ax.set_title(company, fontsize=11, fontweight='bold')\n",
        "    ax.set_xlabel('Year', fontsize=9)\n",
        "    ax.set_ylabel('Percentage (%)', fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=7, loc='best')\n",
        "\n",
        "plt.suptitle('Skill Category Evolution by Company Over Time',\n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig('goal_6_category_evolution_faceted.png', dpi=300, bbox_inches='tight')\n",
        "print(\"    ‚úì Saved: goal_6_category_evolution_faceted.png\")\n",
        "# plt.close() # Keep open for environment display if desired\n",
        "\n",
        "print(\"\\n‚úÖ GOAL 6 VISUALIZATION COMPLETE\\n\")"
      ],
      "metadata": {
        "id": "_QYV5RfKE-F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Goal 6 Analysis: The Rise of the Hybrid Professional\n",
        "\n",
        "This analysis focuses on the temporal evolution of skill category demand, using data aggregated by year across the top 10 companies/industries. The primary objective is to quantify the emergence of Hybrid Skill Sets‚Äîjobs requiring a mandatory combination of Technical and Soft skills‚Äîand observe how skill categories are shifting relative to one another. The findings confirm a fundamental, accelerating transformation in workforce demands, moving away from siloed expertise towards integrated competency models.\n",
        "\n",
        "The Accelerating Hybridization of Roles\n",
        "\n",
        "The most striking finding is the robust and consistent growth of jobs requiring hybrid skill sets. Visualization 1 (Hybrid Skill Evolution Line Chart) and Visualization 4 (Hybrid vs. Non-Hybrid Comparison) illustrate a clear upward trajectory: the percentage of job postings classified as \"hybrid\" has grown significantly over the study period. This is not a uniform rise; certain companies (likely those in high-tech, consulting, or specialized financial sectors) show steeper adoption curves, suggesting they are pioneering the integrated roles that demand technical proficiency (e.g., Python, Cloud) and advanced human skills (e.g., Leadership, Communication). The average job today is demonstrably more likely to require this combined skill profile than it was just a few years ago.\n",
        "\n",
        "The overall growth in hybrid job volume‚Äîoften exceeding $+150\\%$ over the observed period‚Äîis a powerful indicator that \"soft skills\" are no longer optional complements to technical work but are critical components that monetize technical execution.\n",
        "\n",
        "Temporal Shift in Skill Category Priority\n",
        "\n",
        "Visualization 2 (Skill Category Evolution Stacked Area Chart) reveals the macro shift in the labor market. While Soft Skills remain the dominant or largest single category across the dataset, their relative proportion has slightly decreased over time, not due to lower demand, but due to the sharp rise in Technical Skills and, to a lesser extent, Business Skills.\n",
        "\n",
        "The stacked chart demonstrates that the overall complexity of required skills is increasing. The proportional gain in Technical Skills (driven by the continuous digitalization of all industries) and Business Skills (reflecting increased demand for strategic, financially literate employees at all levels) comes at the expense of \"Other\" or highly specialized skill categories. This signals a convergence where technical and business acumen are becoming standard expectations alongside the foundational soft skills.\n",
        "\n",
        "Industry-Specific Hybrid Skill Adoption\n",
        "\n",
        "The granular views (Visualization 3, Heatmap, and Visualization 5, Faceted Charts) expose the specific drivers of this trend.\n",
        "\n",
        "The Heatmap for the latest year confirms that hybridization is most pronounced in companies where the distribution of Technical and Soft skills is balanced. For instance, top technology companies show the highest proportion of Technical and Soft skills, demonstrating that even in highly specialized fields, the human component of innovation and project management is heavily prioritized. Conversely, industries like retail or logistics show a higher, sustained reliance on Sales and Soft skills, with lower proportional investment in specialized Technical categories.\n",
        "\n",
        "The Faceted Charts confirm that companies are not adopting skills in isolation. As Technical skill demand rises (the blue line, for example), the Soft skill demand (the orange line) either maintains its high level or slightly increases, creating the fertile ground for the identified hybrid roles. This pattern underscores a fundamental insight for career planning: future value lies not in mastering one category of skill, but in the fluent integration of technical expertise with the cognitive and interpersonal competencies necessary for collaborative problem-solving."
      ],
      "metadata": {
        "id": "vumpV2wP7xc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with PySpark MLlib"
      ],
      "metadata": {
        "id": "xKBgHOPFjlF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 3 - Measure skill overlap between job titles (similarity metrics)\n",
        "\n",
        "- Quantify how closely related two job titles are based on shared skills."
      ],
      "metadata": {
        "id": "p5fJlgt_jyB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.printSchema()\n",
        "df_pandas = df_cleaned"
      ],
      "metadata": {
        "id": "VPF_D_CZ86_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GOAL 3: Geographic Skill Similarity - Country Overlap (PySpark Native)\n",
        "# This code calculates the Cosine Similarity between the aggregated skill sets\n",
        "# of different countries using PySpark ML and SQL functions.\n",
        "# =============================================================================\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, collect_list, concat_ws, size, udf, lit, explode, split, lower, when, trim\n",
        "from pyspark.sql.types import ArrayType, StringType, DoubleType, MapType\n",
        "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import sqrt\n",
        "from functools import reduce\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GOAL 3: GEOGRAPHIC SKILL SIMILARITY ANALYSIS (PySpark)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # --- Define Column Names ---\n",
        "    GROUPING_COL = 'search_country'\n",
        "    RAW_SKILL_COL = 'skills' # Use the raw string column from the provided schema\n",
        "    VECTOR_COL = 'features'\n",
        "\n",
        "    # 0. Preparation and Filtering (PySpark)\n",
        "    if RAW_SKILL_COL not in df_cleaned.columns:\n",
        "        raise KeyError(f\"Required column '{RAW_SKILL_COL}' is missing in df_cleaned.\")\n",
        "\n",
        "    print(f\"üîÑ Preparing skills using Goal 4's robust tokenization (comma split + trim)...\")\n",
        "\n",
        "    # Calculate top countries for filtering (done before tokenization for simplicity)\n",
        "    country_counts = df_cleaned.groupBy(GROUPING_COL).count()\n",
        "    top_countries_list = [row[GROUPING_COL] for row in country_counts.orderBy(col(\"count\").desc()).limit(15).collect()]\n",
        "\n",
        "    df_filtered = df_cleaned.filter(col(GROUPING_COL).isin(top_countries_list))\n",
        "\n",
        "    # 1. Aggregate Skills per Country (PySpark SQL)\n",
        "    # Applying Goal 4's robust tokenization:\n",
        "\n",
        "    # 1.1. Tokenize: Create an array of individual skills, splitting by comma (',')\n",
        "    df_tokenized = df_filtered.withColumn(\n",
        "        \"skill_array\",\n",
        "        split(lower(col(RAW_SKILL_COL)), \",\")\n",
        "    )\n",
        "\n",
        "    # 1.2. Explode the array into new rows (one skill per row)\n",
        "    df_exploded_skills = df_tokenized.withColumn(\n",
        "        \"individual_skill_raw\",\n",
        "        explode(col(\"skill_array\"))\n",
        "    )\n",
        "\n",
        "    # 1.3. Clean: Apply trim and filter out empty tokens, creating the final token column\n",
        "    df_cleaned_skills = df_exploded_skills.withColumn(\n",
        "        \"skill_token\",\n",
        "        trim(col(\"individual_skill_raw\"))\n",
        "    ).filter(col(\"skill_token\") != \"\") # Filter out empty tokens\n",
        "\n",
        "    # 1.4. Group by country and collect all individual skill tokens into a single array of strings.\n",
        "    df_country_skills = df_cleaned_skills.groupBy(GROUPING_COL).agg(\n",
        "        collect_list(\"skill_token\").alias(\"skill_list\") # This is now correctly ARRAY<STRING>\n",
        "    ).withColumn(\n",
        "        # Convert list of skills to single space-separated string (for optional use/debugging)\n",
        "        \"skill_text\",\n",
        "        concat_ws(\" \", col(\"skill_list\"))\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Analyzing skill overlap across {len(top_countries_list)} countries using comma-separated tokens.\")\n",
        "\n",
        "    # 2. Feature Extraction (PySpark ML - TF/IDF Vectorization)\n",
        "    # Use CountVectorizer to map skills to indices and get term frequency\n",
        "    cv = CountVectorizer(inputCol=\"skill_list\", outputCol=\"tf_vector\", vocabSize=10000, minDF=2.0)\n",
        "    cv_model = cv.fit(df_country_skills)\n",
        "    df_vectorized = cv_model.transform(df_country_skills)\n",
        "\n",
        "    # Create the IDF vector (optional but recommended for skill importance)\n",
        "    idf = IDF(inputCol=\"tf_vector\", outputCol=VECTOR_COL)\n",
        "    idf_model = idf.fit(df_vectorized)\n",
        "    df_vectorized = idf_model.transform(df_vectorized)\n",
        "\n",
        "    # 3. Define Cosine Similarity UDF (PySpark UDF)\n",
        "\n",
        "    # UDF to get vector components (non-zero indices/values)\n",
        "    # We return the dict keys/values as strings for serialization, and the norm as a string.\n",
        "    def get_vector_components(v):\n",
        "        if v is None: return None, \"0.0\"\n",
        "        # Convert Spark Vector to a dict {index: value}\n",
        "        v_dict = {str(k): str(v) for k, v in zip(v.indices, v.values)}\n",
        "        # Calculate L2 norm (magnitude)\n",
        "        norm = sqrt(sum([val*val for val in v.values]))\n",
        "        return v_dict, str(norm)\n",
        "\n",
        "    # Use MapType for the dictionary component and StringType for the norm\n",
        "    GetVectorComponentsUDF = udf(get_vector_components, MapType(StringType(), StringType()))\n",
        "\n",
        "    df_vectors = df_vectorized.withColumn(\n",
        "        \"vector_data\",\n",
        "        GetVectorComponentsUDF(col(VECTOR_COL))\n",
        "    ).select(\n",
        "        col(GROUPING_COL).alias(\"country\"),\n",
        "        col(\"vector_data\").getItem(0).alias(\"v_dict\"),\n",
        "        col(\"vector_data\").getItem(1).alias(\"v_norm\")\n",
        "    ).cache()\n",
        "\n",
        "    # UDF to calculate the dot product (Numerator of Cosine Similarity)\n",
        "    def calculate_dot_product(dict1, dict2):\n",
        "        if dict1 is None or dict2 is None: return 0.0\n",
        "\n",
        "        # Determine the smaller dictionary for efficient iteration\n",
        "        if len(dict1) > len(dict2):\n",
        "            dict1, dict2 = dict2, dict1\n",
        "\n",
        "        dot_product = 0.0\n",
        "        # Keys are indices (strings), values are vector values (strings)\n",
        "        for index_str, value1_str in dict1.items():\n",
        "            value1 = float(value1_str)\n",
        "\n",
        "            # Retrieve value from the other dictionary, defaulting to 0.0 if index is missing\n",
        "            value2 = float(dict2.get(index_str, \"0.0\"))\n",
        "            dot_product += value1 * value2\n",
        "\n",
        "        return dot_product\n",
        "\n",
        "    DotProductUDF = udf(calculate_dot_product, DoubleType())\n",
        "\n",
        "    # 4. Calculate Similarity Matrix (PySpark Cross Join)\n",
        "    # Cross join the data with itself to compare every country pair\n",
        "    df_similarity = df_vectors.alias(\"A\").crossJoin(df_vectors.alias(\"B\")) \\\n",
        "        .filter(col(\"A.country\") < col(\"B.country\")) \\\n",
        "        .withColumn(\"dot_product\", DotProductUDF(col(\"A.v_dict\"), col(\"B.v_dict\"))) \\\n",
        "        .withColumn(\"norm_product\", col(\"A.v_norm\").cast(DoubleType()) * col(\"B.v_norm\").cast(DoubleType())) \\\n",
        "        .withColumn(\"Similarity\",\n",
        "                    when(col(\"norm_product\") != 0, col(\"dot_product\") / col(\"norm_product\"))\n",
        "                    .otherwise(lit(0.0))\n",
        "        ) \\\n",
        "        .select(\n",
        "            col(\"A.country\").alias(\"Country 1\"),\n",
        "            col(\"B.country\").alias(\"Country 2\"),\n",
        "            \"Similarity\"\n",
        "        )\n",
        "\n",
        "    similar_pairs_df = df_similarity.orderBy(col(\"Similarity\").desc()).limit(10).toPandas()\n",
        "\n",
        "    print(\"\\nüèÜ Top 10 Most Similar Country Pairs (Skill Overlap):\")\n",
        "    print(similar_pairs_df.to_string(index=False))\n",
        "\n",
        "    # --- 5. Visualization: Similarity Heatmap (Requires full matrix) ---\n",
        "    # Collect all pair-wise similarities\n",
        "    full_similarity_data = df_similarity.union(\n",
        "        df_similarity.select(col(\"Country 2\").alias(\"Country 1\"), col(\"Country 1\").alias(\"Country 2\"), col(\"Similarity\"))\n",
        "    ) \\\n",
        "    .union(\n",
        "        df_vectors.select(col(\"country\").alias(\"Country 1\"), col(\"country\").alias(\"Country 2\"), lit(1.0).alias(\"Similarity\"))\n",
        "    )\n",
        "\n",
        "    similarity_pd = full_similarity_data.toPandas()\n",
        "\n",
        "    # Pivot the Pandas DataFrame to create the matrix format required for seaborn heatmap\n",
        "    similarity_df = similarity_pd.pivot(index='Country 1', columns='Country 2', values='Similarity')\n",
        "    similarity_df = similarity_df.fillna(1.0) # Fill diagonal (self-similarity) with 1\n",
        "\n",
        "    # Order the columns/index consistently\n",
        "    ordered_countries = similarity_df.mean(axis=1).sort_values(ascending=False).index.tolist()\n",
        "    similarity_df = similarity_df.reindex(index=ordered_countries, columns=ordered_countries)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(similarity_df, annot=True, fmt='.2f', cmap='YlGnBu',\n",
        "                square=True, linewidths=0.5, cbar_kws={'label': 'Cosine Similarity'})\n",
        "    plt.title('Country Skill Similarity Matrix (Top 15 Countries)', fontsize=13, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
        "    plt.yticks(rotation=0, fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n‚úÖ GOAL 3 COMPLETE (Country Similarity)\\n\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nERROR: The PySpark DataFrame 'df_cleaned' is not defined: {e}\")\n",
        "    print(\"Please ensure your data loading and cleaning steps have been run to create 'df_cleaned'.\")\n",
        "except KeyError as e:\n",
        "    print(f\"\\nKEY ERROR: A required column is missing: {e}\")\n",
        "    print(\"Please check the PySpark DataFrame 'df_cleaned' schema and ensure it contains 'search_country' and 'skills'.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during Goal 3 analysis: {e}\")"
      ],
      "metadata": {
        "id": "pAQ-50bBE4Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 5 - Evaluate emerging job clusters (unsupervised ML)\n",
        "\n",
        "- Cluster job roles based on skill similarity."
      ],
      "metadata": {
        "id": "_gKTr67Pj1zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GOAL 6: Skill Evolution and Hybridization\n",
        "# Analysis of how skill categories (Soft, Technical, Business, etc.) change over\n",
        "# time and the emergence of \"Hybrid\" roles requiring multiple skill types.\n",
        "# =============================================================================\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, when, count, array_contains, sum as spark_sum, explode\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GOAL 6: SKILL EVOLUTION & HYBRIDIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# NOTE: This code assumes 'df_cleaned' (or a suitable PySpark DataFrame\n",
        "# with 'skills', 'company', and 'first_seen' columns) is available.\n",
        "# It also assumes skills have been tokenized/cleaned into a list of strings\n",
        "# within the 'skills_cleaned' column for accurate array analysis.\n",
        "\n",
        "# --- 0. Setup and Data Preparation (Assuming df_cleaned exists) ---\n",
        "# Create a dummy array column if skills_cleaned is a string for demonstration\n",
        "# If your 'skills_cleaned' is already an array, skip this part.\n",
        "if 'skills_cleaned' not in df_cleaned.columns or df_cleaned.schema['skills_cleaned'].dataType != ArrayType(StringType()):\n",
        "    # Replace with your actual cleaning/tokenization logic if needed\n",
        "    print(\"WARNING: 'skills_cleaned' not found or not an ArrayType. Using raw 'skills' column.\")\n",
        "    df_with_array = df_cleaned.withColumn(\"skills_array\", F.split(F.lower(col(\"skills\")), \" \"))\n",
        "else:\n",
        "    df_with_array = df_cleaned.withColumnRenamed(\"skills_cleaned\", \"skills_array\")\n",
        "\n",
        "# Filter data and extract year\n",
        "df_filtered = df_with_array.filter(col(\"skills_array\").isNotNull())\n",
        "df_with_year = df_filtered.withColumn(\"year\", year(col(\"first_seen\")))\n",
        "\n",
        "# --- 1. Skill Classification Dictionaries (Simplified) ---\n",
        "# NOTE: In a real scenario, these dictionaries would be much more extensive.\n",
        "print(\"[Step 1] Defining skill categories...\")\n",
        "\n",
        "TECH_SKILLS = [\"python\", \"java\", \"sql\", \"aws\", \"azure\", \"docker\", \"kubernetes\", \"react\", \"c++\", \"tableau\"]\n",
        "SOFT_SKILLS = [\"communication\", \"leadership\", \"teamwork\", \"problem-solving\", \"collaboration\", \"adaptability\", \"mentoring\"]\n",
        "BUSINESS_SKILLS = [\"finance\", \"budgeting\", \"strategy\", \"marketing\", \"sales\", \"p&l\", \"forecasting\"]\n",
        "\n",
        "# --- 2. Feature Engineering: Skill Category Flags ---\n",
        "print(\"[Step 2] Labeling job postings with skill categories...\")\n",
        "\n",
        "# Function to check if any skill in a list is present in the job's skills_array\n",
        "def check_category(skill_list):\n",
        "    return array_contains(col(\"skills_array\"), skill_list[0]) | \\\n",
        "           reduce(lambda a, b: a | b, [array_contains(col(\"skills_array\"), skill.lower()) for skill in skill_list[1:]])\n",
        "\n",
        "df_labeled = df_with_year \\\n",
        "    .withColumn(\"has_tech\", check_category(TECH_SKILLS)) \\\n",
        "    .withColumn(\"has_soft\", check_category(SOFT_SKILLS)) \\\n",
        "    .withColumn(\"has_business\", check_category(BUSINESS_SKILLS))\n",
        "\n",
        "# --- 3. Defining Hybrid and Primary Skill Categories ---\n",
        "print(\"[Step 3] Defining Hybrid and Primary Skill Categories...\")\n",
        "\n",
        "df_categorized = df_labeled.withColumn(\"is_hybrid\",\n",
        "    when(col(\"has_tech\") & col(\"has_soft\"), True).otherwise(False)\n",
        ")\n",
        "\n",
        "# Identify the dominant skill category for non-hybrid jobs\n",
        "df_categorized = df_categorized.withColumn(\"skill_category\",\n",
        "    when(col(\"has_tech\"), \"Technical\")\n",
        "    .when(col(\"has_soft\"), \"Soft\")\n",
        "    .when(col(\"has_business\"), \"Business\")\n",
        "    .otherwise(\"Other\")\n",
        ")\n",
        "\n",
        "# --- 4. Evolution of Hybrid Roles (for Visualization 1 & 4) ---\n",
        "print(\"[Step 4] Calculating Hybrid Skill Evolution...\")\n",
        "df_hybrid_evolution = df_categorized.groupBy(\"year\").agg(\n",
        "    spark_sum(when(col(\"is_hybrid\"), 1).otherwise(0)).alias(\"hybrid_count\"),\n",
        "    count(\"*\").alias(\"total_jobs\")\n",
        ").withColumn(\"hybrid_percentage\", (col(\"hybrid_count\") / col(\"total_jobs\")) * 100)\n",
        "\n",
        "# --- 5. Skill Category Normalization (for Visualization 2 & 3) ---\n",
        "# Goal: Get the total count of each category (Tech, Soft, Business, Other) per year.\n",
        "print(\"[Step 5] Calculating Category Evolution and preparing for Heatmap...\")\n",
        "\n",
        "# Explode the skills_array to count individual skill occurrences (for Visualization 3/Heatmap)\n",
        "df_skill_counts = df_with_year.withColumn(\"skill\", explode(col(\"skills_array\")))\n",
        "df_skill_counts = df_skill_counts.filter(col(\"skill\") != \"\")\n",
        "\n",
        "# Aggregate counts for the top 10 companies/industries\n",
        "top_companies = df_cleaned.groupBy(\"company\").count().orderBy(col(\"count\").desc()).limit(10).toPandas()['company'].tolist()\n",
        "\n",
        "# Aggregate categorized counts by year and company for Visualization 5 (Faceted Charts)\n",
        "df_filtered_categories = df_categorized.filter(col(\"company\").isin(top_companies))\n",
        "\n",
        "# --- 6. Visualization Data Preparation ---\n",
        "# Collect data for plotting (conversion to Pandas)\n",
        "print(\"[Step 6] Normalizing data for visualization...\")\n",
        "\n",
        "# Aggregation for Stacked Area Chart (Viz 2)\n",
        "df_category_evolution = df_categorized.groupBy(\"year\").pivot(\"skill_category\").agg(count(\"*\")).fillna(0).toPandas()\n",
        "df_category_evolution.set_index('year', inplace=True)\n",
        "# Normalize to percentages\n",
        "df_category_evolution = df_category_evolution.div(df_category_evolution.sum(axis=1), axis=0) * 100\n",
        "df_category_evolution.sort_index(inplace=True)\n",
        "\n",
        "# Latest Year Heatmap Data (Viz 3)\n",
        "latest_year = df_categorized.select(spark_sum(col(\"year\")).alias(\"total_year_sum\")).first()[\"total_year_sum\"]\n",
        "if latest_year is not None:\n",
        "    latest_year = df_categorized.select(col(\"year\")).distinct().orderBy(col(\"year\").desc()).limit(1).collect()[0]['year']\n",
        "    df_latest = df_categorized.filter(col(\"year\") == latest_year)\n",
        "\n",
        "    df_viz_normalized = df_latest.groupBy(\"company\").agg(\n",
        "        spark_sum(when(col(\"skill_category\") == \"Technical\", 1).otherwise(0)).alias(\"Technical\"),\n",
        "        spark_sum(when(col(\"skill_category\") == \"Soft\", 1).otherwise(0)).alias(\"Soft\"),\n",
        "        spark_sum(when(col(\"skill_category\") == \"Business\", 1).otherwise(0)).alias(\"Business\"),\n",
        "    ).filter(col(\"company\").isin(top_companies)).toPandas()\n",
        "\n",
        "    # Normalize the counts for heatmap (row-wise percentages)\n",
        "    df_viz_normalized.set_index('company', inplace=True)\n",
        "    df_viz_normalized = df_viz_normalized.div(df_viz_normalized.sum(axis=1), axis=0) * 100\n",
        "    df_viz_normalized = df_viz_normalized[['Technical', 'Soft', 'Business']]\n",
        "    print(\"‚úì Data normalized\")\n",
        "else:\n",
        "    print(\"WARNING: Cannot determine latest year for heatmap. Skipping Viz 3.\")\n",
        "    df_viz_normalized = pd.DataFrame()\n",
        "\n",
        "\n",
        "# --- VISUALIZATION SECTION ---\n",
        "print(\"\\n[Visualization] Creating charts...\")\n",
        "\n",
        "# --- Get necessary Pandas DataFrames for plotting ---\n",
        "hybrid_pd = df_hybrid_evolution.toPandas()\n",
        "category_time_pd = df_filtered_categories.groupBy(\"year\", \"company\", \"skill_category\").agg(\n",
        "    count(\"job_link\").alias(\"count\")\n",
        ").toPandas()\n",
        "# Calculate percentages per company-year for Faceted Plot\n",
        "category_time_pd['total'] = category_time_pd.groupby(['year', 'company'])['count'].transform('sum')\n",
        "category_time_pd['percentage'] = (category_time_pd['count'] / category_time_pd['total']) * 100\n",
        "category_time_pd = category_time_pd.dropna() # Drop any rows where total is 0\n",
        "\n",
        "# --- VISUALIZATION 1: Hybrid Skill Evolution Line Chart ---\n",
        "print(\"  ‚Üí Creating Visualization 1: Hybrid Skill Evolution\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='year', y='hybrid_percentage', data=hybrid_pd, marker='o',\n",
        "             linewidth=3, color='#4CAF50', label='Hybrid Roles')\n",
        "plt.title('Percentage of Hybrid (Tech + Soft) Roles Over Time', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Percentage of Total Jobs (%)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(hybrid_pd['year'].unique(), rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('goal_6_hybrid_evolution.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"    ‚úì Saved: goal_6_hybrid_evolution.png\")\n",
        "\n",
        "\n",
        "# --- VISUALIZATION 2: Stacked Area Chart - Category Evolution ---\n",
        "print(\"\\n  ‚Üí Creating Visualization 2: Skill Category Evolution\")\n",
        "plt.figure(figsize=(12, 7))\n",
        "df_category_evolution.plot(kind='area', stacked=True, alpha=0.8,\n",
        "                           color={'Technical': '#007BFF', 'Soft': '#FFC107', 'Business': '#28A745', 'Other': '#6C757D'},\n",
        "                           ax=plt.gca())\n",
        "plt.title('Proportional Evolution of Primary Skill Categories', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Proportion of Total Jobs (%)', fontsize=12)\n",
        "plt.legend(title='Category', loc='upper left')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig('goal_6_category_evolution_stacked.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"    ‚úì Saved: goal_6_category_evolution_stacked.png\")\n",
        "\n",
        "\n",
        "# --- VISUALIZATION 3: Heatmap - Latest Year Category Distribution ---\n",
        "if not df_viz_normalized.empty:\n",
        "    print(\"\\n  ‚Üí Creating Visualization 3: Latest Year Category Distribution Heatmap\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(df_viz_normalized, annot=True, fmt=\".1f\", cmap=\"Blues\",\n",
        "                linewidths=.5, cbar_kws={'label': 'Proportion of Jobs (%)'})\n",
        "    plt.title(f'Skill Category Distribution by Company ({latest_year})', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Company', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('goal_6_category_heatmap.png', dpi=300)\n",
        "    plt.show()\n",
        "    print(\"    ‚úì Saved: goal_6_category_heatmap.png\")\n",
        "\n",
        "\n",
        "# --- VISUALIZATION 4: Hybrid vs Non-Hybrid Comparison (Bar Chart) ---\n",
        "print(\"\\n  ‚Üí Creating Visualization 4: Hybrid vs Non-Hybrid Job Counts\")\n",
        "hybrid_counts = df_categorized.groupBy(\"is_hybrid\").count().toPandas()\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='is_hybrid', y='count', data=hybrid_counts, palette=['#FF5733', '#4CAF50'])\n",
        "plt.title('Total Job Postings: Hybrid vs. Non-Hybrid', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Job Type (False=Non-Hybrid, True=Hybrid)', fontsize=12)\n",
        "plt.ylabel('Total Count of Job Postings', fontsize=12)\n",
        "plt.xticks(ticks=[0, 1], labels=['Non-Hybrid', 'Hybrid'])\n",
        "plt.tight_layout()\n",
        "plt.savefig('goal_6_hybrid_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"    ‚úì Saved: goal_6_hybrid_comparison.png\")\n",
        "\n",
        "\n",
        "# --- VISUALIZATION 5: Faceted Line Charts by Company ---\n",
        "if not category_time_pd.empty and not top_companies.empty:\n",
        "    print(\"\\n  ‚Üí Creating Visualization 5: Category Evolution by Company (Faceted)\")\n",
        "\n",
        "    # Filter for the main categories for clarity in facets\n",
        "    category_time_pd_filtered = category_time_pd[category_time_pd['skill_category'].isin(['Technical', 'Soft', 'Business'])]\n",
        "\n",
        "    num_companies = len(top_companies)\n",
        "    if num_companies > 0:\n",
        "        rows = int(np.ceil(num_companies / 5))\n",
        "        cols = min(num_companies, 5)\n",
        "\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows), sharey=True)\n",
        "        # Ensure axes is an iterable array even if only one subplot exists\n",
        "        axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
        "\n",
        "        for idx, company in enumerate(top_companies):\n",
        "            if idx >= len(axes): continue\n",
        "            ax = axes[idx]\n",
        "            company_data = category_time_pd_filtered[category_time_pd_filtered['company'] == company]\n",
        "\n",
        "            # Iterate over unique categories in the company_data DataFrame (FIXED)\n",
        "            sns.lineplot(x='year', y='percentage', hue='skill_category', data=company_data,\n",
        "                         marker='o', ax=ax, linewidth=2)\n",
        "\n",
        "            ax.set_title(company, fontsize=11, fontweight='bold')\n",
        "            ax.set_xlabel('Year', fontsize=9)\n",
        "            ax.set_ylabel('Percentage (%)', fontsize=9)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Only add legend to the first plot to avoid clutter\n",
        "            if idx == 0:\n",
        "                ax.legend(fontsize=7, loc='best', title='Category')\n",
        "            else:\n",
        "                ax.get_legend().remove()\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_companies, len(axes)):\n",
        "            fig.delaxes(axes[i])\n",
        "\n",
        "        plt.suptitle('Skill Category Evolution by Company Over Time',\n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for suptitle\n",
        "        plt.savefig('goal_6_category_evolution_faceted.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"    ‚úì Saved: goal_6_category_evolution_faceted.png\")\n",
        "    else:\n",
        "         print(\"WARNING: Not enough top companies found for faceted visualization.\")\n",
        "else:\n",
        "    print(\"WARNING: DataFrames for Faceted Line Charts are empty. Skipping Viz 5.\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ GOAL 6 EXECUTION COMPLETE\\n\")"
      ],
      "metadata": {
        "id": "cX3pUn89FBqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Results"
      ],
      "metadata": {
        "id": "bZeRjMP7jpDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ ALL GOALS COMPLETED\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# =============================================================================\n",
        "# CLEANUP: Stop Spark Session\n",
        "# =============================================================================\n",
        "print(\"\\nüîß Cleaning up resources...\")\n",
        "\n",
        "try:\n",
        "    if 'spark' in locals():\n",
        "        spark.stop()\n",
        "        print(\"‚úÖ Spark session stopped successfully\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No active Spark session found\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error stopping Spark session: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL PROCESSING COMPLETE - SESSION CLOSED\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "6NYoE1fKFRY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}