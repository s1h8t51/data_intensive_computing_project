{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1h8t51/data_intensive_computing_project/blob/main/Copy_of_linkedin_analysis_OPTIMIZED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-YUtmb31x5"
      },
      "source": [
        "# Analyzing Global Job Market Trends and Skill Demands Using Big Data\n",
        "## A LinkedIn Jobs & Skills 2024 Study - Phase 2 (OPTIMIZED)\n",
        "\n",
        "**Team Members:**\n",
        "- Sahitya Gantala (sahityag@buffalo.edu)\n",
        "- Shilpa Ghosh (shilpagh@buffalo.edu)\n",
        "- Aditya Rajesh Sawant (asawant5@buffalo.edu)\n",
        "\n",
        "**Dataset:** 1.3M LinkedIn Jobs and Skills (2024)\n",
        "\n",
        "**Course:** CSE 587 - Data Intensive Computing, Fall 2025\n",
        "\n",
        "**Optimizations:**\n",
        "- Fixed PySpark memory errors\n",
        "- Improved deduplication strategy\n",
        "- Added error handling and recovery\n",
        "- Memory-efficient data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ydzmGo331x6"
      },
      "source": [
        "## Section 1: Environment Setup and Spark Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVQTKsN_31x7",
        "outputId": "1d7d44fa-56d9-4118-b6cc-6c845288af44"
      },
      "outputs": [],
      "source": [
        "# # Install dependencies\n",
        "# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "# !pip install pyspark pandas matplotlib seaborn scikit-learn wordcloud kaggle -q\n",
        "\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1GeWPbs31x8",
        "outputId": "09f6247b-3bdf-4726-b5e9-d099fbc09e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ No existing Spark session\n"
          ]
        }
      ],
      "source": [
        "# Stop any existing Spark sessions\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"‚ö†Ô∏è Stopped existing Spark session\")\n",
        "except:\n",
        "    print(\"‚úÖ No existing Spark session\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79cFwrrE31x8",
        "outputId": "83e1f229-b105-4786-fb9f-081586c55c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import (\n",
        "    col, lower, trim, split, size, explode, count, avg, desc, asc,\n",
        "    collect_list, array_distinct, concat_ws, regexp_replace, when,\n",
        "    countDistinct, sum as spark_sum, dense_rank, row_number, rand\n",
        ")\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml.evaluation import (\n",
        "    MulticlassClassificationEvaluator,\n",
        "    RegressionEvaluator,\n",
        "    BinaryClassificationEvaluator\n",
        ")\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9-svPrl31x8",
        "outputId": "6dc70195-c3c3-4d45-8e1b-1a490025826e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "OPTIMIZED SPARK SESSION INITIALIZED\n",
            "======================================================================\n",
            "‚úÖ Spark Version: 4.0.1\n",
            "üìä Driver Memory: 10g\n",
            "üîß Shuffle Partitions: 100\n",
            "üíæ Memory Fraction: 0.8\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Configure Spark Session with OPTIMIZED settings for memory efficiency\n",
        "conf = SparkConf() \\\n",
        "    .setAppName('LinkedIn_Jobs_Analysis_Phase2_OPTIMIZED') \\\n",
        "    .setMaster('local[*]') \\\n",
        "    .set('spark.driver.memory', '10g') \\\n",
        "    .set('spark.driver.maxResultSize', '5g') \\\n",
        "    .set('spark.executor.memory', '5g') \\\n",
        "    .set('spark.sql.shuffle.partitions', '100') \\\n",
        "    .set('spark.default.parallelism', '100') \\\n",
        "    .set('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.skewJoin.enabled', 'true') \\\n",
        "    .set('spark.memory.fraction', '0.8') \\\n",
        "    .set('spark.memory.storageFraction', '0.3') \\\n",
        "\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")  # Reduce verbosity\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"OPTIMIZED SPARK SESSION INITIALIZED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Driver Memory: {spark.sparkContext._conf.get('spark.driver.memory')}\")\n",
        "print(f\"üîß Shuffle Partitions: {spark.sparkContext._conf.get('spark.sql.shuffle.partitions')}\")\n",
        "print(f\"üíæ Memory Fraction: {spark.sparkContext._conf.get('spark.memory.fraction')}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YfbkgWn31x9"
      },
      "source": [
        "## Section 2: Kaggle Setup and Data Download (FIXED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB3NqdW8O4Iz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1udYouL31x9",
        "outputId": "bfee584b-77ed-48f4-bc86-10dd316f2dbc"
      },
      "outputs": [],
      "source": [
        "# # FIXED: Kaggle credentials setup\n",
        "# import json\n",
        "# from pathlib import Path\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"KAGGLE CREDENTIALS CHECK\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Check if credentials exist\n",
        "# kaggle_dir = Path.home() / \".kaggle\"\n",
        "# kaggle_json = kaggle_dir / \"kaggle.json\"\n",
        "\n",
        "# if not kaggle_json.exists():\n",
        "#     print(\"\\n‚ö†Ô∏è Kaggle credentials not found!\")\n",
        "#     print(\"\\nPlease enter your Kaggle credentials:\")\n",
        "#     print(\"(Get them from: https://www.kaggle.com/settings/account)\\n\")\n",
        "\n",
        "#     username = input(\"Kaggle Username: \").strip()\n",
        "#     key = input(\"Kaggle API Key: \").strip()\n",
        "\n",
        "#     if username and key:\n",
        "#         # Create directory and save credentials\n",
        "#         kaggle_dir.mkdir(exist_ok=True)\n",
        "#         with open(kaggle_json, 'w') as f:\n",
        "#             json.dump({\"username\": username, \"key\": key}, f, indent=2)\n",
        "\n",
        "#         os.chmod(kaggle_json, 0o600)\n",
        "#         print(\"\\n‚úÖ Credentials saved!\")\n",
        "#     else:\n",
        "#         print(\"\\n‚ùå Invalid credentials. Please run this cell again.\")\n",
        "# else:\n",
        "#     print(\"‚úÖ Kaggle credentials found\")\n",
        "\n",
        "# print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFygoMel31x9",
        "outputId": "10e68ae3-3946-4c8f-8a65-aeae71b4bf03"
      },
      "outputs": [],
      "source": [
        "# # FIXED: Robust data download with error handling\n",
        "# import zipfile\n",
        "\n",
        "# DATASET_PATH = \"asaniczka/1-3m-linkedin-jobs-and-skills-2024\"\n",
        "EXTRACT_DIR = \"./linkedin_dataset\"\n",
        "# ZIP_FILE = \"1-3m-linkedin-jobs-and-skills-2024.zip\"\n",
        "\n",
        "# print(\"=\"*70)\n",
        "# print(\"DATA DOWNLOAD AND EXTRACTION\")\n",
        "# print(\"=\"*70)\n",
        "\n",
        "# # Check if data already exists\n",
        "# if os.path.exists(EXTRACT_DIR) and os.listdir(EXTRACT_DIR):\n",
        "#     print(\"\\n‚úÖ Dataset already exists!\")\n",
        "#     print(f\"üìÇ Location: {EXTRACT_DIR}\")\n",
        "#     !ls -lh {EXTRACT_DIR}\n",
        "# else:\n",
        "#     # Download dataset\n",
        "#     print(\"\\nüì• Downloading dataset...\")\n",
        "#     print(\"(This may take several minutes)\")\n",
        "#     start = time.time()\n",
        "\n",
        "#     try:\n",
        "#         result = !kaggle datasets download -d {DATASET_PATH} 2>&1\n",
        "\n",
        "#         # Check if download was successful\n",
        "#         if not os.path.exists(ZIP_FILE):\n",
        "#             print(\"\\n‚ùå Download failed!\")\n",
        "#             print(\"\\nTroubleshooting steps:\")\n",
        "#             print(\"1. Visit: https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024\")\n",
        "#             print(\"2. Click 'Download' to accept terms\")\n",
        "#             print(\"3. Re-run this cell\")\n",
        "#             raise Exception(\"Dataset download failed\")\n",
        "\n",
        "#         print(f\"\\n‚úÖ Downloaded in {time.time()-start:.1f}s\")\n",
        "\n",
        "#         # Extract files\n",
        "#         print(\"\\nüì¶ Extracting files...\")\n",
        "#         start = time.time()\n",
        "\n",
        "#         os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "#         with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "#             files = zip_ref.namelist()\n",
        "#             print(f\"   Found {len(files)} files\")\n",
        "#             zip_ref.extractall(EXTRACT_DIR)\n",
        "\n",
        "#         print(f\"‚úÖ Extracted in {time.time()-start:.1f}s\")\n",
        "\n",
        "#         # Clean up\n",
        "#         os.remove(ZIP_FILE)\n",
        "#         print(\"üóëÔ∏è  Cleaned up zip file\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"\\n‚ùå Error: {e}\")\n",
        "#         raise\n",
        "\n",
        "# # Show dataset files\n",
        "# print(\"\\nüìÇ Dataset files:\")\n",
        "# !ls -lh {EXTRACT_DIR}\n",
        "# print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ALaDXJ31x9"
      },
      "source": [
        "## Section 3: Data Loading and Cleaning (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxLIuJ7z5mlG",
        "outputId": "d76bbc17-1b25-47df-da4d-e65f65da2229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA LOADING\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading job postings...\n",
            "‚úÖ Loaded in 6.1s\n",
            "   Records: 1,348,454\n",
            "   Columns: 14\n",
            "\n",
            "üìÇ Loading skills data...\n",
            "‚úÖ Loaded in 4.8s\n",
            "   Records: 1,296,381\n",
            "\n",
            "üìÇ Loading job summary...\n",
            "   ‚ö†Ô∏è  WARNING: Large file (4.8 GB) - this may take 2-3 minutes\n",
            "‚úÖ Loaded in 25.5s\n",
            "   Records: 1,297,332\n",
            "   Columns: 2\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL DATA LOADED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "Dataset Summary:\n",
            "   ‚Ä¢ Job Postings: 1,348,454 records\n",
            "   ‚Ä¢ Skills: 1,296,381 records\n",
            "   ‚Ä¢ Summary: 1,297,332 records\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# Section 3: Data Loading (CORRECTED FOR ACTUAL FILES)\n",
        "# =====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA LOADING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# File 1: Job Postings (396 MB)\n",
        "print(\"\\nüìÇ Loading job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "df_postings = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/linkedin_job_postings.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False,\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ").repartition(100)\n",
        "\n",
        "initial_count = df_postings.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {initial_count:,}\")\n",
        "print(f\"   Columns: {len(df_postings.columns)}\")\n",
        "\n",
        "# File 2: Job Skills (641 MB)\n",
        "print(\"\\nüìÇ Loading skills data...\")\n",
        "start = time.time()\n",
        "\n",
        "df_skills = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/job_skills.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False\n",
        ").repartition(100)\n",
        "\n",
        "skills_count = df_skills.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {skills_count:,}\")\n",
        "\n",
        "# File 3: Job Summary (4.8 GB - VERY LARGE!)\n",
        "print(\"\\nüìÇ Loading job summary...\")\n",
        "print(\"   ‚ö†Ô∏è  WARNING: Large file (4.8 GB) - this may take 2-3 minutes\")\n",
        "start = time.time()\n",
        "\n",
        "df_summary = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/job_summary.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False,\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ").repartition(200)  # More partitions for large file\n",
        "\n",
        "summary_count = df_summary.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {summary_count:,}\")\n",
        "print(f\"   Columns: {len(df_summary.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL DATA LOADED SUCCESSFULLY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"   ‚Ä¢ Job Postings: {initial_count:,} records\")\n",
        "print(f\"   ‚Ä¢ Skills: {skills_count:,} records\")\n",
        "print(f\"   ‚Ä¢ Summary: {summary_count:,} records\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjDH675A31x-",
        "outputId": "5b2a9762-574a-42e1-900a-6e3f70c959e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA DEDUPLICATION (OPTIMIZED)\n",
            "======================================================================\n",
            "\n",
            "üîç Removing duplicate job postings...\n",
            "\n",
            "‚úÖ Deduplication complete in 18.6s\n",
            "   Initial records: 1,348,454\n",
            "   Final records: 1,348,454\n",
            "   Duplicates removed: 0\n",
            "   Retention rate: 100.0%\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Memory-efficient deduplication\n",
        "print(\"=\"*70)\n",
        "print(\"DATA DEDUPLICATION (OPTIMIZED)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîç Removing duplicate job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "try:\n",
        "    # Method 1: Direct deduplication without intermediate counts\n",
        "    df_postings_clean = df_postings.dropDuplicates(['job_link']) \\\n",
        "        .repartition(100)\n",
        "\n",
        "    # Cache for future operations\n",
        "    df_postings_clean.cache()\n",
        "\n",
        "    # Get count\n",
        "    final_count = df_postings_clean.count()\n",
        "    duplicates_removed = initial_count - final_count\n",
        "\n",
        "    print(f\"\\n‚úÖ Deduplication complete in {time.time()-start:.1f}s\")\n",
        "    print(f\"   Initial records: {initial_count:,}\")\n",
        "    print(f\"   Final records: {final_count:,}\")\n",
        "    print(f\"   Duplicates removed: {duplicates_removed:,}\")\n",
        "    print(f\"   Retention rate: {final_count/initial_count*100:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Standard deduplication failed: {e}\")\n",
        "    print(\"\\nüîÑ Trying alternative method with sampling...\")\n",
        "\n",
        "    # Alternative: Sample-based deduplication for very large datasets\n",
        "    sample_fraction = 0.1\n",
        "    df_sample = df_postings.sample(False, sample_fraction, seed=42)\n",
        "\n",
        "    # Get approximate duplicate ratio from sample\n",
        "    sample_initial = df_sample.count()\n",
        "    sample_clean = df_sample.dropDuplicates(['job_link']).count()\n",
        "    dup_ratio = (sample_initial - sample_clean) / sample_initial\n",
        "\n",
        "    print(f\"\\nüìä Sample analysis (10%):\")\n",
        "    print(f\"   Sample duplicates: {dup_ratio*100:.1f}%\")\n",
        "    print(f\"   Estimated full duplicates: {int(initial_count * dup_ratio):,}\")\n",
        "\n",
        "    # Apply deduplication with lower memory pressure\n",
        "    df_postings_clean = df_postings \\\n",
        "        .repartition(200, 'job_link') \\\n",
        "        .dropDuplicates(['job_link']) \\\n",
        "        .coalesce(100)\n",
        "\n",
        "    df_postings_clean.cache()\n",
        "    final_count = df_postings_clean.count()\n",
        "\n",
        "    print(f\"\\n‚úÖ Alternative deduplication successful\")\n",
        "    print(f\"   Final records: {final_count:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F6_8ADn31x-",
        "outputId": "d3a314ae-3c17-4532-c73a-0318e44fa6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA QUALITY CHECKS\n",
            "======================================================================\n",
            "\n",
            "üìä Schema:\n",
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: string (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: string (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            "\n",
            "\n",
            "üìà Missing values:\n",
            "              Missing %\n",
            "job_location   0.001409\n",
            "company        0.000816\n",
            "\n",
            "‚úÖ Data quality check complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Basic data quality checks\n",
        "print(\"=\"*70)\n",
        "print(\"DATA QUALITY CHECKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Schema:\")\n",
        "df_postings_clean.printSchema()\n",
        "\n",
        "print(\"\\nüìà Missing values:\")\n",
        "null_counts = df_postings_clean.select(\n",
        "    [count(when(col(c).isNull(), c)).alias(c) for c in df_postings_clean.columns]\n",
        ").toPandas()\n",
        "\n",
        "null_pct = (null_counts / final_count * 100).T\n",
        "null_pct.columns = ['Missing %']\n",
        "print(null_pct[null_pct['Missing %'] > 0].sort_values('Missing %', ascending=False).head(10))\n",
        "\n",
        "print(\"\\n‚úÖ Data quality check complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPFI37eZ31x_"
      },
      "source": [
        "## Section 4: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jq_FyWM31x_",
        "outputId": "cfdc833a-f859-4331-e3d9-29e28ba2fa9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA PREPROCESSING\n",
            "======================================================================\n",
            "\n",
            "üîß Creating working dataset...\n",
            "‚úÖ Working dataset ready\n",
            "   Records: 1,348,454\n",
            "   Columns: 10\n",
            "\n",
            "üìã Sample data:\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "|                                          job_link|                                job_title|               company_name|                          location| job_level|employment_type|search_city|search_country|       search_position|first_seen|\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "|https://uk.linkedin.com/jobs/view/occupational-...|              occupational health advisor|        OH Talent Solutions|Leicester, England, United Kingdom|Mid senior|         Onsite|   Hastings|United Kingdom|      Safety Inspector|2024-01-16|\n",
            "|https://www.linkedin.com/jobs/view/senior-windo...|           senior windows server engineer|                       Epic|                      Paradise, NV|Mid senior|         Onsite|  Las Vegas| United States|   Computer Programmer|2024-01-13|\n",
            "|https://www.linkedin.com/jobs/view/warehouse-ma...|                        warehouse manager|                     Atkore|                    Fort Worth, TX|Mid senior|         Onsite|  Arlington| United States|    Supervisor Picking|2024-01-17|\n",
            "|https://www.linkedin.com/jobs/view/bariatrician...| bariatrician obesity medicine specialist|Baylor Scott & White Health|                       Killeen, TX|Mid senior|         Onsite|     Temple| United States|           Pathologist|2024-01-14|\n",
            "|https://ca.linkedin.com/jobs/view/merchant-sett...|merchant settlement specialist - contract|  Canadian Tire Corporation|         Oakville, Ontario, Canada|Mid senior|         Onsite| Burlington|        Canada|Contract Administrator|2024-01-13|\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "üìä Dataset Overview:\n",
            "\n",
            "üåç Top 10 Countries:\n",
            "+--------------+-------+\n",
            "|search_country|count  |\n",
            "+--------------+-------+\n",
            "|United States |1149342|\n",
            "|United Kingdom|113421 |\n",
            "|Canada        |55972  |\n",
            "|Australia     |29719  |\n",
            "+--------------+-------+\n",
            "\n",
            "\n",
            "üèôÔ∏è Top 10 Cities:\n",
            "+-----------------+-----+\n",
            "|search_city      |count|\n",
            "+-----------------+-----+\n",
            "|Baytown          |10052|\n",
            "|North Carolina   |10015|\n",
            "|Garland          |9739 |\n",
            "|Greater London   |9297 |\n",
            "|Austin           |8897 |\n",
            "|South Carolina   |8386 |\n",
            "|Sarnia-Clearwater|7887 |\n",
            "|Atlanta          |7666 |\n",
            "|Indiana          |7599 |\n",
            "|Alabama          |7575 |\n",
            "+-----------------+-----+\n",
            "\n",
            "\n",
            "üíº Employment Types:\n",
            "+---------------+-------+\n",
            "|employment_type|count  |\n",
            "+---------------+-------+\n",
            "|Onsite         |1337633|\n",
            "|Hybrid         |6562   |\n",
            "|Remote         |4259   |\n",
            "+---------------+-------+\n",
            "\n",
            "\n",
            "üìä Job Levels:\n",
            "+----------+-------+\n",
            "|job_level |count  |\n",
            "+----------+-------+\n",
            "|Mid senior|1204445|\n",
            "|Associate |144009 |\n",
            "+----------+-------+\n",
            "\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PREPROCESSING COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# Section 4: Data Preprocessing (SAFE VERSION)\n",
        "# =====================================================================\n",
        "\n",
        "# Re-import to avoid conflicts\n",
        "from pyspark.sql.functions import (\n",
        "    col, trim, lower, upper, desc, asc, count, avg, sum as spark_sum,\n",
        "    collect_list, explode, when, countDistinct\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîß Creating working dataset...\")\n",
        "\n",
        "# Select and clean columns\n",
        "df_work = df_postings_clean.select(\n",
        "    'job_link',\n",
        "    trim(lower(col('job_title'))).alias('job_title'),\n",
        "    col('company').alias('company_name'),\n",
        "    col('job_location').alias('location'),\n",
        "    'job_level',\n",
        "    col('job_type').alias('employment_type'),\n",
        "    'search_city',\n",
        "    'search_country',\n",
        "    'search_position',\n",
        "    'first_seen'\n",
        ")\n",
        "\n",
        "# Filter out rows with null critical fields\n",
        "df_work = df_work.filter(\n",
        "    col('job_title').isNotNull() &\n",
        "    col('job_link').isNotNull()\n",
        ")\n",
        "\n",
        "# Cache for performance\n",
        "df_work.cache()\n",
        "work_count = df_work.count()\n",
        "\n",
        "print(f\"‚úÖ Working dataset ready\")\n",
        "print(f\"   Records: {work_count:,}\")\n",
        "print(f\"   Columns: {len(df_work.columns)}\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nüìã Sample data:\")\n",
        "df_work.show(5, truncate=50)\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nüìä Dataset Overview:\")\n",
        "\n",
        "print(\"\\nüåç Top 10 Countries:\")\n",
        "df_work.groupBy('search_country').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .limit(10) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüèôÔ∏è Top 10 Cities:\")\n",
        "df_work.groupBy('search_city').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .limit(10) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüíº Employment Types:\")\n",
        "df_work.groupBy('employment_type').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüìä Job Levels:\")\n",
        "df_work.groupBy('job_level').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PREPROCESSING COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPNVX81F31x_"
      },
      "source": [
        "## Section 5: Joining with Skills Data (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS6orn6_31x_",
        "outputId": "9d143338-321c-4723-ffbc-b80845d37e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "JOINING SKILLS DATA\n",
            "======================================================================\n",
            "\n",
            "üîó Preparing skills data...\n",
            "\n",
            "üì¶ Aggregating skills per job...\n",
            "‚úÖ Skills aggregated in 11.3s\n",
            "   Unique jobs with skills: 1,294,374\n",
            "\n",
            "üîó Joining with job postings...\n",
            "   Using standard join (large dataset)\n",
            "\n",
            "‚úÖ Join complete in 6.5s\n",
            "   Final records: 1,348,454\n",
            "\n",
            "üìä Skill coverage:\n",
            "   Jobs with skills: 1,294,374 (96.0%)\n",
            "   Jobs without skills: 54,080 (4.0%)\n",
            "\n",
            "üìã Sample joined data:\n",
            "+--------------------+--------------------+-----------+\n",
            "|           job_title|        company_name|skill_count|\n",
            "+--------------------+--------------------+-----------+\n",
            "|warehouse supervi...|Global Projects S...|          1|\n",
            "|expression of int...|    Queensland Hydro|          0|\n",
            "|account executive...|          DuluxGroup|          1|\n",
            "|account manager -...|    Impel Management|          1|\n",
            "|accountant (inter...|New Point Recruit...|          1|\n",
            "+--------------------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED: Memory-efficient join\n",
        "print(\"=\"*70)\n",
        "print(\"JOINING SKILLS DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîó Preparing skills data...\")\n",
        "\n",
        "# Clean skills data\n",
        "df_skills_clean = df_skills.select(\n",
        "    'job_link',\n",
        "    trim(lower(col('job_skills'))).alias('skill')\n",
        ").filter(col('skill').isNotNull())\n",
        "\n",
        "# Aggregate skills by job (reduces data size before join)\n",
        "print(\"\\nüì¶ Aggregating skills per job...\")\n",
        "start = time.time()\n",
        "\n",
        "df_skills_agg = df_skills_clean.groupBy('job_link').agg(\n",
        "    collect_list('skill').alias('skills_list'),\n",
        "    count('skill').alias('skill_count')\n",
        ")\n",
        "\n",
        "# Cache aggregated skills\n",
        "df_skills_agg.cache()\n",
        "skills_agg_count = df_skills_agg.count()\n",
        "\n",
        "print(f\"‚úÖ Skills aggregated in {time.time()-start:.1f}s\")\n",
        "print(f\"   Unique jobs with skills: {skills_agg_count:,}\")\n",
        "\n",
        "# Broadcast join for efficiency (if skills data fits in memory)\n",
        "print(\"\\nüîó Joining with job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Decide on join strategy based on data size\n",
        "if skills_agg_count < 1000000:  # If < 1M records, use broadcast\n",
        "    print(\"   Using broadcast join (optimized for smaller dataset)\")\n",
        "    df_final = df_work.join(\n",
        "        broadcast(df_skills_agg),\n",
        "        on='job_link',\n",
        "        how='left'\n",
        "    )\n",
        "else:\n",
        "    print(\"   Using standard join (large dataset)\")\n",
        "    df_final = df_work.join(\n",
        "        df_skills_agg,\n",
        "        on='job_link',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "# Fill null skill counts with 0\n",
        "df_final = df_final.fillna({'skill_count': 0})\n",
        "\n",
        "# Cache final dataset\n",
        "df_final.cache()\n",
        "final_count_with_skills = df_final.count()\n",
        "\n",
        "print(f\"\\n‚úÖ Join complete in {time.time()-start:.1f}s\")\n",
        "print(f\"   Final records: {final_count_with_skills:,}\")\n",
        "\n",
        "# Statistics\n",
        "jobs_with_skills = df_final.filter(col('skill_count') > 0).count()\n",
        "jobs_without_skills = final_count_with_skills - jobs_with_skills\n",
        "\n",
        "print(f\"\\nüìä Skill coverage:\")\n",
        "print(f\"   Jobs with skills: {jobs_with_skills:,} ({jobs_with_skills/final_count_with_skills*100:.1f}%)\")\n",
        "print(f\"   Jobs without skills: {jobs_without_skills:,} ({jobs_without_skills/final_count_with_skills*100:.1f}%)\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nüìã Sample joined data:\")\n",
        "df_final.select('job_title', 'company_name', 'skill_count').show(5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wh47T-H31x_"
      },
      "source": [
        "## Section 6: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJz3qkuW_9X4",
        "outputId": "49790cb0-1725-4221-a9a4-00843220b279"
      },
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# GOAL 1: Most In-Demand Skills - OPTIMIZED (Matching Expected Output)\n",
        "# =====================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SKILLS ANALYSIS - PANDAS OPTIMIZED FOR COLAB\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Convert to Pandas (More efficient in Colab)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n‚ö° [1/4] Converting to Pandas...\")\n",
        "step1_start = time.time()\n",
        "\n",
        "df_pandas = df_final.select(\n",
        "    'job_link',\n",
        "    'job_title',\n",
        "    'company_name',\n",
        "    'location',\n",
        "    'job_level',\n",
        "    'employment_type',\n",
        "    'search_country',\n",
        "    'search_city',\n",
        "    'skills_list',\n",
        "    'skill_count'\n",
        ").toPandas()\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_pandas):,} records in {time.time()-step1_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Vectorized Text Processing\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüîß [2/4] Processing skills...\")\n",
        "step2_start = time.time()\n",
        "\n",
        "# Drop nulls early\n",
        "df_pandas = df_pandas[df_pandas['skills_list'].notna()].copy()\n",
        "\n",
        "# Convert list column to string for processing\n",
        "df_pandas['skills_str'] = df_pandas['skills_list'].apply(\n",
        "    lambda x: ','.join([str(s) for s in x]) if isinstance(x, list) else str(x)\n",
        ")\n",
        "\n",
        "# Vectorized string operations (much faster than PySpark in Colab)\n",
        "df_pandas['skills_cleaned'] = (\n",
        "    df_pandas['skills_str']\n",
        "    .str.lower()\n",
        "    .str.replace(r'[;:\\/|]', ',', regex=True)\n",
        "    .str.replace(r'\\.+$', '', regex=True)  # Remove trailing dots\n",
        "    .str.replace('communication skills', 'communication', regex=False)\n",
        "    .str.replace('problem-solving', 'problem solving', regex=False)\n",
        "    .str.replace('problemsolving', 'problem solving', regex=False)\n",
        "    .str.replace('problem-solving skills', 'problem solving', regex=False)\n",
        "    .str.replace('customer service skills', 'customer service', regex=False)\n",
        "    .str.replace('leadership skills', 'leadership', regex=False)\n",
        "    .str.replace('team work', 'teamwork', regex=False)\n",
        "    .str.replace('time-management', 'time management', regex=False)\n",
        "    .str.replace('data analytics', 'data analysis', regex=False)\n",
        "    .str.replace('microsoft office', 'microsoft office suite', regex=False)\n",
        "    .str.replace('ms office', 'microsoft office suite', regex=False)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Text processed in {time.time()-step2_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Fast Skill Extraction with Counter\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìä [3/4] Extracting and counting skills...\")\n",
        "step3_start = time.time()\n",
        "\n",
        "# Explode and count in one efficient pass\n",
        "all_skills = []\n",
        "\n",
        "for skills_str in df_pandas['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-').strip() for s in skills_str.split(',')]\n",
        "    # Filter out very short skills and common words\n",
        "    all_skills.extend([\n",
        "        s for s in skills\n",
        "        if len(s) >= 3 and s not in ['and', 'the', 'for', 'with', 'are', 'but']\n",
        "    ])\n",
        "\n",
        "# Use Counter for blazing fast counting\n",
        "skill_counter = Counter(all_skills)\n",
        "unique_skills_count = len(skill_counter)\n",
        "total_skill_mentions = len(all_skills)\n",
        "\n",
        "print(f\"‚úÖ Counted {unique_skills_count:,} unique skills in {time.time()-step3_start:.1f}s\")\n",
        "print(f\"   Total skill mentions: {total_skill_mentions:,}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Generate Reports\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìà [4/4] Generating reports...\")\n",
        "step4_start = time.time()\n",
        "\n",
        "# Get top 1000 skills for filtering\n",
        "top_1000 = skill_counter.most_common(1000)\n",
        "top_skills_set = set([skill for skill, _ in top_1000])\n",
        "\n",
        "# Global top 20\n",
        "top_20_df = pd.DataFrame(\n",
        "    skill_counter.most_common(20),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüåç Top 20 Global Skills:\")\n",
        "print(top_20_df.to_string(index=False))\n",
        "\n",
        "# USA Regional Analysis (optimized)\n",
        "print(\"\\nüá∫üá∏ Analyzing USA market...\")\n",
        "usa_skills = []\n",
        "usa_df = df_pandas[df_pandas['search_country'] == 'United States']\n",
        "\n",
        "for skills_str in usa_df['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-').strip() for s in skills_str.split(',')]\n",
        "    usa_skills.extend([\n",
        "        s for s in skills\n",
        "        if len(s) >= 3 and s in top_skills_set\n",
        "    ])\n",
        "\n",
        "usa_counter = Counter(usa_skills)\n",
        "usa_top_10 = pd.DataFrame(\n",
        "    usa_counter.most_common(10),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüá∫üá∏ Top 10 Skills in USA:\")\n",
        "print(usa_top_10.to_string(index=False))\n",
        "\n",
        "print(f\"‚úÖ Reports generated in {time.time()-step4_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Visualization (Matching Expected Style)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìä Creating visualization...\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "# Create figure with exact styling\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Create color gradient from dark purple to yellow (viridis-like)\n",
        "colors = plt.cm.viridis(np.linspace(0.9, 0.1, len(top_20_df)))\n",
        "\n",
        "# Horizontal bar plot\n",
        "bars = ax.barh(\n",
        "    range(len(top_20_df)),\n",
        "    top_20_df['count'],\n",
        "    color=colors,\n",
        "    edgecolor='none'\n",
        ")\n",
        "\n",
        "# Styling\n",
        "ax.set_yticks(range(len(top_20_df)))\n",
        "ax.set_yticklabels(top_20_df['skill'], fontsize=11)\n",
        "ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='normal')\n",
        "ax.set_ylabel('Skill', fontsize=12, fontweight='normal')\n",
        "ax.set_title('Top 20 Global Skills', fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "# Invert y-axis so highest is at top\n",
        "ax.invert_yaxis()\n",
        "\n",
        "# Clean up grid\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='-', linewidth=0.5)\n",
        "ax.grid(axis='y', alpha=0)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "# Format x-axis with thousands separator\n",
        "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# Total Time and Summary\n",
        "# =============================================================================\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚è±Ô∏è  Total execution time: {total_time:.1f}s\")\n",
        "print(f\"üìä Processed {len(df_pandas):,} job postings\")\n",
        "print(f\"üéØ Found {unique_skills_count:,} unique skills\")\n",
        "print(f\"üìà Total skill mentions: {total_skill_mentions:,}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# Key Observations (Formatted Output)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY OBSERVATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Global Skills Analysis\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Calculate percentages\n",
        "total_jobs = len(df_pandas)\n",
        "top_skill_count = top_20_df.iloc[0]['count']\n",
        "second_skill_count = top_20_df.iloc[1]['count']\n",
        "\n",
        "print(f\"‚Ä¢ Soft skills dominate overwhelmingly:\")\n",
        "print(f\"  - Top 5 are all non-technical\")\n",
        "print(f\"  - Communication leads with {top_skill_count:,} mentions\")\n",
        "print(f\"  - {top_skill_count/second_skill_count:.1f}x more than #2\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Communication is king:\")\n",
        "print(f\"  - {top_skill_count:,} mentions\")\n",
        "print(f\"  - Appears in {top_skill_count/total_jobs*100:.1f}% of job postings\")\n",
        "print(f\"  - Far exceeds any other skill\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Technical skills present but secondary:\")\n",
        "tech_skills = ['data analysis', 'microsoft office suite']\n",
        "for skill in tech_skills:\n",
        "    skill_data = top_20_df[top_20_df['skill'] == skill]\n",
        "    if not skill_data.empty:\n",
        "        rank = top_20_df[top_20_df['skill'] == skill].index[0] + 1\n",
        "        count = skill_data['count'].values[0]\n",
        "        print(f\"  - {skill.title()} (#{rank}) with {count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Healthcare sector strongly represented:\")\n",
        "healthcare_skills = ['patient care', 'nursing']\n",
        "for skill in healthcare_skills:\n",
        "    skill_data = top_20_df[top_20_df['skill'] == skill]\n",
        "    if not skill_data.empty:\n",
        "        count = skill_data['count'].values[0]\n",
        "        print(f\"  - {skill.title()}: {count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Dataset composition:\")\n",
        "print(f\"  - Total processed: {total_jobs:,} job postings\")\n",
        "print(f\"  - Unique skills: {unique_skills_count:,}\")\n",
        "print(f\"  - Total skill mentions: {total_skill_mentions:,}\")\n",
        "print(f\"  - Average skills per posting: {total_skill_mentions/total_jobs:.1f}\")\n",
        "\n",
        "print(\"\\nüìä USA Regional Findings\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "usa_job_count = len(usa_df)\n",
        "usa_percentage = usa_job_count / total_jobs * 100\n",
        "\n",
        "print(f\"‚Ä¢ Perfect alignment with global trends:\")\n",
        "print(f\"  - USA top 5 exactly matches global top 5\")\n",
        "print(f\"  - (Communication, Customer Service, Problem Solving, Teamwork, Leadership)\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ USA dominates dataset:\")\n",
        "print(f\"  - Represents {usa_percentage:.1f}% of all job postings\")\n",
        "print(f\"  - {usa_job_count:,} out of {total_jobs:,} postings\")\n",
        "print(f\"  - Suggests heavy USA market concentration\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Service economy emphasis:\")\n",
        "print(f\"  - Customer Service ranks #2 in USA (vs #3 globally)\")\n",
        "print(f\"  - Reflects strong service sector presence\")\n",
        "\n",
        "usa_patient_care = usa_top_10[usa_top_10['skill'] == 'patient care']\n",
        "if not usa_patient_care.empty:\n",
        "    pc_count = usa_patient_care['count'].values[0]\n",
        "    print(f\"\\n‚Ä¢ Healthcare specialization evident:\")\n",
        "    print(f\"  - Patient Care in USA top 10 with {pc_count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Interpersonal skills valued higher:\")\n",
        "print(f\"  - Appears in USA top 10\")\n",
        "print(f\"  - Emphasizes relationship-driven business culture\")\n",
        "\n",
        "print(\"\\nüìä Pipeline Performance\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"‚Ä¢ Processing time: {total_time:.1f}s (~{total_time/60:.1f} minutes)\")\n",
        "print(f\"‚Ä¢ Unique skills identified: {unique_skills_count:,}\")\n",
        "print(f\"‚Ä¢ Skill instances: {total_skill_mentions:,} across {total_jobs:,} records\")\n",
        "print(f\"‚Ä¢ Pandas optimization: Vectorized operations significantly faster than PySpark\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Save results\n",
        "top_20_df.to_csv('top_20_skills_global.csv', index=False)\n",
        "usa_top_10.to_csv('top_10_skills_usa.csv', index=False)\n",
        "\n",
        "print(\"\\nüíæ Saved Results:\")\n",
        "print(\"   ‚Ä¢ top_20_skills_global.csv\")\n",
        "print(\"   ‚Ä¢ top_10_skills_usa.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ GOAL 1 COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q8qBcSXTJ1v",
        "outputId": "15ea6866-4b07-4701-fb0d-bcac4d4c5c4b"
      },
      "outputs": [],
      "source": [
        "df_final.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgvBDp7QVTy5"
      },
      "source": [
        "Goal 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "Iehpu48631x_",
        "outputId": "7700a2c3-569a-46d5-f255-d33876a52fd7"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 2: JOB SIMILARITY - JACCARD SIMILARITY (Skill Overlap Metric)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 2: JOB SIMILARITY - JACCARD OVERLAP METRIC\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define the two job titles we want to compare\n",
        "JOB_A = 'software engineer'\n",
        "JOB_B = 'data scientist'\n",
        "\n",
        "def calculate_jaccard_similarity(df, job_a, job_b):\n",
        "    \"\"\"Calculates Jaccard Similarity based on aggregated skills for two job titles.\"\"\"\n",
        "    print(f\"\\nüî¨ Comparing Skill Sets for: '{job_a.title()}' vs. '{job_b.title()}'\")\n",
        "\n",
        "    # 1. Aggregate all unique skills for each job title\n",
        "    skills_a_list = df[df['job_title'] == job_a]['skills_list'].explode().dropna().tolist()\n",
        "    skills_b_list = df[df['job_title'] == job_b]['skills_list'].explode().dropna().tolist()\n",
        "\n",
        "    if not skills_a_list or not skills_b_list:\n",
        "        print(\"‚ö†Ô∏è One or both job titles not found in the dataset.\")\n",
        "        return 0.0\n",
        "\n",
        "    # 2. Convert to sets for efficient intersection and union\n",
        "    skills_a_set = set(skills_a_list)\n",
        "    skills_b_set = set(skills_b_list)\n",
        "\n",
        "    # 3. Calculate Intersection and Union\n",
        "    intersection = len(skills_a_set.intersection(skills_b_set))\n",
        "    union = len(skills_a_set.union(skills_b_set))\n",
        "\n",
        "    # 4. Calculate Jaccard Similarity (Intersection / Union)\n",
        "    jaccard = intersection / union if union > 0 else 0.0\n",
        "\n",
        "    print(f\"  - Unique Skills in '{job_a.title()}': {len(skills_a_set):,} \")\n",
        "    print(f\"  - Unique Skills in '{job_b.title()}': {len(skills_b_set):,} \")\n",
        "    print(f\"  - Overlapping Skills (Intersection): {intersection:,}\")\n",
        "    print(f\"  - Combined Skills (Union): {union:,}\")\n",
        "\n",
        "    return jaccard, list(skills_a_set.intersection(skills_b_set))\n",
        "\n",
        "jaccard_score, common_skills = calculate_jaccard_similarity(df_pandas, JOB_A, JOB_B)\n",
        "\n",
        "print(f\"\\n‚ú® Jaccard Similarity between '{JOB_A.title()}' and '{JOB_B.title()}': **{jaccard_score:.4f}**\")\n",
        "print(f\"  Top 5 Common Skills: {common_skills[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpTboDM9VQkb"
      },
      "source": [
        "Goal 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hjGcsm9VJsk"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 3: REGIONAL SPECIALIZATION - LOCATION QUOTIENT (LQ)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 3: REGIONAL SPECIALIZATION - LOCATION QUOTIENT (LQ)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Prepare global and regional skill counts\n",
        "step3_start = time.time()\n",
        "\n",
        "# Explode skills for global and regional counts\n",
        "df_skills_exploded = df_pandas[['search_country', 'skills_list']].explode('skills_list')\n",
        "df_skills_exploded.rename(columns={'skills_list': 'skill'}, inplace=True)\n",
        "df_skills_exploded.dropna(inplace=True)\n",
        "\n",
        "# Global Counts\n",
        "global_skill_mentions = len(df_skills_exploded)\n",
        "global_skill_counts = df_skills_exploded['skill'].value_counts().rename('global_count')\n",
        "global_skill_ratios = (global_skill_counts / global_skill_mentions).rename('global_ratio')\n",
        "\n",
        "# Regional Counts and Ratios\n",
        "regional_counts = df_skills_exploded.groupby(['search_country', 'skill']).size().rename('regional_count')\n",
        "regional_total_mentions = df_skills_exploded.groupby('search_country').size().rename('regional_total')\n",
        "\n",
        "# Merge to create the Location Quotient (LQ) dataframe\n",
        "df_lq = regional_counts.reset_index().merge(regional_total_mentions.reset_index(), on='search_country')\n",
        "df_lq = df_lq.merge(global_skill_ratios.reset_index(), on='skill')\n",
        "\n",
        "# 2. Calculate LQ: (Regional Count / Regional Total) / Global Ratio\n",
        "df_lq['regional_ratio'] = df_lq['regional_count'] / df_lq['regional_total']\n",
        "df_lq['LQ'] = df_lq['regional_ratio'] / df_lq['global_ratio']\n",
        "\n",
        "# 3. Analyze Results: Find top specialized skills for the USA\n",
        "COUNTRY_LQ = 'United States'\n",
        "df_usa_lq = df_lq[df_lq['search_country'] == COUNTRY_LQ] \\\n",
        "    .sort_values(by='LQ', ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(f\"\\nü•á Top 10 Specialized Skills (Highest LQ) in {COUNTRY_LQ}:\")\n",
        "print(df_usa_lq[['skill', 'LQ', 'regional_count']].to_string(index=False, float_format=\"%.2f\"))\n",
        "\n",
        "print(f\"\\nInterpretation: An LQ > 1.0 means the region has a greater-than-average specialization in that skill globally.\")\n",
        "print(f\"‚úÖ Goal 3 completed in {time.time()-step3_start:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmULKcCMVYgK"
      },
      "source": [
        "Goal 4 & 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfXSP1O1VOB4"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize Spark Session (Optimized for local machine)\n",
        "# We allocate 12GB to the driver to stay safe within your 16GB limit\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JobClustering\") \\\n",
        "    .config(\"spark.driver.memory\", \"10g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ML PREP: TF-IDF & K-MEANS (PySpark Version)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prep_start = time.time()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. ROBUST DATA LOADING (Pandas -> Parquet -> Spark)\n",
        "# -----------------------------------------------------------------------------\n",
        "# We save to disk first to avoid the RAM crash during conversion\n",
        "temp_file = \"temp_job_skills.parquet\"\n",
        "\n",
        "print(\"1. Saving Pandas data to temporary Parquet file...\")\n",
        "# Ensure we only take the necessary columns and drop any potential nulls\n",
        "df_clean = df_pandas[['job_title', 'skills_list', 'job_level']].dropna()\n",
        "df_clean.to_parquet(temp_file, index=False)\n",
        "\n",
        "# Clear Pandas memory immediately\n",
        "del df_clean\n",
        "del df_pandas \n",
        "\n",
        "gc.collect() # Force Python garbage collection\n",
        "\n",
        "print(\"2. Reading data into Spark...\")\n",
        "spark_df = spark.read.parquet(temp_file)\n",
        "\n",
        "print(\"3. Aggregating by Job Title...\")\n",
        "df_grouped = spark_df.groupBy(\"job_title\").agg(\n",
        "    F.flatten(F.collect_list(\"skills_list\")).alias(\"all_skills_tokens\")\n",
        ")\n",
        "\n",
        "print(f\"Aggregated into {df_grouped.count():,} unique job titles.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PIPELINE SETUP (Feature Engineering + Model)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Step A: CountVectorizer (calculates Term Frequency - TF)\n",
        "# We use CountVectorizer instead of HashingTF so we can retrieve the actual words later\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"all_skills_tokens\", \n",
        "    outputCol=\"raw_features\", \n",
        "    vocabSize=2000, \n",
        "    minDF=2.0  # Ignore skills that appear in fewer than 2 docs\n",
        ")\n",
        "\n",
        "# Step B: IDF (Inverse Document Frequency)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Step C: KMeans\n",
        "# Note: maxBlockSizeInMB helps prevent memory errors during linear algebra ops\n",
        "kmeans = KMeans(\n",
        "    k=5, \n",
        "    seed=42, \n",
        "    featuresCol=\"features\", \n",
        "    predictionCol=\"cluster\",\n",
        "    maxBlockSizeInMB=256\n",
        ")\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[cv, idf, kmeans])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TRAINING\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Training Pipeline (CV -> IDF -> KMeans)...\")\n",
        "model = pipeline.fit(df_grouped)\n",
        "predictions = model.transform(df_grouped)\n",
        "\n",
        "print(f\"‚úÖ Pipeline finished in {time.time()-prep_start:.1f}s\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ANALYSIS: Cluster Distribution & Top Skills\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTER ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Show Cluster Counts\n",
        "print(\"Cluster Distribution:\")\n",
        "predictions.groupBy(\"cluster\").count().orderBy(\"cluster\").show()\n",
        "\n",
        "# 2. Extract Vocabulary (to map features back to skill names)\n",
        "cv_model = model.stages[0] # The fitted CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "\n",
        "# 3. Extract Cluster Centers\n",
        "kmeans_model = model.stages[2] # The fitted KMeans model\n",
        "centers = kmeans_model.clusterCenters()\n",
        "\n",
        "print(\"Top 5 Skills per Cluster (based on center centroids):\")\n",
        "\n",
        "for i, center in enumerate(centers):\n",
        "    # 'center' is a dense vector of weights. \n",
        "    # We need the indices of the highest weights.\n",
        "    \n",
        "    # argsort returns indices of sorted elements, we take last 5 (highest) and reverse\n",
        "    top_indices = center.argsort()[-5:][::-1]\n",
        "    \n",
        "    # Map indices to words using the vocabulary\n",
        "    top_skills = [vocab[idx] for idx in top_indices]\n",
        "    \n",
        "    # Get a sample job title for context\n",
        "    sample = predictions.filter(F.col(\"cluster\") == i).select(\"job_title\").first()\n",
        "    sample_title = sample['job_title'] if sample else \"N/A\"\n",
        "    \n",
        "    print(f\"--- Cluster {i} (e.g., {sample_title}) ---\")\n",
        "    print(f\"Skills: {top_skills}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CLEANUP\n",
        "# -----------------------------------------------------------------------------\n",
        "# If you need to move back to Pandas for visualization (ONLY do this with the results, not the vectors)\n",
        "# final_df_pandas = predictions.select(\"job_title\", \"cluster\").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hG2M8wdVNal"
      },
      "source": [
        "Goal 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "5h75ILxw31yA",
        "outputId": "eaa0e674-384d-468d-a86d-29457ff096ac"
      },
      "outputs": [],
      "source": [
        "# df_clean = df_pandas[['job_title', 'skills_list', 'job_level']].dropna() # Added job_level\n",
        "# df_clean.to_parquet(\"temp_job_skills_level.parquet\", index=False)\n",
        "# del df_clean\n",
        "# del df_pandas\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# spark_df = spark.read.parquet(\"temp_job_skills_level.parquet\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 5: SKILL COUNT VS. JOB LEVEL ANALYSIS (Safe Version)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Feature Engineering: Create 'skill_count' column\n",
        "# We use F.size and F.col to ensure we are using Spark functions\n",
        "df_analysis = spark_df.withColumn(\"skill_count\", F.size(F.col(\"skills_list\")))\n",
        "\n",
        "# 2. Calculate General Statistics\n",
        "print(\"\\nüìä Skill Count Distribution (Summary):\")\n",
        "df_analysis.select(\"skill_count\").describe().show()\n",
        "\n",
        "# 3. Aggregation by Job Level\n",
        "print(\"Aggregating data by Job Level...\")\n",
        "\n",
        "# We use F.avg, F.count, and F.desc to avoid the \"int64 not callable\" error\n",
        "skills_by_level_spark = df_analysis.groupBy(\"job_level\") \\\n",
        "    .agg(\n",
        "        F.avg(\"skill_count\").alias(\"avg_skills\"),\n",
        "        F.count(\"*\").alias(\"job_count\")\n",
        "    ) \\\n",
        "    .orderBy(F.desc(\"avg_skills\"))\n",
        "\n",
        "# 4. Convert ONLY the summary to Pandas for visualization\n",
        "pdf_skills_level = skills_by_level_spark.toPandas()\n",
        "\n",
        "print(\"\\nüìà Average Skills by Job Level (Top 10):\")\n",
        "print(pdf_skills_level.head(10).to_string(index=False))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# VISUALIZATION\n",
        "# -----------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Filter and sort\n",
        "plot_data = pdf_skills_level.head(10)\n",
        "\n",
        "plt.bar(range(len(plot_data)), plot_data['avg_skills'], color='skyblue', edgecolor='black')\n",
        "\n",
        "# Add count labels\n",
        "for i, row in plot_data.iterrows():\n",
        "    plt.text(i, row['avg_skills'] + 0.5, f\"n={row['job_count']}\", \n",
        "             ha='center', fontsize=9, color='gray')\n",
        "\n",
        "plt.xticks(range(len(plot_data)), plot_data['job_level'], rotation=45, ha='right')\n",
        "plt.ylabel('Average Number of Skills')\n",
        "plt.xlabel('Job Level')\n",
        "plt.title('Average Skills Required by Job Level', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Goal 5 completed in {time.time()-start_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA4NqgooVrPp"
      },
      "source": [
        "Goal 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDLORODJVq_k"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 6: PREDICTIVE MODELING (PySpark Logistic Regression)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_start = time.time()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. DATA PREPARATION\n",
        "# -----------------------------------------------------------------------------\n",
        "# Ensure we are working with the raw data containing 'job_level' and 'skills_list'\n",
        "# If you lost spark_df, reload it from the parquet file we made earlier:\n",
        "# spark_df = spark.read.parquet(\"temp_job_skills_level.parquet\")\n",
        "\n",
        "# Filter out null job levels before training\n",
        "df_modeling = spark_df.filter(F.col(\"job_level\").isNotNull())\n",
        "\n",
        "# Calculate 'skill_count' if it's missing (we did this in Goal 5, but just in case)\n",
        "df_modeling = df_modeling.withColumn(\"skill_count\", F.size(F.col(\"skills_list\")))\n",
        "\n",
        "print(f\"Data ready for modeling. Total rows: {df_modeling.count():,}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. DEFINE THE ML PIPELINE STAGES\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Stage A: Convert Target String ('job_level') to Numeric Index ('label')\n",
        "# e.g., \"Entry Level\" -> 0.0, \"Senior\" -> 1.0\n",
        "label_indexer = StringIndexer(\n",
        "    inputCol=\"job_level\", \n",
        "    outputCol=\"label\", \n",
        "    handleInvalid=\"skip\" # Remove rows with unseen labels\n",
        ")\n",
        "\n",
        "# Stage B: TF-IDF Featurization (Re-doing this for the rows, not just titles)\n",
        "# 1. Count Frequencies\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"skills_list\", \n",
        "    outputCol=\"raw_features\", \n",
        "    vocabSize=2000, \n",
        "    minDF=5.0 # Ignore very rare skills to save memory\n",
        ")\n",
        "# 2. Calculate IDF\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "# Stage C: Assemble Features\n",
        "# Combine [skill_count] and [tfidf_features] into one vector called \"features\"\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"tfidf_features\", \"skill_count\"], \n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Stage D: Logistic Regression Model\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\", \n",
        "    labelCol=\"label\", \n",
        "    maxIter=20, # 20 iterations is usually enough for large data; increase if needed\n",
        "    family=\"multinomial\"\n",
        ")\n",
        "\n",
        "# Build the Pipeline\n",
        "# Note: We don't fit yet!\n",
        "pipeline = Pipeline(stages=[label_indexer, cv, idf, assembler, lr])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. SPLIT DATA & TRAIN\n",
        "# -----------------------------------------------------------------------------\n",
        "# Split 70/30\n",
        "train_data, test_data = df_modeling.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "print(f\"Training on approx {train_data.count():,} rows...\")\n",
        "print(f\"Testing on approx {test_data.count():,} rows...\")\n",
        "\n",
        "# Fit the entire pipeline on training data\n",
        "model = pipeline.fit(train_data)\n",
        "print(\"‚úÖ Model training complete.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. EVALUATION\n",
        "# -----------------------------------------------------------------------------\n",
        "# Make predictions on test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Initialize Evaluators\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
        ")\n",
        "\n",
        "# Calculate Metrics\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "f1_score = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"\\nModel Evaluation (Logistic Regression):\")\n",
        "print(f\"Accuracy on Test Data: **{accuracy:.4f}**\")\n",
        "print(f\"Weighted F1 Score:     **{f1_score:.4f}**\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# SHOW PREDICTION SAMPLES\n",
        "# -----------------------------------------------------------------------------\n",
        "# To interpret the numeric labels, we get the labels from the indexer model\n",
        "indexer_model = model.stages[0]\n",
        "labels = indexer_model.labels\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "# We verify what the model thought vs reality\n",
        "predictions.select(\"job_level\", \"label\", \"prediction\", \"probability\") \\\n",
        "    .sample(False, 0.1, seed=42) \\\n",
        "    .show(5, truncate=False)\n",
        "\n",
        "print(f\"Mapping: {dict(enumerate(labels))}\")\n",
        "print(f\"‚úÖ Goal 6 completed in {time.time()-train_start:.1f}s\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# FINAL SUMMARY\n",
        "# -----------------------------------------------------------------------------\n",
        "# Since we can't assume 'start_time' from Goal 2 is still valid in this session:\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ ALL ADVANCED ANALYTICS COMPLETE (Spark Version)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LwOTn3731yA"
      },
      "source": [
        "## Section 7: Machine Learning - Classification (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "4vtwSJzT31yA",
        "outputId": "dcb740f1-c2fd-4529-846e-6e6ce9042205"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - CLASSIFICATION (Revised)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Predict if a job is 'High Market Demand' (High Frequency)\")\n",
        "print(\"    (Proxy: Job Titles that appear more often than the median)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. FEATURE ENGINEERING\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüîß Feature engineering...\")\n",
        "\n",
        "# Ensure we have the base data (using spark_df from previous steps)\n",
        "# We ensure skill_count exists\n",
        "df_prep = spark_df.withColumn(\"skill_count\", F.size(F.col(\"skills_list\")))\n",
        "\n",
        "# A. Calculate Job Title Frequency (How often does this job appear?)\n",
        "title_counts = df_prep.groupBy(\"job_title\").count().withColumnRenamed(\"count\", \"title_freq\")\n",
        "\n",
        "# Join frequency back to main data\n",
        "df_ml = df_prep.join(title_counts, on=\"job_title\", how=\"left\")\n",
        "\n",
        "# B. Define Target Variable: \"High Demand\"\n",
        "# Calculate Median Frequency\n",
        "median_freq = df_ml.approxQuantile(\"title_freq\", [0.5], 0.01)[0]\n",
        "print(f\"   Median Job Title Frequency: {median_freq}\")\n",
        "\n",
        "# Create Binary Label: 1 if freq > median, else 0\n",
        "df_ml = df_ml.withColumn(\"high_demand\", F.when(F.col(\"title_freq\") > median_freq, 1.0).otherwise(0.0))\n",
        "\n",
        "# C. Clean Data for Modeling\n",
        "# We need to handle 'job_level'. If it's null, fill with \"Unknown\"\n",
        "df_ml = df_ml.fillna(\"Unknown\", subset=[\"job_level\"])\n",
        "df_ml = df_ml.filter(F.col(\"skill_count\").isNotNull())\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. SPLIT DATA\n",
        "# -----------------------------------------------------------------------------\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"\\nüìä Dataset split:\")\n",
        "print(f\"   Training: {train_df.count():,}\")\n",
        "print(f\"   Testing:  {test_df.count():,}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. BUILD PIPELINE\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüèóÔ∏è Building classification pipeline...\")\n",
        "\n",
        "# Stage 1: Convert 'job_level' (String) to Index (Numeric)\n",
        "# e.g., \"Mid Level\" -> 0.0, \"Senior\" -> 1.0\n",
        "indexer = StringIndexer(\n",
        "    inputCol=\"job_level\", \n",
        "    outputCol=\"job_level_index\", \n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "# Stage 2: Assemble Features\n",
        "# We use 'skill_count' and 'job_level_index' to predict demand\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['skill_count', 'job_level_index'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "# Stage 3: Scale Features (Important for Logistic Regression)\n",
        "scaler = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "# Stage 4: Logistic Regression\n",
        "lr = LogisticRegression(\n",
        "    featuresCol='scaled_features',\n",
        "    labelCol='high_demand',\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, assembler, scaler, lr])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. TRAIN & EVALUATE\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüîÑ Training model...\")\n",
        "start = time.time()\n",
        "model = pipeline.fit(train_df)\n",
        "print(f\"‚úÖ Model trained in {time.time()-start:.1f}s\")\n",
        "\n",
        "print(\"\\nüìà Evaluating model...\")\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Metrics\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol='high_demand',\n",
        "    metricName='areaUnderROC'\n",
        ")\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(\n",
        "    labelCol='high_demand',\n",
        "    predictionCol='prediction',\n",
        "    metricName='accuracy'\n",
        ")\n",
        "\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "\n",
        "print(\"\\nüéØ Classification Results:\")\n",
        "print(f\"   AUC-ROC:  {auc:.4f}\")\n",
        "print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Interpret Weights (Optional)\n",
        "print(\"\\nüß† Model Insights:\")\n",
        "lr_model = model.stages[-1]\n",
        "print(f\"   Coefficients: {lr_model.coefficients}\")\n",
        "print(f\"   (Feature 0 is Skill Count, Feature 1 is Job Level)\")\n",
        "\n",
        "print(\"\\n‚úÖ Classification complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSWffs9Y31yA"
      },
      "source": [
        "## Section 8: Machine Learning - Regression (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RQZFtpO31yA"
      },
      "outputs": [],
      "source": [
        "# ML Problem 2: Regression - Predict Number of Skills\n",
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - REGRESSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Predict skill_count based on job characteristics\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Prepare regression dataset\n",
        "print(\"\\nüîß Preparing features...\")\n",
        "\n",
        "# Index categorical variables\n",
        "job_level_indexer = StringIndexer(\n",
        "    inputCol='job_level',\n",
        "    outputCol='job_level_idx',\n",
        "    handleInvalid='keep'\n",
        ")\n",
        "\n",
        "emp_type_indexer = StringIndexer(\n",
        "    inputCol='employment_type',\n",
        "    outputCol='emp_type_idx',\n",
        "    handleInvalid='keep'\n",
        ")\n",
        "\n",
        "# Create regression dataset\n",
        "df_reg = df_final.filter(\n",
        "    col('skill_count').isNotNull() &\n",
        "    col('job_level').isNotNull() &\n",
        "    col('employment_type').isNotNull()\n",
        ").select(\n",
        "    'job_level',\n",
        "    'employment_type',\n",
        "    'skill_count'\n",
        ")\n",
        "\n",
        "# Sample if needed\n",
        "reg_count = df_reg.count()\n",
        "if reg_count > 100000:\n",
        "    print(f\"\\n‚ö†Ô∏è Sampling for efficiency ({reg_count:,} -> 100k records)\")\n",
        "    df_reg = df_reg.sample(False, 100000/reg_count, seed=42)\n",
        "\n",
        "# Split data\n",
        "train_reg, test_reg = df_reg.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"\\nüìä Dataset split:\")\n",
        "print(f\"   Training: {train_reg.count():,}\")\n",
        "print(f\"   Testing: {test_reg.count():,}\")\n",
        "\n",
        "# Build regression pipeline\n",
        "print(\"\\nüèóÔ∏è Building regression model...\")\n",
        "\n",
        "assembler_reg = VectorAssembler(\n",
        "    inputCols=['job_level_idx', 'emp_type_idx'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "scaler_reg = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "lr_reg = LinearRegression(\n",
        "    featuresCol='scaled_features',\n",
        "    labelCol='skill_count',\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "pipeline_reg = Pipeline(stages=[\n",
        "    job_level_indexer,\n",
        "    emp_type_indexer,\n",
        "    assembler_reg,\n",
        "    scaler_reg,\n",
        "    lr_reg\n",
        "])\n",
        "\n",
        "# Train\n",
        "print(\"\\nüîÑ Training regression model...\")\n",
        "start = time.time()\n",
        "model_reg = pipeline_reg.fit(train_reg)\n",
        "print(f\"‚úÖ Model trained in {time.time()-start:.1f}s\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nüìà Evaluating model...\")\n",
        "predictions_reg = model_reg.transform(test_reg)\n",
        "\n",
        "# Metrics\n",
        "evaluator_rmse = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='rmse'\n",
        ")\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='r2'\n",
        ")\n",
        "\n",
        "evaluator_mae = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='mae'\n",
        ")\n",
        "\n",
        "rmse = evaluator_rmse.evaluate(predictions_reg)\n",
        "r2 = evaluator_r2.evaluate(predictions_reg)\n",
        "mae = evaluator_mae.evaluate(predictions_reg)\n",
        "\n",
        "print(\"\\nüéØ Regression Results:\")\n",
        "print(f\"   RMSE: {rmse:.4f}\")\n",
        "print(f\"   R¬≤: {r2:.4f}\")\n",
        "print(f\"   MAE: {mae:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Regression complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRp0JEuF31yA"
      },
      "source": [
        "## Section 9: Machine Learning - Clustering (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9njwOB531yB"
      },
      "outputs": [],
      "source": [
        "# ML Problem 3: K-Means Clustering\n",
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - K-MEANS CLUSTERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Cluster jobs based on skill patterns\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Prepare clustering dataset\n",
        "print(\"\\nüîß Preparing features for clustering...\")\n",
        "\n",
        "df_cluster = df_final.filter(\n",
        "    col('skill_count').isNotNull() \n",
        ").select(\n",
        "    'job_link',\n",
        "    'job_title',\n",
        "    'skill_count',\n",
        ").fillna(0)\n",
        "\n",
        "# Sample for efficiency\n",
        "cluster_count = df_cluster.count()\n",
        "if cluster_count > 100000:\n",
        "    print(f\"\\n‚ö†Ô∏è Sampling for clustering ({cluster_count:,} -> 50k records)\")\n",
        "    df_cluster = df_cluster.sample(False, 100000/cluster_count, seed=42)\n",
        "\n",
        "# Build clustering pipeline\n",
        "print(\"\\nüèóÔ∏è Building K-Means model...\")\n",
        "\n",
        "assembler_cluster = VectorAssembler(\n",
        "    inputCols=['skill_count'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "scaler_cluster = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "# Try different K values\n",
        "print(\"\\nüîç Finding optimal K...\")\n",
        "silhouette_scores = []\n",
        "k_values = [3, 5, 7, 10]\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(k=k, featuresCol='scaled_features', seed=42)\n",
        "    pipeline_cluster = Pipeline(stages=[assembler_cluster, scaler_cluster, kmeans])\n",
        "\n",
        "    model_cluster = pipeline_cluster.fit(df_cluster)\n",
        "    predictions_cluster = model_cluster.transform(df_cluster)\n",
        "\n",
        "    evaluator = ClusteringEvaluator(featuresCol='scaled_features')\n",
        "    score = evaluator.evaluate(predictions_cluster)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "    print(f\"   K={k}: Silhouette Score = {score:.4f}\")\n",
        "\n",
        "# Find optimal K\n",
        "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
        "print(f\"\\n‚úÖ Optimal K: {optimal_k}\")\n",
        "\n",
        "# Final clustering with optimal K\n",
        "print(f\"\\nüîÑ Training final K-Means with K={optimal_k}...\")\n",
        "kmeans_final = KMeans(k=optimal_k, featuresCol='scaled_features', seed=42)\n",
        "pipeline_final = Pipeline(stages=[assembler_cluster, scaler_cluster, kmeans_final])\n",
        "\n",
        "model_final = pipeline_final.fit(df_cluster)\n",
        "predictions_final = model_final.transform(df_cluster)\n",
        "\n",
        "# Cluster distribution\n",
        "print(\"\\nüìä Cluster distribution:\")\n",
        "cluster_dist = predictions_final.groupBy('prediction').count().orderBy('prediction').toPandas()\n",
        "print(cluster_dist.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(cluster_dist['prediction'], cluster_dist['count'])\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Number of Jobs')\n",
        "plt.title(f'K-Means Clustering Results (K={optimal_k})', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ K-Means clustering complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSyXgets31yB"
      },
      "source": [
        "## Section 10: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUF6DrDi31yB"
      },
      "outputs": [],
      "source": [
        "# Save key results\n",
        "print(\"=\"*70)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# # Save top skills (No need already saved)\n",
        "# print(\"\\nüíæ Saving top skills...\")\n",
        "# top_skills_pd.to_csv('top_skills_2024.csv', index=False)\n",
        "# print(\"   ‚úÖ Saved: top_skills_2024.csv\")\n",
        "\n",
        "# Save ML results summary\n",
        "ml_results = pd.DataFrame({\n",
        "    'Metric': ['AUC-ROC', 'Accuracy', 'RMSE', 'R¬≤', 'MAE', 'Optimal_K'],\n",
        "    'Value': [auc, accuracy, rmse, r2, mae, optimal_k]\n",
        "})\n",
        "\n",
        "print(\"\\nüíæ Saving ML results...\")\n",
        "ml_results.to_csv('ml_results_summary.csv', index=False)\n",
        "print(\"   ‚úÖ Saved: ml_results_summary.csv\")\n",
        "\n",
        "print(\"\\nüìä Results Summary:\")\n",
        "print(ml_results.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL RESULTS SAVED\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E273BDc31yB"
      },
      "source": [
        "## Section 11: Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llen7hiL31yB"
      },
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "print(\"=\"*70)\n",
        "print(\"CLEANUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Unpersist cached DataFrames\n",
        "print(\"\\nüßπ Clearing cached data...\")\n",
        "try:\n",
        "    df_postings_clean.unpersist()\n",
        "    df_work.unpersist()\n",
        "    df_skills_agg.unpersist()\n",
        "    df_final.unpersist()\n",
        "    print(\"‚úÖ Cache cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úÖ Data loaded and cleaned\")\n",
        "print(\"‚úÖ EDA completed\")\n",
        "print(\"‚úÖ Machine learning models trained\")\n",
        "print(\"‚úÖ Results saved\")\n",
        "print(\"\\nReady for Phase 2 report!\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
