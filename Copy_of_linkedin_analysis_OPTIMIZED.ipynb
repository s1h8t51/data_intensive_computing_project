{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1h8t51/data_intensive_computing_project/blob/main/Copy_of_linkedin_analysis_OPTIMIZED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-YUtmb31x5"
      },
      "source": [
        "# Analyzing Global Job Market Trends and Skill Demands Using Big Data\n",
        "## A LinkedIn Jobs & Skills 2024 Study - Phase 2 (OPTIMIZED)\n",
        "\n",
        "**Team Members:**\n",
        "- Sahitya Gantala (sahityag@buffalo.edu)\n",
        "- Shilpa Ghosh (shilpagh@buffalo.edu)\n",
        "- Aditya Rajesh Sawant (asawant5@buffalo.edu)\n",
        "\n",
        "**Dataset:** 1.3M LinkedIn Jobs and Skills (2024)\n",
        "\n",
        "**Course:** CSE 587 - Data Intensive Computing, Fall 2025\n",
        "\n",
        "**Optimizations:**\n",
        "- Fixed PySpark memory errors\n",
        "- Improved deduplication strategy\n",
        "- Added error handling and recovery\n",
        "- Memory-efficient data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ydzmGo331x6"
      },
      "source": [
        "## Section 1: Environment Setup and Spark Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVQTKsN_31x7",
        "outputId": "1d7d44fa-56d9-4118-b6cc-6c845288af44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark pandas matplotlib seaborn scikit-learn wordcloud kaggle -q\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1GeWPbs31x8",
        "outputId": "09f6247b-3bdf-4726-b5e9-d099fbc09e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ No existing Spark session\n"
          ]
        }
      ],
      "source": [
        "# Stop any existing Spark sessions\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"‚ö†Ô∏è Stopped existing Spark session\")\n",
        "except:\n",
        "    print(\"‚úÖ No existing Spark session\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79cFwrrE31x8",
        "outputId": "83e1f229-b105-4786-fb9f-081586c55c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.functions import (\n",
        "    col, lower, trim, split, size, explode, count, avg, desc, asc,\n",
        "    collect_list, array_distinct, concat_ws, regexp_replace, when,\n",
        "    countDistinct, sum as spark_sum, dense_rank, row_number\n",
        ")\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.evaluation import (\n",
        "    MulticlassClassificationEvaluator,\n",
        "    RegressionEvaluator,\n",
        "    BinaryClassificationEvaluator\n",
        ")\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9-svPrl31x8",
        "outputId": "6dc70195-c3c3-4d45-8e1b-1a490025826e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "OPTIMIZED SPARK SESSION INITIALIZED\n",
            "======================================================================\n",
            "‚úÖ Spark Version: 3.5.1\n",
            "üìä Driver Memory: 12g\n",
            "üîß Shuffle Partitions: 100\n",
            "üíæ Memory Fraction: 0.8\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Configure Spark Session with OPTIMIZED settings for memory efficiency\n",
        "conf = SparkConf() \\\n",
        "    .setAppName('LinkedIn_Jobs_Analysis_Phase2_OPTIMIZED') \\\n",
        "    .setMaster('local[*]') \\\n",
        "    .set('spark.driver.memory', '12g') \\\n",
        "    .set('spark.driver.maxResultSize', '3g') \\\n",
        "    .set('spark.executor.memory', '4g') \\\n",
        "    .set('spark.sql.shuffle.partitions', '100') \\\n",
        "    .set('spark.default.parallelism', '100') \\\n",
        "    .set('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n",
        "    .set('spark.sql.adaptive.skewJoin.enabled', 'true') \\\n",
        "    .set('spark.memory.fraction', '0.8') \\\n",
        "    .set('spark.memory.storageFraction', '0.3')\n",
        "\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")  # Reduce verbosity\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"OPTIMIZED SPARK SESSION INITIALIZED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Driver Memory: {spark.sparkContext._conf.get('spark.driver.memory')}\")\n",
        "print(f\"üîß Shuffle Partitions: {spark.sparkContext._conf.get('spark.sql.shuffle.partitions')}\")\n",
        "print(f\"üíæ Memory Fraction: {spark.sparkContext._conf.get('spark.memory.fraction')}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YfbkgWn31x9"
      },
      "source": [
        "## Section 2: Kaggle Setup and Data Download (FIXED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LB3NqdW8O4Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1udYouL31x9",
        "outputId": "bfee584b-77ed-48f4-bc86-10dd316f2dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "KAGGLE CREDENTIALS CHECK\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è Kaggle credentials not found!\n",
            "\n",
            "Please enter your Kaggle credentials:\n",
            "(Get them from: https://www.kaggle.com/settings/account)\n",
            "\n",
            "Kaggle Username: sahityagantalausa\n",
            "Kaggle API Key: bf2ff23fa341b8b34e596aa5409cee0d\n",
            "\n",
            "‚úÖ Credentials saved!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Kaggle credentials setup\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"KAGGLE CREDENTIALS CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if credentials exist\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_json = kaggle_dir / \"kaggle.json\"\n",
        "\n",
        "if not kaggle_json.exists():\n",
        "    print(\"\\n‚ö†Ô∏è Kaggle credentials not found!\")\n",
        "    print(\"\\nPlease enter your Kaggle credentials:\")\n",
        "    print(\"(Get them from: https://www.kaggle.com/settings/account)\\n\")\n",
        "\n",
        "    username = input(\"Kaggle Username: \").strip()\n",
        "    key = input(\"Kaggle API Key: \").strip()\n",
        "\n",
        "    if username and key:\n",
        "        # Create directory and save credentials\n",
        "        kaggle_dir.mkdir(exist_ok=True)\n",
        "        with open(kaggle_json, 'w') as f:\n",
        "            json.dump({\"username\": username, \"key\": key}, f, indent=2)\n",
        "\n",
        "        os.chmod(kaggle_json, 0o600)\n",
        "        print(\"\\n‚úÖ Credentials saved!\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Invalid credentials. Please run this cell again.\")\n",
        "else:\n",
        "    print(\"‚úÖ Kaggle credentials found\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFygoMel31x9",
        "outputId": "10e68ae3-3946-4c8f-8a65-aeae71b4bf03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA DOWNLOAD AND EXTRACTION\n",
            "======================================================================\n",
            "\n",
            "üì• Downloading dataset...\n",
            "(This may take several minutes)\n",
            "\n",
            "‚úÖ Downloaded in 34.9s\n",
            "\n",
            "üì¶ Extracting files...\n",
            "   Found 3 files\n",
            "‚úÖ Extracted in 70.8s\n",
            "üóëÔ∏è  Cleaned up zip file\n",
            "\n",
            "üìÇ Dataset files:\n",
            "total 5.8G\n",
            "-rw-r--r-- 1 root root 642M Nov 19 03:51 job_skills.csv\n",
            "-rw-r--r-- 1 root root 4.8G Nov 19 03:52 job_summary.csv\n",
            "-rw-r--r-- 1 root root 397M Nov 19 03:52 linkedin_job_postings.csv\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Robust data download with error handling\n",
        "import zipfile\n",
        "\n",
        "DATASET_PATH = \"asaniczka/1-3m-linkedin-jobs-and-skills-2024\"\n",
        "EXTRACT_DIR = \"./linkedin_dataset\"\n",
        "ZIP_FILE = \"1-3m-linkedin-jobs-and-skills-2024.zip\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA DOWNLOAD AND EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if data already exists\n",
        "if os.path.exists(EXTRACT_DIR) and os.listdir(EXTRACT_DIR):\n",
        "    print(\"\\n‚úÖ Dataset already exists!\")\n",
        "    print(f\"üìÇ Location: {EXTRACT_DIR}\")\n",
        "    !ls -lh {EXTRACT_DIR}\n",
        "else:\n",
        "    # Download dataset\n",
        "    print(\"\\nüì• Downloading dataset...\")\n",
        "    print(\"(This may take several minutes)\")\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        result = !kaggle datasets download -d {DATASET_PATH} 2>&1\n",
        "\n",
        "        # Check if download was successful\n",
        "        if not os.path.exists(ZIP_FILE):\n",
        "            print(\"\\n‚ùå Download failed!\")\n",
        "            print(\"\\nTroubleshooting steps:\")\n",
        "            print(\"1. Visit: https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024\")\n",
        "            print(\"2. Click 'Download' to accept terms\")\n",
        "            print(\"3. Re-run this cell\")\n",
        "            raise Exception(\"Dataset download failed\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Downloaded in {time.time()-start:.1f}s\")\n",
        "\n",
        "        # Extract files\n",
        "        print(\"\\nüì¶ Extracting files...\")\n",
        "        start = time.time()\n",
        "\n",
        "        os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "        with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "            files = zip_ref.namelist()\n",
        "            print(f\"   Found {len(files)} files\")\n",
        "            zip_ref.extractall(EXTRACT_DIR)\n",
        "\n",
        "        print(f\"‚úÖ Extracted in {time.time()-start:.1f}s\")\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(ZIP_FILE)\n",
        "        print(\"üóëÔ∏è  Cleaned up zip file\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {e}\")\n",
        "        raise\n",
        "\n",
        "# Show dataset files\n",
        "print(\"\\nüìÇ Dataset files:\")\n",
        "!ls -lh {EXTRACT_DIR}\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ALaDXJ31x9"
      },
      "source": [
        "## Section 3: Data Loading and Cleaning (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# Section 3: Data Loading (CORRECTED FOR ACTUAL FILES)\n",
        "# =====================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA LOADING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# File 1: Job Postings (396 MB)\n",
        "print(\"\\nüìÇ Loading job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "df_postings = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/linkedin_job_postings.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False,\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ").repartition(100)\n",
        "\n",
        "initial_count = df_postings.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {initial_count:,}\")\n",
        "print(f\"   Columns: {len(df_postings.columns)}\")\n",
        "\n",
        "# File 2: Job Skills (641 MB)\n",
        "print(\"\\nüìÇ Loading skills data...\")\n",
        "start = time.time()\n",
        "\n",
        "df_skills = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/job_skills.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False\n",
        ").repartition(100)\n",
        "\n",
        "skills_count = df_skills.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {skills_count:,}\")\n",
        "\n",
        "# File 3: Job Summary (4.8 GB - VERY LARGE!)\n",
        "print(\"\\nüìÇ Loading job summary...\")\n",
        "print(\"   ‚ö†Ô∏è  WARNING: Large file (4.8 GB) - this may take 2-3 minutes\")\n",
        "start = time.time()\n",
        "\n",
        "df_summary = spark.read.csv(\n",
        "    f\"{EXTRACT_DIR}/job_summary.csv\",\n",
        "    header=True,\n",
        "    inferSchema=False,\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ").repartition(200)  # More partitions for large file\n",
        "\n",
        "summary_count = df_summary.count()\n",
        "print(f\"‚úÖ Loaded in {time.time()-start:.1f}s\")\n",
        "print(f\"   Records: {summary_count:,}\")\n",
        "print(f\"   Columns: {len(df_summary.columns)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL DATA LOADED SUCCESSFULLY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"   ‚Ä¢ Job Postings: {initial_count:,} records\")\n",
        "print(f\"   ‚Ä¢ Skills: {skills_count:,} records\")\n",
        "print(f\"   ‚Ä¢ Summary: {summary_count:,} records\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxLIuJ7z5mlG",
        "outputId": "d76bbc17-1b25-47df-da4d-e65f65da2229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA LOADING\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading job postings...\n",
            "‚úÖ Loaded in 17.5s\n",
            "   Records: 1,348,454\n",
            "   Columns: 14\n",
            "\n",
            "üìÇ Loading skills data...\n",
            "‚úÖ Loaded in 15.1s\n",
            "   Records: 1,296,381\n",
            "\n",
            "üìÇ Loading job summary...\n",
            "   ‚ö†Ô∏è  WARNING: Large file (4.8 GB) - this may take 2-3 minutes\n",
            "‚úÖ Loaded in 79.5s\n",
            "   Records: 1,297,332\n",
            "   Columns: 2\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL DATA LOADED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "Dataset Summary:\n",
            "   ‚Ä¢ Job Postings: 1,348,454 records\n",
            "   ‚Ä¢ Skills: 1,296,381 records\n",
            "   ‚Ä¢ Summary: 1,297,332 records\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjDH675A31x-",
        "outputId": "5b2a9762-574a-42e1-900a-6e3f70c959e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA DEDUPLICATION (OPTIMIZED)\n",
            "======================================================================\n",
            "\n",
            "üîç Removing duplicate job postings...\n",
            "\n",
            "‚úÖ Deduplication complete in 82.7s\n",
            "   Initial records: 1,348,454\n",
            "   Final records: 1,348,454\n",
            "   Duplicates removed: 0\n",
            "   Retention rate: 100.0%\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Memory-efficient deduplication\n",
        "print(\"=\"*70)\n",
        "print(\"DATA DEDUPLICATION (OPTIMIZED)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîç Removing duplicate job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "try:\n",
        "    # Method 1: Direct deduplication without intermediate counts\n",
        "    df_postings_clean = df_postings.dropDuplicates(['job_link']) \\\n",
        "        .repartition(100)\n",
        "\n",
        "    # Cache for future operations\n",
        "    df_postings_clean.cache()\n",
        "\n",
        "    # Get count\n",
        "    final_count = df_postings_clean.count()\n",
        "    duplicates_removed = initial_count - final_count\n",
        "\n",
        "    print(f\"\\n‚úÖ Deduplication complete in {time.time()-start:.1f}s\")\n",
        "    print(f\"   Initial records: {initial_count:,}\")\n",
        "    print(f\"   Final records: {final_count:,}\")\n",
        "    print(f\"   Duplicates removed: {duplicates_removed:,}\")\n",
        "    print(f\"   Retention rate: {final_count/initial_count*100:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Standard deduplication failed: {e}\")\n",
        "    print(\"\\nüîÑ Trying alternative method with sampling...\")\n",
        "\n",
        "    # Alternative: Sample-based deduplication for very large datasets\n",
        "    sample_fraction = 0.1\n",
        "    df_sample = df_postings.sample(False, sample_fraction, seed=42)\n",
        "\n",
        "    # Get approximate duplicate ratio from sample\n",
        "    sample_initial = df_sample.count()\n",
        "    sample_clean = df_sample.dropDuplicates(['job_link']).count()\n",
        "    dup_ratio = (sample_initial - sample_clean) / sample_initial\n",
        "\n",
        "    print(f\"\\nüìä Sample analysis (10%):\")\n",
        "    print(f\"   Sample duplicates: {dup_ratio*100:.1f}%\")\n",
        "    print(f\"   Estimated full duplicates: {int(initial_count * dup_ratio):,}\")\n",
        "\n",
        "    # Apply deduplication with lower memory pressure\n",
        "    df_postings_clean = df_postings \\\n",
        "        .repartition(200, 'job_link') \\\n",
        "        .dropDuplicates(['job_link']) \\\n",
        "        .coalesce(100)\n",
        "\n",
        "    df_postings_clean.cache()\n",
        "    final_count = df_postings_clean.count()\n",
        "\n",
        "    print(f\"\\n‚úÖ Alternative deduplication successful\")\n",
        "    print(f\"   Final records: {final_count:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F6_8ADn31x-",
        "outputId": "d3a314ae-3c17-4532-c73a-0318e44fa6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA QUALITY CHECKS\n",
            "======================================================================\n",
            "\n",
            "üìä Schema:\n",
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: string (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: string (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            "\n",
            "\n",
            "üìà Missing values:\n",
            "              Missing %\n",
            "job_location   0.001409\n",
            "company        0.000816\n",
            "\n",
            "‚úÖ Data quality check complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Basic data quality checks\n",
        "print(\"=\"*70)\n",
        "print(\"DATA QUALITY CHECKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Schema:\")\n",
        "df_postings_clean.printSchema()\n",
        "\n",
        "print(\"\\nüìà Missing values:\")\n",
        "null_counts = df_postings_clean.select(\n",
        "    [count(when(col(c).isNull(), c)).alias(c) for c in df_postings_clean.columns]\n",
        ").toPandas()\n",
        "\n",
        "null_pct = (null_counts / final_count * 100).T\n",
        "null_pct.columns = ['Missing %']\n",
        "print(null_pct[null_pct['Missing %'] > 0].sort_values('Missing %', ascending=False).head(10))\n",
        "\n",
        "print(\"\\n‚úÖ Data quality check complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPFI37eZ31x_"
      },
      "source": [
        "## Section 4: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jq_FyWM31x_",
        "outputId": "cfdc833a-f859-4331-e3d9-29e28ba2fa9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA PREPROCESSING\n",
            "======================================================================\n",
            "\n",
            "üîß Creating working dataset...\n",
            "‚úÖ Working dataset ready\n",
            "   Records: 1,348,454\n",
            "   Columns: 10\n",
            "\n",
            "üìã Sample data:\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "|                                          job_link|                                job_title|               company_name|                          location| job_level|employment_type|search_city|search_country|       search_position|first_seen|\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "|https://uk.linkedin.com/jobs/view/occupational-...|              occupational health advisor|        OH Talent Solutions|Leicester, England, United Kingdom|Mid senior|         Onsite|   Hastings|United Kingdom|      Safety Inspector|2024-01-16|\n",
            "|https://www.linkedin.com/jobs/view/senior-windo...|           senior windows server engineer|                       Epic|                      Paradise, NV|Mid senior|         Onsite|  Las Vegas| United States|   Computer Programmer|2024-01-13|\n",
            "|https://www.linkedin.com/jobs/view/warehouse-ma...|                        warehouse manager|                     Atkore|                    Fort Worth, TX|Mid senior|         Onsite|  Arlington| United States|    Supervisor Picking|2024-01-17|\n",
            "|https://www.linkedin.com/jobs/view/bariatrician...| bariatrician obesity medicine specialist|Baylor Scott & White Health|                       Killeen, TX|Mid senior|         Onsite|     Temple| United States|           Pathologist|2024-01-14|\n",
            "|https://ca.linkedin.com/jobs/view/merchant-sett...|merchant settlement specialist - contract|  Canadian Tire Corporation|         Oakville, Ontario, Canada|Mid senior|         Onsite| Burlington|        Canada|Contract Administrator|2024-01-13|\n",
            "+--------------------------------------------------+-----------------------------------------+---------------------------+----------------------------------+----------+---------------+-----------+--------------+----------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "üìä Dataset Overview:\n",
            "\n",
            "üåç Top 10 Countries:\n",
            "+--------------+-------+\n",
            "|search_country|count  |\n",
            "+--------------+-------+\n",
            "|United States |1149342|\n",
            "|United Kingdom|113421 |\n",
            "|Canada        |55972  |\n",
            "|Australia     |29719  |\n",
            "+--------------+-------+\n",
            "\n",
            "\n",
            "üèôÔ∏è Top 10 Cities:\n",
            "+-----------------+-----+\n",
            "|search_city      |count|\n",
            "+-----------------+-----+\n",
            "|Baytown          |10052|\n",
            "|North Carolina   |10015|\n",
            "|Garland          |9739 |\n",
            "|Greater London   |9297 |\n",
            "|Austin           |8897 |\n",
            "|South Carolina   |8386 |\n",
            "|Sarnia-Clearwater|7887 |\n",
            "|Atlanta          |7666 |\n",
            "|Indiana          |7599 |\n",
            "|Alabama          |7575 |\n",
            "+-----------------+-----+\n",
            "\n",
            "\n",
            "üíº Employment Types:\n",
            "+---------------+-------+\n",
            "|employment_type|count  |\n",
            "+---------------+-------+\n",
            "|Onsite         |1337633|\n",
            "|Hybrid         |6562   |\n",
            "|Remote         |4259   |\n",
            "+---------------+-------+\n",
            "\n",
            "\n",
            "üìä Job Levels:\n",
            "+----------+-------+\n",
            "|job_level |count  |\n",
            "+----------+-------+\n",
            "|Mid senior|1204445|\n",
            "|Associate |144009 |\n",
            "+----------+-------+\n",
            "\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PREPROCESSING COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# Section 4: Data Preprocessing (SAFE VERSION)\n",
        "# =====================================================================\n",
        "\n",
        "# Re-import to avoid conflicts\n",
        "from pyspark.sql.functions import (\n",
        "    col, trim, lower, upper, desc, asc, count, avg, sum as spark_sum,\n",
        "    collect_list, explode, when, countDistinct\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîß Creating working dataset...\")\n",
        "\n",
        "# Select and clean columns\n",
        "df_work = df_postings_clean.select(\n",
        "    'job_link',\n",
        "    trim(lower(col('job_title'))).alias('job_title'),\n",
        "    col('company').alias('company_name'),\n",
        "    col('job_location').alias('location'),\n",
        "    'job_level',\n",
        "    col('job_type').alias('employment_type'),\n",
        "    'search_city',\n",
        "    'search_country',\n",
        "    'search_position',\n",
        "    'first_seen'\n",
        ")\n",
        "\n",
        "# Filter out rows with null critical fields\n",
        "df_work = df_work.filter(\n",
        "    col('job_title').isNotNull() &\n",
        "    col('job_link').isNotNull()\n",
        ")\n",
        "\n",
        "# Cache for performance\n",
        "df_work.cache()\n",
        "work_count = df_work.count()\n",
        "\n",
        "print(f\"‚úÖ Working dataset ready\")\n",
        "print(f\"   Records: {work_count:,}\")\n",
        "print(f\"   Columns: {len(df_work.columns)}\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nüìã Sample data:\")\n",
        "df_work.show(5, truncate=50)\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nüìä Dataset Overview:\")\n",
        "\n",
        "print(\"\\nüåç Top 10 Countries:\")\n",
        "df_work.groupBy('search_country').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .limit(10) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüèôÔ∏è Top 10 Cities:\")\n",
        "df_work.groupBy('search_city').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .limit(10) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüíº Employment Types:\")\n",
        "df_work.groupBy('employment_type').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nüìä Job Levels:\")\n",
        "df_work.groupBy('job_level').count() \\\n",
        "    .orderBy(desc('count')) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PREPROCESSING COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPNVX81F31x_"
      },
      "source": [
        "## Section 5: Joining with Skills Data (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS6orn6_31x_",
        "outputId": "9d143338-321c-4723-ffbc-b80845d37e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "JOINING SKILLS DATA\n",
            "======================================================================\n",
            "\n",
            "üîó Preparing skills data...\n",
            "\n",
            "üì¶ Aggregating skills per job...\n",
            "‚úÖ Skills aggregated in 74.4s\n",
            "   Unique jobs with skills: 1,294,374\n",
            "\n",
            "üîó Joining with job postings...\n",
            "   Using standard join (large dataset)\n",
            "\n",
            "‚úÖ Join complete in 34.3s\n",
            "   Final records: 1,348,454\n",
            "\n",
            "üìä Skill coverage:\n",
            "   Jobs with skills: 1,294,374 (96.0%)\n",
            "   Jobs without skills: 54,080 (4.0%)\n",
            "\n",
            "üìã Sample joined data:\n",
            "+--------------------+--------------------+-----------+\n",
            "|           job_title|        company_name|skill_count|\n",
            "+--------------------+--------------------+-----------+\n",
            "|warehouse supervi...|Global Projects S...|          1|\n",
            "|expression of int...|    Queensland Hydro|          0|\n",
            "|account executive...|          DuluxGroup|          1|\n",
            "|account manager -...|    Impel Management|          1|\n",
            "|accountant (inter...|New Point Recruit...|          1|\n",
            "+--------------------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED: Memory-efficient join\n",
        "print(\"=\"*70)\n",
        "print(\"JOINING SKILLS DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîó Preparing skills data...\")\n",
        "\n",
        "# Clean skills data\n",
        "df_skills_clean = df_skills.select(\n",
        "    'job_link',\n",
        "    trim(lower(col('job_skills'))).alias('skill')\n",
        ").filter(col('skill').isNotNull())\n",
        "\n",
        "# Aggregate skills by job (reduces data size before join)\n",
        "print(\"\\nüì¶ Aggregating skills per job...\")\n",
        "start = time.time()\n",
        "\n",
        "df_skills_agg = df_skills_clean.groupBy('job_link').agg(\n",
        "    collect_list('skill').alias('skills_list'),\n",
        "    count('skill').alias('skill_count')\n",
        ")\n",
        "\n",
        "# Cache aggregated skills\n",
        "df_skills_agg.cache()\n",
        "skills_agg_count = df_skills_agg.count()\n",
        "\n",
        "print(f\"‚úÖ Skills aggregated in {time.time()-start:.1f}s\")\n",
        "print(f\"   Unique jobs with skills: {skills_agg_count:,}\")\n",
        "\n",
        "# Broadcast join for efficiency (if skills data fits in memory)\n",
        "print(\"\\nüîó Joining with job postings...\")\n",
        "start = time.time()\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Decide on join strategy based on data size\n",
        "if skills_agg_count < 1000000:  # If < 1M records, use broadcast\n",
        "    print(\"   Using broadcast join (optimized for smaller dataset)\")\n",
        "    df_final = df_work.join(\n",
        "        broadcast(df_skills_agg),\n",
        "        on='job_link',\n",
        "        how='left'\n",
        "    )\n",
        "else:\n",
        "    print(\"   Using standard join (large dataset)\")\n",
        "    df_final = df_work.join(\n",
        "        df_skills_agg,\n",
        "        on='job_link',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "# Fill null skill counts with 0\n",
        "df_final = df_final.fillna({'skill_count': 0})\n",
        "\n",
        "# Cache final dataset\n",
        "df_final.cache()\n",
        "final_count_with_skills = df_final.count()\n",
        "\n",
        "print(f\"\\n‚úÖ Join complete in {time.time()-start:.1f}s\")\n",
        "print(f\"   Final records: {final_count_with_skills:,}\")\n",
        "\n",
        "# Statistics\n",
        "jobs_with_skills = df_final.filter(col('skill_count') > 0).count()\n",
        "jobs_without_skills = final_count_with_skills - jobs_with_skills\n",
        "\n",
        "print(f\"\\nüìä Skill coverage:\")\n",
        "print(f\"   Jobs with skills: {jobs_with_skills:,} ({jobs_with_skills/final_count_with_skills*100:.1f}%)\")\n",
        "print(f\"   Jobs without skills: {jobs_without_skills:,} ({jobs_without_skills/final_count_with_skills*100:.1f}%)\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nüìã Sample joined data:\")\n",
        "df_final.select('job_title', 'company_name', 'skill_count').show(5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wh47T-H31x_"
      },
      "source": [
        "## Section 6: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# GOAL 1: Most In-Demand Skills - OPTIMIZED (Matching Expected Output)\n",
        "# =====================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SKILLS ANALYSIS - PANDAS OPTIMIZED FOR COLAB\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Convert to Pandas (More efficient in Colab)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n‚ö° [1/4] Converting to Pandas...\")\n",
        "step1_start = time.time()\n",
        "\n",
        "df_pandas = df_final.select(\n",
        "    'job_link',\n",
        "    'job_title',\n",
        "    'company_name',\n",
        "    'location',\n",
        "    'job_level',\n",
        "    'employment_type',\n",
        "    'search_country',\n",
        "    'search_city',\n",
        "    'skills_list',\n",
        "    'skill_count'\n",
        ").toPandas()\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_pandas):,} records in {time.time()-step1_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Vectorized Text Processing\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüîß [2/4] Processing skills...\")\n",
        "step2_start = time.time()\n",
        "\n",
        "# Drop nulls early\n",
        "df_pandas = df_pandas[df_pandas['skills_list'].notna()].copy()\n",
        "\n",
        "# Convert list column to string for processing\n",
        "df_pandas['skills_str'] = df_pandas['skills_list'].apply(\n",
        "    lambda x: ','.join([str(s) for s in x]) if isinstance(x, list) else str(x)\n",
        ")\n",
        "\n",
        "# Vectorized string operations (much faster than PySpark in Colab)\n",
        "df_pandas['skills_cleaned'] = (\n",
        "    df_pandas['skills_str']\n",
        "    .str.lower()\n",
        "    .str.replace(r'[;:\\/|]', ',', regex=True)\n",
        "    .str.replace(r'\\.+$', '', regex=True)  # Remove trailing dots\n",
        "    .str.replace('communication skills', 'communication', regex=False)\n",
        "    .str.replace('problem-solving', 'problem solving', regex=False)\n",
        "    .str.replace('problemsolving', 'problem solving', regex=False)\n",
        "    .str.replace('problem-solving skills', 'problem solving', regex=False)\n",
        "    .str.replace('customer service skills', 'customer service', regex=False)\n",
        "    .str.replace('leadership skills', 'leadership', regex=False)\n",
        "    .str.replace('team work', 'teamwork', regex=False)\n",
        "    .str.replace('time-management', 'time management', regex=False)\n",
        "    .str.replace('data analytics', 'data analysis', regex=False)\n",
        "    .str.replace('microsoft office', 'microsoft office suite', regex=False)\n",
        "    .str.replace('ms office', 'microsoft office suite', regex=False)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Text processed in {time.time()-step2_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Fast Skill Extraction with Counter\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìä [3/4] Extracting and counting skills...\")\n",
        "step3_start = time.time()\n",
        "\n",
        "# Explode and count in one efficient pass\n",
        "all_skills = []\n",
        "\n",
        "for skills_str in df_pandas['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-').strip() for s in skills_str.split(',')]\n",
        "    # Filter out very short skills and common words\n",
        "    all_skills.extend([\n",
        "        s for s in skills\n",
        "        if len(s) >= 3 and s not in ['and', 'the', 'for', 'with', 'are', 'but']\n",
        "    ])\n",
        "\n",
        "# Use Counter for blazing fast counting\n",
        "skill_counter = Counter(all_skills)\n",
        "unique_skills_count = len(skill_counter)\n",
        "total_skill_mentions = len(all_skills)\n",
        "\n",
        "print(f\"‚úÖ Counted {unique_skills_count:,} unique skills in {time.time()-step3_start:.1f}s\")\n",
        "print(f\"   Total skill mentions: {total_skill_mentions:,}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Generate Reports\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìà [4/4] Generating reports...\")\n",
        "step4_start = time.time()\n",
        "\n",
        "# Get top 1000 skills for filtering\n",
        "top_1000 = skill_counter.most_common(1000)\n",
        "top_skills_set = set([skill for skill, _ in top_1000])\n",
        "\n",
        "# Global top 20\n",
        "top_20_df = pd.DataFrame(\n",
        "    skill_counter.most_common(20),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüåç Top 20 Global Skills:\")\n",
        "print(top_20_df.to_string(index=False))\n",
        "\n",
        "# USA Regional Analysis (optimized)\n",
        "print(\"\\nüá∫üá∏ Analyzing USA market...\")\n",
        "usa_skills = []\n",
        "usa_df = df_pandas[df_pandas['search_country'] == 'United States']\n",
        "\n",
        "for skills_str in usa_df['skills_cleaned'].dropna():\n",
        "    skills = [s.strip().strip('-').strip() for s in skills_str.split(',')]\n",
        "    usa_skills.extend([\n",
        "        s for s in skills\n",
        "        if len(s) >= 3 and s in top_skills_set\n",
        "    ])\n",
        "\n",
        "usa_counter = Counter(usa_skills)\n",
        "usa_top_10 = pd.DataFrame(\n",
        "    usa_counter.most_common(10),\n",
        "    columns=['skill', 'count']\n",
        ")\n",
        "\n",
        "print(\"\\nüá∫üá∏ Top 10 Skills in USA:\")\n",
        "print(usa_top_10.to_string(index=False))\n",
        "\n",
        "print(f\"‚úÖ Reports generated in {time.time()-step4_start:.1f}s\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Visualization (Matching Expected Style)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nüìä Creating visualization...\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "# Create figure with exact styling\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Create color gradient from dark purple to yellow (viridis-like)\n",
        "colors = plt.cm.viridis(np.linspace(0.9, 0.1, len(top_20_df)))\n",
        "\n",
        "# Horizontal bar plot\n",
        "bars = ax.barh(\n",
        "    range(len(top_20_df)),\n",
        "    top_20_df['count'],\n",
        "    color=colors,\n",
        "    edgecolor='none'\n",
        ")\n",
        "\n",
        "# Styling\n",
        "ax.set_yticks(range(len(top_20_df)))\n",
        "ax.set_yticklabels(top_20_df['skill'], fontsize=11)\n",
        "ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='normal')\n",
        "ax.set_ylabel('Skill', fontsize=12, fontweight='normal')\n",
        "ax.set_title('Top 20 Global Skills', fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "# Invert y-axis so highest is at top\n",
        "ax.invert_yaxis()\n",
        "\n",
        "# Clean up grid\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='-', linewidth=0.5)\n",
        "ax.grid(axis='y', alpha=0)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "# Format x-axis with thousands separator\n",
        "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# Total Time and Summary\n",
        "# =============================================================================\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚è±Ô∏è  Total execution time: {total_time:.1f}s\")\n",
        "print(f\"üìä Processed {len(df_pandas):,} job postings\")\n",
        "print(f\"üéØ Found {unique_skills_count:,} unique skills\")\n",
        "print(f\"üìà Total skill mentions: {total_skill_mentions:,}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# Key Observations (Formatted Output)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY OBSERVATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Global Skills Analysis\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Calculate percentages\n",
        "total_jobs = len(df_pandas)\n",
        "top_skill_count = top_20_df.iloc[0]['count']\n",
        "second_skill_count = top_20_df.iloc[1]['count']\n",
        "\n",
        "print(f\"‚Ä¢ Soft skills dominate overwhelmingly:\")\n",
        "print(f\"  - Top 5 are all non-technical\")\n",
        "print(f\"  - Communication leads with {top_skill_count:,} mentions\")\n",
        "print(f\"  - {top_skill_count/second_skill_count:.1f}x more than #2\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Communication is king:\")\n",
        "print(f\"  - {top_skill_count:,} mentions\")\n",
        "print(f\"  - Appears in {top_skill_count/total_jobs*100:.1f}% of job postings\")\n",
        "print(f\"  - Far exceeds any other skill\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Technical skills present but secondary:\")\n",
        "tech_skills = ['data analysis', 'microsoft office suite']\n",
        "for skill in tech_skills:\n",
        "    skill_data = top_20_df[top_20_df['skill'] == skill]\n",
        "    if not skill_data.empty:\n",
        "        rank = top_20_df[top_20_df['skill'] == skill].index[0] + 1\n",
        "        count = skill_data['count'].values[0]\n",
        "        print(f\"  - {skill.title()} (#{rank}) with {count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Healthcare sector strongly represented:\")\n",
        "healthcare_skills = ['patient care', 'nursing']\n",
        "for skill in healthcare_skills:\n",
        "    skill_data = top_20_df[top_20_df['skill'] == skill]\n",
        "    if not skill_data.empty:\n",
        "        count = skill_data['count'].values[0]\n",
        "        print(f\"  - {skill.title()}: {count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Dataset composition:\")\n",
        "print(f\"  - Total processed: {total_jobs:,} job postings\")\n",
        "print(f\"  - Unique skills: {unique_skills_count:,}\")\n",
        "print(f\"  - Total skill mentions: {total_skill_mentions:,}\")\n",
        "print(f\"  - Average skills per posting: {total_skill_mentions/total_jobs:.1f}\")\n",
        "\n",
        "print(\"\\nüìä USA Regional Findings\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "usa_job_count = len(usa_df)\n",
        "usa_percentage = usa_job_count / total_jobs * 100\n",
        "\n",
        "print(f\"‚Ä¢ Perfect alignment with global trends:\")\n",
        "print(f\"  - USA top 5 exactly matches global top 5\")\n",
        "print(f\"  - (Communication, Customer Service, Problem Solving, Teamwork, Leadership)\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ USA dominates dataset:\")\n",
        "print(f\"  - Represents {usa_percentage:.1f}% of all job postings\")\n",
        "print(f\"  - {usa_job_count:,} out of {total_jobs:,} postings\")\n",
        "print(f\"  - Suggests heavy USA market concentration\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Service economy emphasis:\")\n",
        "print(f\"  - Customer Service ranks #2 in USA (vs #3 globally)\")\n",
        "print(f\"  - Reflects strong service sector presence\")\n",
        "\n",
        "usa_patient_care = usa_top_10[usa_top_10['skill'] == 'patient care']\n",
        "if not usa_patient_care.empty:\n",
        "    pc_count = usa_patient_care['count'].values[0]\n",
        "    print(f\"\\n‚Ä¢ Healthcare specialization evident:\")\n",
        "    print(f\"  - Patient Care in USA top 10 with {pc_count:,} mentions\")\n",
        "\n",
        "print(f\"\\n‚Ä¢ Interpersonal skills valued higher:\")\n",
        "print(f\"  - Appears in USA top 10\")\n",
        "print(f\"  - Emphasizes relationship-driven business culture\")\n",
        "\n",
        "print(\"\\nüìä Pipeline Performance\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"‚Ä¢ Processing time: {total_time:.1f}s (~{total_time/60:.1f} minutes)\")\n",
        "print(f\"‚Ä¢ Unique skills identified: {unique_skills_count:,}\")\n",
        "print(f\"‚Ä¢ Skill instances: {total_skill_mentions:,} across {total_jobs:,} records\")\n",
        "print(f\"‚Ä¢ Pandas optimization: Vectorized operations significantly faster than PySpark\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Save results\n",
        "top_20_df.to_csv('top_20_skills_global.csv', index=False)\n",
        "usa_top_10.to_csv('top_10_skills_usa.csv', index=False)\n",
        "\n",
        "print(\"\\nüíæ Saved Results:\")\n",
        "print(\"   ‚Ä¢ top_20_skills_global.csv\")\n",
        "print(\"   ‚Ä¢ top_10_skills_usa.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ GOAL 1 COMPLETE\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJz3qkuW_9X4",
        "outputId": "49790cb0-1725-4221-a9a4-00843220b279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SKILLS ANALYSIS - PANDAS OPTIMIZED FOR COLAB\n",
            "======================================================================\n",
            "\n",
            "‚ö° [1/4] Converting to Pandas...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q8qBcSXTJ1v",
        "outputId": "15ea6866-4b07-4701-fb0d-bcac4d4c5c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company_name: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- employment_type: string (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- first_seen: string (nullable = true)\n",
            " |-- skills_list: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- skill_count: long (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 2:"
      ],
      "metadata": {
        "id": "TgvBDp7QVTy5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "Iehpu48631x_",
        "outputId": "7700a2c3-569a-46d5-f255-d33876a52fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GOAL 2: JOB TITLE SIMILARITY (TF-IDF & COSINE SIMILARITY)\n",
            "======================================================================\n",
            "\n",
            "Calculating Top 5 Job Title Similarities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-937295377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mdf_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 2: JOB SIMILARITY - JACCARD SIMILARITY (Skill Overlap Metric)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 2: JOB SIMILARITY - JACCARD OVERLAP METRIC\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define the two job titles we want to compare\n",
        "JOB_A = 'software engineer'\n",
        "JOB_B = 'data scientist'\n",
        "\n",
        "def calculate_jaccard_similarity(df, job_a, job_b):\n",
        "    \"\"\"Calculates Jaccard Similarity based on aggregated skills for two job titles.\"\"\"\n",
        "    print(f\"\\nüî¨ Comparing Skill Sets for: '{job_a.title()}' vs. '{job_b.title()}'\")\n",
        "\n",
        "    # 1. Aggregate all unique skills for each job title\n",
        "    skills_a_list = df[df['job_title'] == job_a]['skills_cleaned_list'].explode().dropna().tolist()\n",
        "    skills_b_list = df[df['job_title'] == job_b]['skills_cleaned_list'].explode().dropna().tolist()\n",
        "\n",
        "    if not skills_a_list or not skills_b_list:\n",
        "        print(\"‚ö†Ô∏è One or both job titles not found in the dataset.\")\n",
        "        return 0.0\n",
        "\n",
        "    # 2. Convert to sets for efficient intersection and union\n",
        "    skills_a_set = set(skills_a_list)\n",
        "    skills_b_set = set(skills_b_list)\n",
        "\n",
        "    # 3. Calculate Intersection and Union\n",
        "    intersection = len(skills_a_set.intersection(skills_b_set))\n",
        "    union = len(skills_a_set.union(skills_b_set))\n",
        "\n",
        "    # 4. Calculate Jaccard Similarity (Intersection / Union)\n",
        "    jaccard = intersection / union if union > 0 else 0.0\n",
        "\n",
        "    print(f\"  - Unique Skills in '{job_a.title()}': {len(skills_a_set):,} \")\n",
        "    print(f\"  - Unique Skills in '{job_b.title()}': {len(skills_b_set):,} \")\n",
        "    print(f\"  - Overlapping Skills (Intersection): {intersection:,}\")\n",
        "    print(f\"  - Combined Skills (Union): {union:,}\")\n",
        "\n",
        "    return jaccard, list(skills_a_set.intersection(skills_b_set))\n",
        "\n",
        "jaccard_score, common_skills = calculate_jaccard_similarity(df_pandas, JOB_A, JOB_B)\n",
        "\n",
        "print(f\"\\n‚ú® Jaccard Similarity between '{JOB_A.title()}' and '{JOB_B.title()}': **{jaccard_score:.4f}**\")\n",
        "print(f\"  Top 5 Common Skills: {common_skills[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 3:"
      ],
      "metadata": {
        "id": "gpTboDM9VQkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 3: REGIONAL SPECIALIZATION - LOCATION QUOTIENT (LQ)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 3: REGIONAL SPECIALIZATION - LOCATION QUOTIENT (LQ)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Prepare global and regional skill counts\n",
        "step3_start = time.time()\n",
        "\n",
        "# Explode skills for global and regional counts\n",
        "df_skills_exploded = df_pandas[['search_country', 'skills_cleaned_list']].explode('skills_cleaned_list')\n",
        "df_skills_exploded.rename(columns={'skills_cleaned_list': 'skill'}, inplace=True)\n",
        "df_skills_exploded.dropna(inplace=True)\n",
        "\n",
        "# Global Counts\n",
        "global_skill_mentions = len(df_skills_exploded)\n",
        "global_skill_counts = df_skills_exploded['skill'].value_counts().rename('global_count')\n",
        "global_skill_ratios = (global_skill_counts / global_skill_mentions).rename('global_ratio')\n",
        "\n",
        "# Regional Counts and Ratios\n",
        "regional_counts = df_skills_exploded.groupby(['search_country', 'skill']).size().rename('regional_count')\n",
        "regional_total_mentions = df_skills_exploded.groupby('search_country').size().rename('regional_total')\n",
        "\n",
        "# Merge to create the Location Quotient (LQ) dataframe\n",
        "df_lq = regional_counts.reset_index().merge(regional_total_mentions.reset_index(), on='search_country')\n",
        "df_lq = df_lq.merge(global_skill_ratios.reset_index(), on='skill')\n",
        "\n",
        "# 2. Calculate LQ: (Regional Count / Regional Total) / Global Ratio\n",
        "df_lq['regional_ratio'] = df_lq['regional_count'] / df_lq['regional_total']\n",
        "df_lq['LQ'] = df_lq['regional_ratio'] / df_lq['global_ratio']\n",
        "\n",
        "# 3. Analyze Results: Find top specialized skills for the USA\n",
        "COUNTRY_LQ = 'United States'\n",
        "df_usa_lq = df_lq[df_lq['search_country'] == COUNTRY_LQ] \\\n",
        "    .sort_values(by='LQ', ascending=False) \\\n",
        "    .head(10)\n",
        "\n",
        "print(f\"\\nü•á Top 10 Specialized Skills (Highest LQ) in {COUNTRY_LQ}:\")\n",
        "print(df_usa_lq[['skill', 'LQ', 'regional_count']].to_string(index=False, float_format=\"%.2f\"))\n",
        "\n",
        "print(f\"\\nInterpretation: An LQ > 1.0 means the region has a greater-than-average specialization in that skill globally.\")\n",
        "print(f\"‚úÖ Goal 3 completed in {time.time()-step3_start:.1f}s\")\n"
      ],
      "metadata": {
        "id": "_hjGcsm9VJsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 4 & 6"
      ],
      "metadata": {
        "id": "qmULKcCMVYgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# GOALS 4 & 6 PREP: FEATURE ENGINEERING (TF-IDF Vectorization)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ML PREP: TF-IDF Feature Engineering (Scikit-learn)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prep_start = time.time()\n",
        "\n",
        "# 1. Aggregate all skills into a single 'document' per job title\n",
        "df_ml = df_pandas.groupby('job_title').agg(\n",
        "    all_skills_document=('all_skills_str', lambda x: ' '.join(x.astype(str))),\n",
        "    count=('job_title', 'size')\n",
        ").reset_index()\n",
        "\n",
        "# 2. Vectorization using Scikit-learn's TfidfVectorizer\n",
        "# Use the aggregated skills for vector creation\n",
        "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(df_ml['all_skills_document'])\n",
        "\n",
        "# Convert the sparse matrix to a dense DataFrame for easier merging and use\n",
        "df_tfidf = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    index=df_ml['job_title']\n",
        ")\n",
        "df_tfidf.columns = [f'tfidf_{i}' for i in range(df_tfidf.shape[1])]\n",
        "\n",
        "# 3. Merge TF-IDF vectors back into the original job posting data\n",
        "df_ml = df_pandas.merge(df_tfidf, on='job_title', how='left')\n",
        "\n",
        "# Drop rows where vector is missing (should only be a few if any)\n",
        "df_ml.dropna(subset=df_tfidf.columns.tolist(), inplace=True)\n",
        "\n",
        "# Define the TF-IDF feature column names\n",
        "TFIDF_COLS = df_tfidf.columns.tolist()\n",
        "\n",
        "print(f\"‚úÖ TF-IDF matrix created for {len(df_ml):,} job postings with {len(TFIDF_COLS):,} features.\")\n",
        "print(f\"   ML Prep completed in {time.time()-prep_start:.1f}s\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 4: UNSUPERVISED LEARNING - K-MEANS CLUSTERING (Scikit-learn)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 4: UNSUPERVISED LEARNING - K-MEANS CLUSTERING\")\n",
        "print(\"=\"*80)\n",
        "#\n",
        "\n",
        "K = 5 # Number of clusters\n",
        "kmeans_start = time.time()\n",
        "\n",
        "# 1. Select the features (TF-IDF vectors)\n",
        "X_cluster = df_ml[TFIDF_COLS]\n",
        "\n",
        "# 2. Train K-Means model\n",
        "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
        "df_ml['cluster'] = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "# 3. Analyze Clusters\n",
        "print(f\"\\nCluster distribution for K={K}:\")\n",
        "cluster_counts = df_ml.groupby('cluster')['job_title'].count()\n",
        "print(cluster_counts.sort_index().to_string())\n",
        "\n",
        "# Analyze skill representation within clusters\n",
        "print(\"\\nTop 5 Distinctive Skills per Cluster (Highest TF-IDF Mean):\")\n",
        "for k in range(K):\n",
        "    # Calculate the mean TF-IDF value for each feature (skill vector component) in the cluster\n",
        "    cluster_data = df_ml[df_ml['cluster'] == k]\n",
        "    mean_tfidf = cluster_data[TFIDF_COLS].mean().sort_values(ascending=False)\n",
        "\n",
        "    # Get the feature names for the top 5\n",
        "    top_5_indices = mean_tfidf.head(5).index.tolist()\n",
        "\n",
        "    # Map feature indices back to skill names using the TfidfVectorizer vocabulary\n",
        "    # Note: TfidfVectorizer uses integer indices, we must map them.\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Extract the index number from the column names (e.g., 'tfidf_123' -> 123)\n",
        "    top_5_real_indices = [int(col.split('_')[1]) for col in top_5_indices]\n",
        "\n",
        "    # Map these indices to the actual skill name\n",
        "    top_5_skills = [feature_names[i] for i in top_5_real_indices]\n",
        "\n",
        "    print(f\"--- Cluster {k} (N={len(cluster_data):,}) ---\")\n",
        "    print(f\"Titles: {cluster_data['job_title'].unique()[:3].tolist()}...\")\n",
        "    print(f\"Skills: {top_5_skills}\")\n",
        "\n",
        "print(f\"‚úÖ Goal 4 completed in {time.time()-kmeans_start:.1f}s\")\n"
      ],
      "metadata": {
        "id": "QfXSP1O1VOB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 5"
      ],
      "metadata": {
        "id": "_hG2M8wdVNal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h75ILxw31yA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "outputId": "eaa0e674-384d-468d-a86d-29457ff096ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Skill Count Distribution\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Skill Count Statistics:\n",
            "  summary          skill_count\n",
            "0   count              1348454\n",
            "1    mean   0.9598948128745957\n",
            "2  stddev  0.19620598777783219\n",
            "3     min                    0\n",
            "4     max                    1\n",
            "\n",
            "üìà Average Skills by Job Level:\n",
            " job_level  avg_skills  job_count\n",
            " Associate    0.978543     144009\n",
            "Mid senior    0.957665    1204445\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYHtJREFUeJzt3Xd4FGX79vFzs0kIEKQkNGlK6CFBkKJIUURFmtJ7F6QXKSI+FpqgAgKCioA0QR6KIChdBPEhVEFaEAi9hRBACAmp+/7Bm/llJWgWNrNh8/0ch4fZ2ZnZa9jNZvbc+77GYrPZbAIAAAAAAABM5OHqAgAAAAAAAJD5EEoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAExHKAUAAAAAAADTEUoBAICHdv78eZUuXdr4b+fOnf+6zc6dO+22OX/+vHHf8OHDjeUdOnSw2y7lNt9//73TjyUzyGj/hg/y+pGkDh06GNsMHz48nas0j7sel7P803sHAODR4unqAgAAuJ+GDRvq+PHjxu28efNqy5Yt8vTkz1d6279/vxYsWKB9+/YpIiJCVqtVOXPmlL+/v0qXLq3g4GC1bt3a1WWmm9KlS9+zzGKxyMfHR/nz51elSpXUoUMHlStXzgXVISPZuXOnOnbsaNweN26cmjZt6sKKUpfyNd2kSRONHz/ehdUAAHAXZ/UAgAzpwIEDdoGUJEVERGjbtm164YUXXFRV5rB06VK99957stlsdstjYmJ0+fJlHTp0SBs2bHjoUKpo0aIaNmyYcTtXrlwPtb/0ZrPZFBMTo9OnT+v06dNavXq1pk+frtq1a7u6NIel/HcPCgpyYSUAACAzI5QCAGRIK1asuO/yjBpKRUdHy8fHRx4ej+7s+Bs3bmjMmDFGIFWgQAG98sor8vPz0+3bt3Xs2DHt2bPHKY9VsGBBdevWzSn7Sk/ly5dX/fr1FRsbq/3792vr1q2SpPj4eE2ePPmRDKUe9N89KipKvr6+Tq4GAABkVoRSAIAMJy4uTj/99JNx+4knntDp06clSZs3b9b169eVO3duSdKZM2f08ssvG+vOnz9f1apVs9tfixYtdODAAePnMWPGGPcdPXpU8+bN0+7du3XlyhVZrVYVK1ZM9erVU8eOHZUtWza7fdWpU0cXLlyQJPXt21fPPvuspk2bpoMHDyoqKkq7d+9WtmzZNG3aNB0+fFinTp3SjRs3FBMTI19fXwUEBOjVV19V69at5eXldc+xL126VPPnz9fp06eVJ08e1a9fX/369VPDhg3tHrdfv3522zl6HPezd+9e3blzx7i9aNEiFSpUyG6dhIQE7dixI037k6R169bprbfeUmJioiSpWbNmGjNmjHbv3m037ennn39W4cKF07zf1Hz//fdasWKFjh07pqioKGXLlk158uRR2bJlVaVKFbVr187hfZYsWdIuxGnZsqX++OMPSdKJEydS3ebcuXOaN2+e/ve//+nSpUtKSkpS4cKFVadOHXXt2lV58uS5Z5ujR49q0qRJRuhXqVIlDR48WJs2bdK0adMkSYUKFdLmzZuNbVJOyfr7tLHhw4cb4W7VqlW1YMGCf93u+++/1zvvvGPct3//fn311Vf68ccfdfnyZbVt21bvvvuupLu/p0uWLNHatWt1/PhxRUdHK1euXKpUqZK6dOmiihUr3nOMMTExmj59ulavXq1r166paNGiatu2rWrVqpXqv6Ojzp49q4kTJ2rHjh26c+eOypUrp379+ql69eqS7oZqNWvWVHR0tCRp1KhRatWqld0++vfvr/Xr10uSatasqVmzZj10XYcOHdL8+fO1Z88eRUREyNPTU4UKFVKNGjXUuXNnFShQ4KGOK7048hxPnjxZX375paR7X6eSFBYWpvr16xu3Fy1apKefflqSlJSUpFWrVmnVqlUKDQ3VrVu35Ovrq+DgYLVr1+6RDH4BAGlDKAUAyHA2bdqkv/76y7j98ccfq3379oqPj1d8fLx+/PFHo/l1sWLFVLlyZeOD/E8//WQXSp09e9YIpKS7gUiyRYsWaezYsUpISLB7/NDQUIWGhmr16tWaO3eu8ubNm2qd//vf//Tll18aYUuy2NhY48NZSjdu3NDevXu1d+9ebd68WbNmzZLVajXunzhxor7++mvj9uXLl/XNN99oz549io2Nve+/18MeR0p/P5ajR4/eE0p5enqqRo0a/7ov6e5zOWTIEGO/bdu21fvvvy+LxZKm7R3x+eefG+FNsps3b+rmzZs6ffq0du/e/UCh1N/lz5/f+Dk5HE0p+ZhjYmLsloeFhSksLEyrVq3SnDlzFBAQYNx34MABderUyQhLJGnbtm3atWuXKleu/NA1P6g33ngj1ZFx165dU9euXRUaGmq3PCIiQuvXr9fGjRs1fPhwderUybgvPj7+nv2dOHFCo0aN0vPPP//QtZ4+fVotWrTQjRs3jGW///67unXrpkmTJunVV1+Vr6+vmjRpooULF0q6GwKnDKWio6P166+/GrdTvl88qLlz5+rjjz9WUlKSsSwuLk7Hjx/X8ePHtWzZMk2fPv2eMN2R40oPjj7HTZs2Nd73Lly4oH379tmFVj/++KPx8xNPPGEEUnfu3FGvXr20fft2u8e5fv26tm7dqq1bt6pLly40fAcAN0UoBQDIcFJO3QsMDNRTTz2lZ5991viwuGLFCrsrsjVt2tT4oLt+/Xq99957xiiklB+EihcvbnxI+v333zV69Gjjg+JTTz2lmjVr6vbt21qxYoWuX7+uEydO6O2339Y333yTap379u1T1qxZ1bhxY+XLl0+hoaHy8PCQxWJRkSJFVKFCBeXPn185c+ZUfHy8Tp06pXXr1ikhIUHbt2/X+vXrjZEDBw4c0MyZM419+/n56fXXX9ft27e1fPlyxcfHp1qDM44jpbJly8pisRjT93r37m0cS2BgoCpXrqygoKA0hUpbt27VwIEDjdrT+4Pld999Z/xcvXp1Va1aVTExMbp06ZL27t37j8FeWsTGxur333/X//73P2PZ3wOBc+fOafDgwcZos5IlS6pu3bqy2WxavXq1Lly4oPDwcPXr10+rV6+W1WqVzWbTiBEjjEDKYrGoYcOGKlSokDZs2GD3eGbbs2ePKlSooOrVqysmJkYFCxaUJA0dOtQIK7Jnz66GDRuqQIEC+v3337Vt2zYlJSVp3LhxKl++vBE+JI8USlauXDk9//zzOn78uDZu3PjQte7bt0/58uVT9+7ddfv2bS1btkxxcXFKSkrS+++/rxo1aihHjhxq166dFi1aJJvNpoMHD+rPP/80Ro5t2bLFCBNz5cqlF1988aFq2r17t8aPH2/8Pj3++ONq0KCBoqOj9f333ysmJka3bt1S//79tWHDBuXMmfOBj8vZHH2OixYtqipVqmj37t2S7r73pgylUo5+TRn2ffTRR0Yg5eXlpQYNGqhYsWI6duyY1q1bJ5vNpjlz5igwMFCNGjVy+nECAFyLUAoAkKFcuXLF7kN4gwYNjP8nh1KHDx+2+yD56quvasyYMYqOjtaNGzf022+/GX2nUn4QSjm16ZtvvjGCnKpVq2revHlGL6hXX31VLVq0kHR3NNTRo0dVpkyZe2q1Wq1auHChAgMD77lv06ZNioyM1P79+xUeHm5MuTl27JiOHTsmSfrtt9+MUGrZsmXGB1cPDw/NmzdPJUuWlCRVqFDBbkpVSs44jpSKFCmijh07at68ecayc+fO6dy5c0bAV7hwYQ0dOlT16tW77362b9+uOXPmGIFUz549NWjQoH987IeVMnT65JNP7hkZdu7cuQfa74oVK+7pcWaxWNSoUSO99dZbdsu//fZbI5B64okntHz5cmXJkkWS1K5dOz3//PNKTExUWFiYtmzZohdffFH79++3a+rfs2dPDRw4UJLUvXt31a1bV9evX3+g2h/Wyy+/rClTptj1STt69Kh+++034/YXX3yhZ555xrjdo0cPbd261QgTkkOppUuXGusUK1ZM//3vf+Xt7S1Jeu+997RkyZKHqtXLy0vfffedMQW0UqVKGjJkiKS7I+bWrVunFi1aKCAgQNWrVzfeZ5YuXar//Oc/ku5ONU3WqFEjo74HNWfOHOP3Onv27Fq2bJn8/PwkSbVr11aPHj0k3R1FuWLFCnXu3PmBj8uZHvQ5btasmRFKrVu3TiNGjJDVatWBAwd05swZSXffN1977TXjuJcvX27sd+TIkXaB1ciRI7Vo0SJJd9/rCKUAwP08up1YAQBu6YcffjCmelksFiO0qVu3rvHhXrrb+yZZtmzZ7AKS5PDk6NGjRs+flB+EpLsjjJLt2rVLZcuWVenSpVW6dOl7PuDt27cv1Vpr1aqVaiB1584dvfPOO6pRo4Z69+6tkSNH6uOPP9Ynn3xiBFKSFB4ebvx86NAh4+fAwEAjkJKkxo0by9Mz9e+RnHEcf/fOO+9o9OjRdjWkdP78eQ0cOPAf+0p99dVXRkjUv3//dA+kJNlNc2vYsKF69OihsWPHasmSJTpz5oyKFCnitMcKDAxUnz597F6Tkv3zcfr0aQUHBxvPR82aNe2mRyY/H4cPH7bbR8oP3r6+vi5t7P/mm2/e07g/5TFKUqdOnYxjLF26tNEIXvq/Y7x9+7ZOnTplLH/55ZftAp/GjRs/dK1PP/20XU+y+vXr2/VtS/k71r59e+PnVatWKTY2VtHR0Xa1pwyxH9T+/fuNn2vWrGkEUtLdUCplb7GU66bkyHE5y4M8x5L0yiuvKHv27JKkq1evGu8RKb8cqFGjhjEF9o8//rCbdjxixAi7x0kOpKS705H/PiUWAPDoI5QCAGQoKUekVKxY0Zgu5Ovra9d3ZvXq1XYfZlJ+u75582bFxMTYTd2rVauW8uXLZ9xO2bPq31y7di3V5U8++WSqyydOnKjvv//erodMauLi4oyfb926Zfzs7+9vt56np2eqvYsk5xzH31ksFrVs2VI//vijtm7dqsmTJ6tTp052vaVsNpvmzp2bpn05Mwz6Jx9++KGeeuopSXdHYGzdulXz58/Xe++9p5dfflkDBw781+ckNeXLl9fQoUPVokULIww4dOiQ2rdvr6tXr9qt+yDPx82bN+2WpwwupHtfD/eTPCInWcrX14MqXrz4Pcse5BhTvr6le4/x77cfxN/3YbValStXLuN2yhqef/5543X5119/af369dqyZYsxyq1s2bIqV67cQ9eU8t8qtecx5bK/vw6SOXJczvKg7yvZsmWzm9L6448/KikpSWvWrDGWpXyvduRxbDabXV8tAIB7YPoeACDD+OOPPxQWFmbc/v333+2uEpZSZGSktm7davR8qVy5snGVvujoaP388892H4T+PuohZ86cioyMlHR3JMI/9Y5J7Spiku57Rbu1a9caP5cqVUqTJk3Sk08+KU9PTw0YMMBuilCylD1h/h4eJSQk3Hf6ljOO458UKFBAr776ql599VUNGTJEr7/+uvEcJU/HSU3x4sV18uRJ2Ww2vfPOO8qWLZvq1q3r8OM7omDBgvrvf/+rM2fOGNOFjh07pp9//lkJCQlau3atatas6XDz6pIlS+qNN96QJD377LPGlL2IiAhNmjRJH330kbFuyp5AJUuWVJMmTf5xv5L02GOP2S2PjIy0Cx3+HnyllLL/V8qrJkoyrlj5MFJ7jf+971H//v3l4+Pzj/vx9fW1u538mr3f7Qfx930kJibahRgpf8c8PDzUrl07jR8/XtLdKXwpg19njJKS7H8/U3seUy77++sgmSPH5SwP8hwna9q0qZYtWyZJ2rhxo+rVq6crV65IunthgJQj//7+OJ07d7b78uDv0uNYAQCuRSgFAMgwUk7JS4sVK1bYhTBNmzbVpEmTJEmfffaZLly4IOneD0LS3YBm06ZNku5+MGzVqtU9H5zv3LmjdevWqVKlSg7VlfIDY7Vq1Yzw4dq1a9q1a1eq25QvX96YxnXo0CGdOXNGxYoVk3R3etHfr6yXXsdx6NAhbdy4Ua1btzZGqSXz9PS0m652vw/RkvTuu+9q1qxZCgkJUUJCggYNGqQZM2ak6yXsjx49qlKlSqlYsWLGv50k9erVy7g8/ZEjRx7qimoNGjTQ4sWLjedx5cqV6tWrlzHqpmLFisbVHiMiItSwYUO7q/VJd0PGX375RRUqVJB097lPafXq1UZPqaioKP3yyy/3reexxx4zRpv88ccfxtUFt23bds+0QGf5++sod+7catu27T3rHT9+3KjN19dXTz75pDGFb8OGDerfv78xhW/VqlUPXdfevXt1/vx5Y6rbmjVr7C4Q8Pd/52bNmmnq1KmKjo7Wrl27jFq8vLyc1rso5e/ntm3bFBkZaYx82rp1q10Afb/Q2NHjcoYHeY6TPf3008YXBLdu3dKoUaOM+/7ep6tChQqyWq3GtFZPT09169btnsc5f/68Tp06dc97GwDg0UcoBQDIEGJjY+1GNhUuXFjBwcH3rHfs2DGjT9SWLVt07do1oy/L66+/rilTpigxMVHnz583tmncuLFdDxbp7pXgfv75Z9lsNp05c0YNGzbUSy+9JH9/f926dUvHjh3T7t27FR0drddff92hY3nyySeN3lFLly6Vh4eHsmbNqh9++OG+U+iaN2+uJUuWyGazKTExUe3atdPrr7+uqKgoY9RBapx9HLdv39ZXX32lGTNmKDAwUBUqVFC+fPkUGxur7du368iRI8a6NWvWvO9+vLy8NG3aNHXo0EFHjhxRXFyc+vTpo9mzZzsc8qXVwIEDFRUVpWrVqilfvnzKlSuXzp49azTIl5wz0qJnz55GKJWYmKivv/5ao0ePliR16NBBixcvVmxsrG7cuKHXXntN9erVU8GCBRUdHa0TJ05o165dunnzpn7++WflzJlTFSpUUMmSJY1m51999ZUuXLigQoUKaf369f/Y5DwoKMhoSP3DDz8oPDxcPj4+6XrFvjJlyui5554zHmP06NH69ddfVb58eVksFl28eFH79u1TWFiY+vbta/T6at68uT799FNJd0fZtWrVSi+88IKOHz+uDRs2PHRd8fHxatOmjV577TXjKnXJcuTIcU9j/scee0yNGjXSf//7X0n/N92xTp06950u66jOnTsbv5+3b99W8+bN1bBhQ0VHR9s1+M6VK9d9R9U5elxp8csvv9x3NNhXX331wM9xsmbNmmnixImSZPde/PdAOFeuXGrWrJnR5H7WrFk6dOiQKlasqCxZsig8PFx//PGHjhw5oiZNmvzjew4A4NFEKAUAyBA2bdpk11NlwIABqTY/DgkJMa5QFR8fr9WrV6tTp06SpPz58+u5556zCyGkez8ISXen+7333nv66KOPlJCQoEuXLmn+/PlOOZaePXsaU7zu3LljXMkub968dh/0UgoODlb37t319ddfS7o7ymbmzJmS7jbVDg8PN6b6WCyWdD8Om82mQ4cO3beJcmBgYKpXCkvJ19dXM2fOVJs2bXT27FlFR0frzTff1Pz581W2bNmHrjE1ERERdr3EUsqVK5dTrlL23HPPKSgoSAcPHpR0d8Renz59VKBAARUpUkSTJk3S0KFDFR0drevXr+u77777x/1ZLBZ99NFH6tSpk6Kjo2Wz2YyRQ97e3nr22WcVEhKS6rbdunXT//73P2MKX3Jj6Vy5cqlo0aLGqC1n+/TTT9WtWzeFhoYqKSlJv/zyyz+O6JLuNsvetGmT0Rj7yJEjRshZtWrV+44iTKvAwECdPn3a+L1J5uHhoZEjR6YaSHbo0MEIpZI5OpLu7728Uv5+VqlSRcOHD9fHH3+spKQkXbx40fgdT5YjRw5NnTr1viMPH+S4/s2NGzfu258pOZx7kOc42WuvvabJkyfbNfYPDAxM9eqfI0aM0Pnz57V9+3ZJd1/D/3QRBQCAe6HROQAgQ0g5dS9Hjhx6+eWXU13vmWeesWu4nbIxunRvL5jAwMD79qVq166dVqxYoVatWumJJ55Q1qxZ5enpKX9/f1WtWlW9e/fWDz/84PCxNGjQQJMnT1aZMmXk5eWlXLlyqX79+lqyZMk/9ksZPHiwcdU7Ly8v5c2bV+3bt9fcuXMVFRVlrPf3D6/OPI6KFStq7ty56tmzp6pWrapixYrJ19dXnp6eypUrl6pUqaIRI0Zo8eLFaZpK4+/vr9mzZxsNnW/evKmuXbva9Q5zlsGDB6t169YKDAxU3rx55eXlpaxZs6p48eJq27atli9fbvfaeRhvvvmm8XN8fLxmzZpl3K5bt65Wr16tLl26qFSpUsqWLZvRnLpixYrq1q2bvvvuO7srqgUHB+u7775TrVq1lC1bNmXLlk01atTQokWL9PTTT9+3jurVq2vatGkKDAw0XmuNGjXS999/r4CAAKcca2r8/Py0ZMkSffjhh3rmmWeUO3duWa1WZcuWTcWLF1fjxo01YcIEu6lYXl5e+uabb9StWzflz59fXl5eevLJJzV8+HCNHTv2oWsqVaqUli5dqpdeekk5c+aUj4+PKlasqK+//loNGjRIdZuSJUvqmWeeMW7ny5dPNWrUcOhxL1++bHf7778XnTt31pIlS/Taa6+pUKFC8vLyko+PjwICAtS5c2etXr1a1apVc+pxOcODPMfJ8ufPf8+/4/1GZmXNmlWzZ8/WxIkTVbt2bfn7+8vT01M+Pj4qWrSoXnnlFY0ePVrDhw9Pl+MEALiWxfb3r3cAAIBL3LlzJ9Vmwr/88ot69uxp3P7uu+/SbQocMp7PP/9c06ZNkyQVKlTI6I8F53j//feN0VI9evTQ4MGD07zt9evX1bVrV7tprWvXrk31qoUAAOBeTN8DACCDmDRpkkJDQ1WnTh0VLlxYCQkJOnTokBYtWmSsU758+Qe6ih6A/3P+/HmdO3dOYWFhWrlypaS7TbZbtWqVpu2vXLmibt266ezZs3ZXPQwODiaQAgDAAYRSAABkEDabTbt27bpvb51ixYppypQpdj1rADhuxYoVxuizZJ06dbKbUvlP4uLijIsZJPP399f48eOdViMAAJkBoRQAABlE3bp1FRkZqT/++EPXrl1TXFyccuTIoZIlS+qll15SixYtlDVrVleXCbgNT09PFSpUSM2bN9cbb7zh0LYWi0XZsmVT0aJFVatWLXXq1El+fn7pVCkAAO6JnlIAAAAAAAAwHVffAwAAAAAAgOkIpQAAAAAAAGC6TNdTKikpSQkJCfLw8KBRLAAAAAAAgJPZbDYlJSXJ09NTHh73Hw+V6UKphIQEHTx40NVlAAAAAAAAuLWgoCB5e3vf9/5MF0olJ3RBQUGyWq0urgbAoy4xMVEHDx7kPQUAADzSOKcB4EzJ7yn/NEpKyoShVPKUPavVypstAKfhPQUAALgDzmkAONO/tU2i0TkAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA07k0lNq9e7d69uypGjVqqHTp0tq0adO/brNz5041adJE5cuX10svvaTvv//ehEoBAAAAAADgTC4NpaKjo1W6dGl98MEHaVr/3LlzevPNN1WtWjX98MMP6tSpk/7zn/9o27Zt6VwpAAAAAAAAnMnTlQ9eu3Zt1a5dO83rL168WIULF9bw4cMlSQEBAdq7d6/mzp2rmjVrpleZAAAAAAAAcDKXhlKO2r9/v5599lm7ZTVq1NBHH33k8L4SExOdVRaATCz5vYT3FAAA8CjjnAaAM6X1veSRCqWuXr0qf39/u2X+/v6KiorSnTt35OPjk+Z9HTx40NnlAcjEeE8BAADugHMaAGZ6pEIpZwoKCpLVanV1GQAecYmJiTp48CDvKQAA4JHGOQ0AZ0p+T/k3j1Qo5e/vr6tXr9otu3r1qnx9fR0aJSVJVquVN1sATsN7CgAAcAec0wAwk0uvvueop556Sjt27LBbtn37dj311FOuKQgAAAAAAAAPxKWh1O3btxUaGqrQ0FBJ0vnz5xUaGqqLFy9KkiZOnKhhw4YZ67du3Vrnzp3TJ598orCwMC1cuFBr165V586dXVE+AAAAAAAAHpBLp+8dOnRIHTt2NG6PGzdOktSkSRONHz9eERERunTpknF/kSJFNGPGDI0bN07z589XgQIFNGbMGNWsWdP02gEAAAAAAPDgXBpKVatWTX/++ed97x8/fnyq26xcuTIdqwIAAAAAAEB6e6R6SgEAAAAAAMA9EEoBAAAAAADAdIRSAPCQsmbN6uoSAAAAAOCRQyj1iEtMsrm6BCBTs1qtKleunKxWq6tLATI1/h4CAAA8elza6BwPz+ph0YDF+3TiSpSrSwEAwCVK5PPVlNYVXV0GAAAAHEQo5QZOXInS4Ys3XV0GAAAAAABAmjF9DwAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAMqaNaurSwCQyRBKAQAAwOUSk2yuLgHI1KxWq8qVKyer1erqUoBMLbP9PfR0dQEAAACA1cOiAYv36cSVKFeXAgCAS5TI56sprSu6ugxTEUoBAAAgQzhxJUqHL950dRkAAMAkTN8DAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6VweSi1cuFB16tRRUFCQWrRooQMHDvzj+nPnztUrr7yi4OBg1a5dWx999JFiY2NNqhYAAAAAAADO4NJQas2aNRo3bpz69OmjFStWqEyZMurWrZsiIyNTXX/16tWaOHGi+vbtqzVr1mjs2LFas2aNJk2aZHLlAAAAAAAAeBguDaXmzJmjli1bqlmzZipRooRGjhwpHx8fLV++PNX19+3bp0qVKqlRo0YqXLiwatSooYYNG/7r6CoAAAAAAABkLC4LpeLi4nT48GFVr179/4rx8FD16tW1b9++VLepWLGiDh8+bIRQ586d09atW1W7dm1TagYAAAAAAIBzeLrqga9fv67ExET5+fnZLffz89PJkydT3aZRo0a6fv262rZtK5vNpoSEBLVu3Vo9e/Z0+PETExMfqO6Mxmq1uroEAAAyBHf5255ZcU4DAMBd7nBOk9ZjcFko9SB27typGTNm6IMPPlBwcLDOnj2rsWPHavr06erTp49D+zp48GA6VWmerFmzqly5cq4uAwCADOHPP/9UTEyMq8vAA+CcBgCA/5OZzmlcFkrlzp1bVqv1nqbmkZGR8vf3T3WbKVOmqHHjxmrRooUkqXTp0oqOjtb777+vXr16ycMj7bMRg4KC+EYOAAA3Urp0aVeXAAAA8NDc4ZwmMTExTYOBXBZKeXt7KzAwUCEhIapbt64kKSkpSSEhIWrfvn2q29y5c+ee4Ck5WLLZbA49vtVqJZQCAMCN8HcdAAC4g8x0TuPS6XtdunTR22+/rfLlyys4OFjz5s1TTEyMmjZtKkkaNmyY8ufPr8GDB0uSXnjhBc2ZM0flypUzpu9NmTJFL7zwQqZ60gAAAAAAAB51Lg2l6tevr2vXrmnq1KmKiIhQ2bJlNWvWLGP63qVLl+xGRvXq1UsWi0WTJ09WeHi48uTJoxdeeEGDBg1y1SEAAAAAAADgAbi80Xn79u3vO11vwYIFdrc9PT3Vt29f9e3b14zSAAAAAAAAkE7S3hkcAAAAAAAAcBJCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6Twd3eDSpUuyWCwqUKCAJOnAgQNavXq1SpQooVatWjm9QAAAAAAAALgfh0dKDR48WDt27JAkRUREqEuXLjp48KA+++wzTZs2zekFAgAAAAAAwP04HEodP35cwcHBkqS1a9eqZMmSWrx4sSZMmKAVK1Y4vUAAAAAAAAC4H4dDqYSEBHl7e0uStm/frjp16kiSihcvroiICOdWBwAAAAAAALfkcChVokQJLV68WHv27NH27dtVq1YtSdKVK1eUK1cuZ9cHAAAAAAAAN+RwKDVkyBD997//VYcOHdSgQQOVKVNGkrR582ZjWh8AAAAAAADwTxy++l61atW0Y8cORUVFKWfOnMbyli1bKmvWrE4tDgAAAAAAAO7J4VBKkqxWq10gJUmFCxd2SkEAAAAAAABwf2kKpV5//XVZLJY07ZAr8AEAAAAAAODfpCmUqlu3bnrXAQAAAAAAgEwkTaFU375907sOAAAAAAAAZCIOX30PAAAAAAAAeFhpGilVpUqVNPeU2rVr10MVBAAAAAAAAPeXplBqxIgR6V0HAAAAAAAAMpE0hVJNmjRJ7zoAAAAAAACQiaQplIqKipKvr6/x8z9JXg8AAAAAAAC4nzT3lPrtt9/k5+enypUrp9pfymazyWKxKDQ01OlFAgAAAAAAwL2kKZSaN2+ecubMafyc1qbnAAAAAAAAQGrSFEpVrVrV+LlSpUry8vJKdb1r1645pyoAAAAAAAC4NQ9HN3jrrbdks9nuWX716lV17NjRKUUBAAAAAADAvTkcSl28eFHvvvuu3bIrV66oQ4cOKl68uNMKAwAAAAAAgPtyOJSaOXOm9u3bp3HjxkmSwsPD1aFDB5UqVUqTJ092dn0AAAAAAABwQ2nqKZVSnjx59M0336ht27aSpC1btqhcuXKaMGGCPDwczrgAAAAAAACQCT1QilSwYEF98803Wr16tYKCgjRp0iRZrVZn1wYAAAAAAAA3laaRUlWqVJHFYrlneUxMjH755RdVq1bNWLZr1y7nVQcAAAAAAAC3lKZQasSIEeldBwAAAAAAADKRNIVSTZo0Se86AAAAAAAAkImkudF5QkKCkpKS5O3tbSy7evWqFi9erOjoaNWpU0eVK1dOlyIBAAAAAADgXtIcSr333nvy8vLSqFGjJElRUVFq3ry5YmNjlTdvXs2bN09ffPGFateunW7FAgAAAAAAwD2k+ep7v//+u15++WXj9g8//KDExERt2LBBq1atUufOnTVr1qx0KRIAAAAAAADuJc2hVHh4uIoVK2bcDgkJ0SuvvKIcOXJIutt36sSJE86vEAAAAAAAAG4nzaFUlixZFBsba9zev3+/goOD7e6Pjo52bnUAAAAAAABwS2kOpcqUKaMffvhBkrRnzx5FRkbqmWeeMe4/e/as8uXL5/wKAQAAAAAA4HbS3Oi8T58+6t69u9auXauIiAg1adLELoTauHGjKlWqlC5FAgAAAAAAwL2kOZSqWrWqvv/+e/3222/Kmzev6tWrZ3d/2bJl7abzAQAAAAAAAPeT5lBKkgICAhQQEJDqfa1atXJKQQAAAAAAAHB/ae4pBQAAAAAAADgLoRQAAAAAAABMRygFAAAAAAAA06UplJo/f75iY2MlSRcvXpTNZkvXogAAAAAAAODe0hRKjR8/XlFRUZKkF198UdeuXUvXogAAAAAAAODe0nT1vXz58mn9+vWqXbu2bDabLl++bIyc+rvHH3/cqQUCAAAAAADA/aQplOrVq5dGjx6t0aNHy2KxqHnz5vesY7PZZLFYFBoa6lABCxcu1OzZsxUREaEyZcrovffeU3Bw8H3Xv3nzpj777DNt3LhRN27cUKFChTRixAjVrl3boccFAAAAAACA66QplGrVqpUaNGigixcvqnHjxpozZ45y58790A++Zs0ajRs3TiNHjlSFChU0b948devWTevWrZOfn98968fFxalLly7y8/PTlClTlD9/fl28eFGPPfbYQ9cCAAAAAAAA86QplJIkX19flSpVSuPGjdPTTz8tb2/vh37wOXPmqGXLlmrWrJkkaeTIkdqyZYuWL1+uHj163LP+8uXL9ddff2nx4sXy8vKSJBUuXPih6wAAAAAAAIC50hxKJWvSpIkk6dChQwoLC5MklShRQoGBgQ7tJy4uTocPH9abb75pLPPw8FD16tW1b9++VLfZvHmznnrqKY0aNUo///yz8uTJo4YNG6p79+6yWq2OHgoAAAAAAABcxOFQKjIyUoMGDdKuXbuMaXM3b95UtWrV9NlnnylPnjxp2s/169eVmJh4zzQ9Pz8/nTx5MtVtzp07px07dqhRo0b6+uuvdfbsWY0cOVIJCQnq27evQ8eRmJjo0PoZFWEcAAB3ucvf9syKcxoAAO5yh3OatB6Dw6HU6NGjdfv2bf30008KCAiQJJ04cUJvv/22xowZo0mTJjm6yzSz2Wzy8/PT6NGjZbVaVb58eYWHh2v27NkOh1IHDx5MpyrNkzVrVpUrV87VZQAAkCH8+eefiomJcXUZeACc0wAA8H8y0zmNw6HUtm3bNGfOHCOQku5O3/vggw/UtWvXNO8nd+7cslqtioyMtFseGRkpf3//VLfJmzevPD097b5JK168uCIiIhQXF+dQn6ugoCC+kQMAwI2ULl3a1SUAAAA8NHc4p0lMTEzTYCCHQ6mkpCSjybjdjjw9lZSUlOb9eHt7KzAwUCEhIapbt66x75CQELVv3z7VbSpVqqQff/xRSUlJ8vDwkCSdPn1aefPmdbjxutVqJZQCAMCN8HcdAAC4g8x0TuPh6AbPPPOMxo4dq/DwcGNZeHi4xo0bp2effdahfXXp0kVLlizRihUrFBYWpg8//FAxMTFq2rSpJGnYsGGaOHGisX6bNm1048YNjR07VqdOndKWLVs0Y8YMtWvXztHDAAAAAAAAgAs5PFLq/fffV69evfTiiy+qQIECkqTLly+rZMmS+vTTTx3aV/369XXt2jVNnTpVERERKlu2rGbNmmVM37t06ZIxIkqSChYsqNmzZ2vcuHFq3Lix8ufPr44dO6p79+6OHgYAAAAAAABcyOFQqmDBglqxYoW2b99uXCUvICBA1atXf6AC2rdvf9/pegsWLLhnWcWKFbVkyZIHeiwAAAAAAABkDA6HUpJksVj03HPP6bnnnnN2PQAAAAAAAMgEHO4pBQAAAAAAADwsQikAAAAAAACYjlAKAAAAAAAApnMolEpISNDKlSt19erV9KoHAAAAAAAAmYBDoZSnp6c++OADxcbGplc9AAAAAAAAyAQcnr4XHBys0NDQ9KgFAAAAAAAAmYSnoxu0adNG48eP1+XLlxUYGKisWbPa3V+mTBmnFQcAAAAAAAD35HAo9dZbb0mSxowZYyyzWCyy2WyyWCyMogIAAAAAAMC/cjiU+vnnn9OjDgAAAAAAAGQiDodShQoVSo86AAAAAAAAkIk43OhcklauXKnWrVurRo0aunDhgiRp7ty52rRpk1OLAwAAAAAAgHtyOJRatGiRxo8fr9q1a+vWrVtKSkqSJD322GOaN2+e0wsEAAAAAACA+3E4lPr22281ZswY9erVSx4e/7d5+fLldezYMacWBwAAAAAAAPfkcCh1/vx5lS1b9p7l3t7eiomJcUpRAAAAAAAAcG8Oh1KFCxdWaGjoPcu3bdumgIAApxQFAAAAAAAA9+bw1fe6dOmiUaNGKS4uTpJ04MAB/fjjj/r66681ZswYpxcIAAAAAAAA9+NwKNWiRQtlyZJFkydPVkxMjAYPHqx8+fJpxIgRatCgQXrUCAAAAAAAADfjcCglSY0bN1bjxo0VExOj6Oho+fn5ObsuAAAAAAAAuLEHCqUkKTIyUqdOnZIkWSwW5cmTx2lFAQAAAAAAwL05HEpFRUVp5MiR+umnn5SUlCRJslqtevXVV/XBBx8oR44cTi8SAAAAAAAA7sXhq+/95z//0YEDBzRjxgzt2bNHe/bs0VdffaVDhw7p/fffT48aAQAAAAAA4GYcHim1ZcsWzZo1S5UrVzaW1axZU2PGjNEbb7zh1OIAAAAAAADgnhweKZUrV65Up+j5+vrqsccec0pRAAAAAAAAcG8Oh1K9evXS+PHjFRERYSyLiIjQp59+qt69ezu1OAAAAAAAALinNE3fe/3112WxWIzbp0+f1gsvvKCCBQtKki5duiQvLy9du3ZNrVu3Tp9KAQAAAAAA4DbSFErVrVs3vesAAAAAAABAJpKmUKpv377pXQcAAAAAAAAyEYevvpfS7du3ZbPZ7Jb5+vo+VEEAAAAAAABwfw6HUufOndPo0aO1a9cuxcbGGsttNpssFotCQ0OdWiAAAAAAAADcj8Oh1NChQyVJH330kfz8/OwaoAMAAAAAAABp4XAo9eeff2r58uUqXrx4etQDAAAAAACATMDD0Q3Kly+vy5cvp0ctAAAAAAAAyCQcHik1duxYffDBBwoPD1fJkiXl6Wm/izJlyjitOAAAAAAAALgnh0Opa9eu6ezZs3rnnXeMZRaLhUbnAAAAAAAASDOHQ6kRI0aoXLlymjRpEo3OAQAAAAAA8EAcDqUuXryoL7/8UsWKFUuPegAAAAAAAJAJONzo/JlnntHRo0fToxYAAAAAAABkEg6PlHrhhRc0btw4HTt2TKVKlbqn0fmLL77otOIAAAAAAADgnhwOpT744ANJ0vTp0++5j0bnAAAAAAAASAuHQymm7gEAAAAAAOBhOdxTCgAAAAAAAHhYDo+UmjZt2j/e37dv3wcuBgAAAAAAAJmDw6HUpk2b7G4nJCTo/PnzslqtKlq0KKEUAAAAAAAA/pXDodTKlSvvWRYVFaXhw4erbt26zqgJAAAAAAAAbs4pPaV8fX3Vr18/TZ061Rm7AwAAAAAAgJtzWqPzW7du6datW87aHQAAAAAAANyYw9P35s+fb3fbZrMpIiJCP/zwg2rVquW0wgAAAAAAAOC+HA6l5s6da3fbw8NDefLkUZMmTdSjRw9n1QUAAAAAAAA35nAotXnz5vSoAwAAAAAAAJmI03pKAQAAAAAAAGmV5pFS77zzzr+uY7FY9NFHHz1UQQAAAAAAAHB/aQ6lbt68ed/7EhMTFRISori4OEIpAAAAAAAA/Ks0h1LTp09PdfmmTZv02WefydvbW3369HFaYQAAAAAAAHBfDjc6T7Z3715NnDhRR44cUbt27dSjRw/lzJnTmbUBAAAAAADATTkcSp04cUITJkzQtm3b9Nprr2nSpEkqUKBAetQGAAAAAAAAN5XmUOrSpUuaOnWqVq1apeeff16rVq1SQEBAetYGAAAAAAAAN5XmUKpevXqyWCzq3LmzKlWqpNOnT+v06dP3rPfiiy86sz4AAAAAAAC4oTSHUrGxsZKk2bNna/bs2amuY7FYFBoa6pzKAAAAAAAA4LbSHEodPXo0PesAAAAAAABAJuLh6gIAAAAAAACQ+RBKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0z1QKHXz5k0tXbpUEydO1I0bNyRJhw8fVnh4uDNrAwAAAAAAgJvydHSDo0ePqkuXLsqRI4cuXLigli1bKleuXNqwYYMuXbqkTz75JD3qBAAAAAAAgBtxeKTU+PHj1aRJE23YsEHe3t7G8tq1a2vPnj1OLQ4AAAAAAADuyeFQ6uDBg2rduvU9y/Pnz6+IiAinFAUAAAAAAAD35nAo5e3traioqHuWnz59Wnny5HFKUQAAAAAAAHBvDodSderU0fTp0xUfH28su3jxoiZMmKCXX37ZqcUBAAAAAADAPTkcSg0fPlzR0dGqXr26YmNj1aFDB7388svKnj27Bg0alB41AgAAAAAAwM04fPW9HDlyaM6cOdqzZ4/+/PNPRUdHKzAwUNWrV0+P+gAAAAAAAOCGHA6lklWuXFmVK1d2Zi0AAAAAAADIJBwOpebPn5/qcovFoixZsqho0aKqUqWKrFbrQxcHAAAAAAAA9+RwKDV37lxdv35dMTExypkzpyTpr7/+UtasWZUtWzZFRkaqSJEimj9/vgoWLOj0ggEAAAAAAPDoc7jR+VtvvaXy5ctrw4YN2rlzp3bu3Kn169crODhY7777rrZs2SJ/f3+NGzcuPeoFAAAAAACAG3A4lJo8ebJGjBihokWLGsuKFSumt99+WxMnTlSBAgU0dOhQ/f77704tFAAAAAAAAO7D4VAqIiJCCQkJ9yxPSEjQ1atXJUn58uXT7du3H746AAAAAAAAuCWHQ6lq1arpgw8+0JEjR4xlR44c0YcffqhnnnlGknTs2DEVLlzYeVUCAAAAAADArTjc6Hzs2LEaNmyYmjZtKk/Pu5snJibq2Wef1dixYyVJ2bJl09tvv+3cSgEAAAAAAOA2HA6l8ubNqzlz5igsLEynT5+WJD355JMqXry4sU7yiCkAAAAAAAAgNQ5P30sWEBCgF198US+++KJdIPUgFi5cqDp16igoKEgtWrTQgQMH0rTdTz/9pNKlS6t3794P9fgAAAAAAAAwl8MjpSTp8uXL+vnnn3Xp0iXFx8fb3ffOO+84tK81a9Zo3LhxGjlypCpUqKB58+apW7duWrdunfz8/O673fnz5/Xxxx+rcuXKD3IIAAAAAAAAcCGHQ6mQkBD16tVLRYoU0cmTJ1WyZElduHBBNptN5cqVc7iAOXPmqGXLlmrWrJkkaeTIkdqyZYuWL1+uHj16pLpNYmKihgwZon79+mnv3r26efOmw48LAAAAAAAA13E4lJo4caK6du2q/v37q2LFivr888+VJ08eDRkyRDVr1nRoX3FxcTp8+LDefPNNY5mHh4eqV6+uffv23Xe76dOny8/PTy1atNDevXsdPQRJd4Mtd2C1Wl1dAgAAGYK7/G3PrDinAQDgLnc4p0nrMTgcSoWFhWnSpEl3N/b01J07d5Q9e3YNGDBAvXv3Vtu2bdO8r+vXrysxMfGeaXp+fn46efJkqtvs2bNHy5Yt08qVKx0t3c7BgwcfavuMIGvWrA80Og0AAHf0559/KiYmxtVl4AFwTgMAwP/JTOc0DodS2bJlM/pI5c2bV2fPnlXJkiUl3Q2Z0lNUVJSGDRum0aNHK0+ePA+1r6CgIL6RAwDAjZQuXdrVJQAAADw0dzinSUxMTNNgIIdDqQoVKmjv3r0KCAhQ7dq19fHHH+vYsWPauHGjKlSo4NC+cufOLavVqsjISLvlkZGR8vf3v2f9c+fO6cKFC+rVq5exLCkpSZJUrlw5rVu3TkWLFk3TY1utVkIpAADcCH/XAQCAO8hM5zQOh1LvvPOObt++LUnq16+fbt++rTVr1uiJJ57Q8OHDHdqXt7e3AgMDFRISorp160q6GzKFhISoffv296xfvHhxrV692m7Z5MmTdfv2bb377rsqUKCAo4cDAAAAAAAAF3AolEpMTNTly5eNoWTZsmXTqFGjHqqALl266O2331b58uUVHBysefPmKSYmRk2bNpUkDRs2TPnz59fgwYOVJUsWlSpVym77xx57TJLuWQ4AAAAAAICMy6FQymq1qmvXrlqzZo0RBj2s+vXr69q1a5o6daoiIiJUtmxZzZo1y5i+d+nSJXl4eDjlsQAAAAAAAJAxODx9r2TJkjp//ryKFCnitCLat2+f6nQ9SVqwYME/bjt+/Hin1QEAAAAAAABzODwEaeDAgfr444/1yy+/6MqVK4qKirL7DwAAAAAAAPg3Do+U6tGjhySpV69eslgsxnKbzSaLxaLQ0FDnVQcAAAAAAAC35HAoNX/+/PSoAwAAAAAAAJmIw6FU1apV06MOAAAAAAAAZCIPdFm7PXv2aMiQIWrdurXCw8MlSStXrtSePXucWhwAAAAAAADck8Oh1Pr169WtWzf5+Pjo8OHDiouLkyRFRUVpxowZTi8QAAAAAAAA7sfhUOrLL7/UyJEjNWbMGHl6/t/sv0qVKunIkSNOLQ4AAAAAAADuyeFQ6tSpU6pcufI9y3PkyKGbN286pSgAAAAAAAC4N4dDKX9/f509e/ae5Xv37lWRIkWcUhQAAAAAAADcm8OhVMuWLTV27Fj98ccfslgsCg8P16pVq/Txxx+rTZs26VEjAAAAAAAA3Iznv69ir0ePHkpKSlLnzp0VExOj9u3by9vbW127dlWHDh3So0YAAAAAAAC4GYdDKYvFol69eqlbt246e/asoqOjFRAQoOzZs6dHfQAAAAAAAHBDDk/f++GHHxQTEyNvb2+VKFFCwcHBBFIAAAAAAABwiMOh1Lhx41S9enUNHjxYW7duVWJiYnrUBQAAAAAAADfm8PS93377Tdu2bdOPP/6ogQMHysfHR/Xq1VOjRo1UqVKl9KgRAAAAAAAAbsbhUMrT01MvvPCCXnjhBcXExGjjxo368ccf1bFjRxUoUECbNm1KjzoBAAAAAADgRhwOpVLKmjWratSooZs3b+rixYsKCwtzVl0AAAAAAABwYw8USiWPkFq9erVCQkJUsGBBNWjQQFOmTHF2fQAAAAAAAHBDDodSgwYN0pYtW+Tj46NXX31VvXv3VsWKFdOjNgAAAAAAALgph0MpDw8PTZ48WTVq1JDVarW779ixYypVqpTTigMAAAAAAIB7cjiUmjhxot3tqKgo/fTTT1q6dKkOHz6s0NBQpxUHAAAAAAAA9/TAjc53796tZcuWacOGDcqXL59eeuklvf/++86sDQAAAAAAAG7KoVAqIiJCK1as0LJlyxQVFaVXX31VcXFxmj59ukqUKJFeNQIAAAAAAMDNpDmU6tmzp3bv3q3nn39eI0aMUM2aNWW1WrV48eL0rA8AAAAAAABuKM2h1K+//qoOHTqoTZs2euKJJ9KxJAAAAAAAALg7j7SuuGjRIt2+fVtNmzZVixYt9O233+ratWvpWRsAAAAAAADcVJpDqaeeekpjxozRb7/9platWumnn35SrVq1lJSUpP/973+KiopKzzoBAAAAAADgRtIcSiXLli2bmjdvru+++06rVq1Sly5dNHPmTFWvXl09e/ZMjxoBAAAAAADgZhwOpVIqXry4hg0bpq1bt2rSpEnOqgkAAAAAAABuLs2Nzv+J1WpV3bp1VbduXWfsDgAAAAAAAG7uoUZKAQAAAAAAAA+CUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6TJEKLVw4ULVqVNHQUFBatGihQ4cOHDfdZcsWaK2bduqSpUqqlKlijp37vyP6wMAAAAAACDjcXkotWbNGo0bN059+vTRihUrVKZMGXXr1k2RkZGprr9z5041aNBA8+fP1+LFi1WwYEF17dpV4eHhJlcOAAAAAACAB+XyUGrOnDlq2bKlmjVrphIlSmjkyJHy8fHR8uXLU11/4sSJateuncqWLauAgACNGTNGSUlJCgkJMblyAAAAAAAAPCiXhlJxcXE6fPiwqlevbizz8PBQ9erVtW/fvjTtIyYmRgkJCcqZM2d6lQkAAAAAAAAn83Tlg1+/fl2JiYny8/OzW+7n56eTJ0+maR8TJkxQvnz57IKttEhMTHRo/YzKarW6ugQAADIEd/nbnllxTgMAwF3ucE6T1mNwaSj1sL7++mutWbNG8+fPV5YsWRza9uDBg+lUlXmyZs2qcuXKuboMAAAyhD///FMxMTGuLgMPgHMaAAD+T2Y6p3FpKJU7d25ZrdZ7mppHRkbK39//H7edPXu2vv76a82ZM0dlypRx+LGDgoL4Rg4AADdSunRpV5cAAADw0NzhnCYxMTFNg4FcGkp5e3srMDBQISEhqlu3riQZTcvbt29/3+1mzpypr776SrNnz1ZQUNADPbbVaiWUAgDAjfB3HQAAuIPMdE7j8ul7Xbp00dtvv63y5csrODhY8+bNU0xMjJo2bSpJGjZsmPLnz6/BgwdLujtlb+rUqZo4caIKFSqkiIgISVK2bNmUPXt2lx0HAAAAAAAA0s7loVT9+vV17do1TZ06VRERESpbtqxmzZplTN+7dOmSPDz+7yKBixcvVnx8vPr372+3n759+6pfv36m1g4AAAAAAIAH4/JQSpLat29/3+l6CxYssLu9efNmM0oCAAAAAABAOvL491UAAAAAAAAA5yKUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkyRCi1cOFC1alTR0FBQWrRooUOHDjwj+uvXbtW9erVU1BQkBo1aqStW7eaVCkAAAAAAACcweWh1Jo1azRu3Dj16dNHK1asUJkyZdStWzdFRkamuv7vv/+uwYMHq3nz5lq5cqVefPFF9enTR8eOHTO5cgAAAAAAADwol4dSc+bMUcuWLdWsWTOVKFFCI0eOlI+Pj5YvX57q+vPnz1fNmjX1xhtvKCAgQAMHDlS5cuX07bffmlw5AAAAAAAAHpRLQ6m4uDgdPnxY1atXN5Z5eHioevXq2rdvX6rb7N+/X88++6zdsho1amj//v3pWSoAAAAAAACcyNOVD379+nUlJibKz8/Pbrmfn59OnjyZ6jZXr16Vv7//PetfvXo1TY9ps9kk3Q3ErFbrA1SdsVitVpUtkF1ZHv1DAQDggRTPm12JiYlKTEx0dSl4CJzTAAAyO3c6p0k+huQM5n5cGkq5QlJSkiTpyJEjLq7EedoESArI5uoyAABwERsjpt0E5zQAgMzN/c5pkjOY+3FpKJU7d25ZrdZ7mppHRkbeMxoqmb+//z2jov5p/b/z9PRUUFCQPDw8ZLFYHqxwAAAAAAAApMpmsykpKUmenv8cO7k0lPL29lZgYKBCQkJUt25dSXdTtJCQELVv3z7VbZ566int2LFDnTt3NpZt375dTz31VJoe08PDQ97e3g9bOgAAAAAAAB6Cy6++16VLFy1ZskQrVqxQWFiYPvzwQ8XExKhp06aSpGHDhmnixInG+h07dtS2bdv0zTffKCwsTJ9//rkOHTp03xALAAAAAAAAGY/Le0rVr19f165d09SpUxUREaGyZctq1qxZxnS8S5cuycPj/7KzSpUqacKECZo8ebImTZqkJ554QtOnT1epUqVcdQgAAAAAAABwkMX2b63QAQAAAAAAACdz+fQ9AAAAAAAAZD6EUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAADgppIvtn7z5k0XVwIA9yKUAgAAAAA3ZbFYtHHjRv3nP/9RRESEq8sBADuEUgCQQvK3iZKUmJjowkoAAAAeXPI5zenTpzV58mTVrl1bfn5+Lq4KAOx5uroAAMgobDabLBaLfvvtN23ZskWhoaFq2LChypUrpwoVKri6PAAAgDSzWCw6cOCAduzYoeDgYDVq1MjVJQHAPRgpBQD/n8Vi0aZNm9SvXz95enqqcuXKWrZsmcaNG6dz5865ujwAAACHzJ49W5MmTdKBAwcUFxcnDw8Pu1HhAOBqhFIA8P+Fh4friy++0NChQzV8+HD16tVLp0+f1tNPP60iRYq4ujwAAACHTJkyRS1btlRERIS+//57RUVFyWKxuLosADAQSgHItP7+TaHNZlN8fLzq1auns2fP6uWXX1b9+vU1dOhQSdLu3bt19epVV5QKAADwj5LPa27duqWYmBhFRUVJkkaNGqXnnntO3333nTZu3KiYmBi79QHAlQilAGQqSUlJxv+Tvyk8efKkbty4obi4OCUmJurEiRPq2rWratasqZEjR0qSjh8/ruXLl+vixYsuqx0AACA1yX0xN2/erAEDBuj111/X8OHDtXDhQknSZ599plKlSmnmzJlav369oqOjGTEFIEMglAKQqXh4eOj8+fN64403JEmbN2/Wm2++qcjISBUtWlRly5ZVx44d9fTTT2vs2LHy8Lj7Nrlq1SodO3ZMBQoUcGX5AAAA97BYLNq6dasGDBigKlWqqGnTpsqXL58+/vhjTZkyRdLdqXxly5bVp59+qs2bNzNSCkCGwNX3AGQ6Fy9e1KVLl9SgQQOdPHlSEydOVEBAgCSpX79+unHjhrZv365ffvlFf/31lw4fPqzly5dr0aJFypcvn4urBwAAmV3yyKhkcXFxWrlypdq3b69evXpJkqKiolS6dGl99NFHevzxx9WiRQtNnDhR//nPfxQcHMxIKQAZAiOlAGQ6VatW1euvv66wsDAVLVpU9evXl3R3Sl+xYsX0zjvvqGrVqho1apRmz56tM2fOaNGiRSpTpoyLKwcAAJndtGnT1KNHD7tlNptNYWFhio+PN5b5+vqqQYMGatCggXbv3q07d+5IksaMGaOiRYuaWjMA3A8jpQBkKklJSfLw8FCxYsXUu3dv/frrr2rRooUWLFggHx8fJSUlqUSJEpo4caLCw8OVPXt2WSwWZc+e3dWlAwAA6Pnnn9crr7xityxLliyqWbOmjh49qjNnzqhYsWKS7gZTefLkUUhIiDw9+egHIONhpBSATCG5b0Jyj6h69eqpf//+GjRokBISEtShQwfFxcUZ9+/fv1/e3t7y9fUlkAIAABlG+fLlVbJkSe3cuVNt27ZVYmKiJCk4OFiXL1/W8uXLdebMGWP9W7duqVChQsZ6AJCRWGx0uAPg5pL7Lhw4cED79++Xh4eHgoODFRwcrPj4eO3evVuffvqpPDw8NHnyZC1btkwbNmzQggUL5O/v7+ryAQAA7nHgwAF1795dpUuX1rx582SxWLRgwQL997//Va5cuVSoUCElJCTol19+oQ0BgAyLUAqAW0sOpDZs2KBRo0apcOHCypo1qw4ePKgJEybo+eefV3x8vPbv36/x48fr8uXLypIliyZPnqzg4GBXlw8AAJCqxMREHTlyRAMGDFCBAgW0aNEiSXevLBwaGqpdu3bpiSeeULt27VSqVCkXVwsAqSOUAuB2/n5Fmr1796pfv34aMGCAWrVqpdDQUDVp0kSS9Mknn6hx48ZKSkpSTEyMjhw5oqJFiyp//vyuKh8AAMBO8rlNWFiYrl69qgIFChh9ow4cOKCBAweqQIECWrhwoXEOlDxdz2q1uqxuAPg3hFIA3Mq8efP05JNPqlatWrLZbIqPj9ecOXMUHR2tQYMG6fLly2rTpo2qV68uHx8fLVq0SJ9//rnq1q3r6tIBAADua8OGDRo+fLj8/Px07tw5DR06VE2aNFGePHmMYKpw4cL65ptvaGoO4JFBo3MAbsFms+nOnTv69ddfjcscWywWeXt7q06dOqpdu7aio6M1cOBA1ahRQ2PHjlWTJk3k4eGhvn37at26dS4+AgAAAHvJ4wcuXbqkGTNmaNiwYZo/f77efvttTZs2TfPnz1dkZKSCg4M1ZcoUHTp0SH369HFx1QCQdkToANyGj4+PZs6cKQ8PD+3bt0/Xr19X7dq1VbJkSUnS0aNHFRcXpw4dOkiScuTIoXr16ql48eLGOgAAABmFxWLR9u3bdfjwYZUrV05NmzaVt7e3unTpoixZsmjChAmSpI4dOyooKEjffvstVw0G8EghlALgFpL7JyQlJSkpKUnjx49XTEyMrFaratasKQ8PD924cUNHjhzRzZs3JUnff/+9rl27plGjRnECBwAAMqSdO3dqxowZKlKkiK5fv270vWzbtq0kacqUKYqJidGbb76pcuXKubJUAHAY0/cAuIXk4e137tyRp6envvzyS+XMmVNffvmltm7dqsTERD3zzDNq1KiR2rdvryZNmmjBggV6++23CaQAAECGNWjQIA0ePFjnzp3T2rVrdfv2beO+tm3bqmfPnlq/fr0LKwSAB0ejcwCPvOQr0mzdulXfffed+vbtq/Lly+uvv/5Sr169lJSUpJ49e+r5559XTEyMNm3apKioKD333HNG/ykAAABXSz6nuXjxomJiYhQfH68yZcpIkiZNmqRZs2bp3XffVZMmTZQtWzZju5s3b+qxxx5zVdkA8MCYvgfgkWexWLRx40YNGzZMnTp1UmxsrGw2mzFSqlevXvryyy9lsVhUo0YNNWrUyNUlAwAA2EkOpDZu3Kjp06fr1q1b8vPzU86cOTVz5ky99dZbslqtGjt2rKxWqxo1amSM9s6RI4eLqweAB8P0PQCPvEuXLmnixIkaMGCABg4cqKeffloWi0Xx8fFGMOXt7a2PP/5Y27dvd3W5AAAA97BYLAoJCdHQoUPVqlUrLV68WO3atdO2bdu0bNkySdKAAQP05ptv6sMPP9TatWuN9gXJvTUB4FFDKAXgkXf9+nUlJCSoVq1aku5+02iz2eTl5aWkpCTlzJlTU6ZM0eOPP64nn3zSxdUCAACkbseOHWrbtq3atGmjpKQkTZkyRW3btlXz5s2NdQYMGKD+/furYsWKhFEAHnmEUgAeWSlb4kVFReny5cuS7n5bmHzftm3bdPDgQeXJk0dff/21Chcu7JJaAQAA/s3x48fl5eWlyMhItWzZUjVq1NB7770nSVq9erWWLl0qSerdu7cCAgJcWSoAOAWhFIBHSsogKvnbwVy5csnPz0+rV6/WpUuXJEkeHnff3n7++WfNmzdPcXFxfJsIAAAyjORzmqioKGNZ1apVderUKTVt2lS1atXSqFGjJEmxsbHavXu3Lly4oLi4OJfUCwDpgUbnAB4ZyQ1ADxw4oJMnT+qvv/7SCy+8oKJFi2rYsGHq37+/JOnFF19Uvnz5tHr1aq1bt04LFy6Ut7e3i6sHAAC4K/mcZsuWLVq6dKn69OmjcuXKqUqVKpozZ468vLzUqVMnSVJcXJy++OILbd26VXPnzuWcBoBbsdhSDjsAgAxu3bp1eu+991S4cGFFRUUpIiJCgwcPVocOHbR9+3Z9/vnnOn36tB577DH5+Pho/PjxKlu2rKvLBgAAsLNhwwYNHz5cHTp0UKNGjVSiRAlJ0p49e9SrVy+VKFFCSUlJ8vf31++//67Zs2erXLlyLq4aAJyLUArAI+P48ePq3LmzhgwZopdeekm+vr767LPPtHjxYvXv31/t2rXTtWvXFBMTo9jYWOMyygAAABnJqVOn1LlzZ/Xs2VNt2rQxlp88eVLFixfXuXPn9Ouvv+rIkSMqV66catSooWLFirmwYgBIH0zfA/DIuH79unx9fVW1alVly5ZNkjRo0CDZbDZNnDhRdevWVf78+V1cJQAAwD+7efOm8uTJo3r16un69etas2aNNmzYoCNHjqhmzZoaNGiQ2rVr5+oyASDd0egcwCPj5s2bunz5srJmzSoPDw/duXNHktS3b1/lyJFDISEhLq4QAADg3+XJk0ehoaEaPXq0WrVqpe3bt6tSpUp6//33FRISoiNHjri6RAAwBaEUgAwpeWbx0aNHtWfPHklSnTp1VKJECQ0dOlRxcXHy8fGRzWZTVFSUsmfPrhw5criyZAAAAMOtW7dSXZ6UlKQiRYpowYIFslgsaty4sYYPH64BAwaoUaNGKl68uGJiYkyuFgBcg1AKQIaTfEWaDRs2qHfv3tq5c6fOnz8vi8WiXr166fr16+revbvOnz+vEydOaOHChbp586bKlCnj6tIBAAC0YMECffrppzp79uw993l4eMhms6lKlSr6+OOP1bdvXxUpUkSSNGnSJJ09e1aVK1c2u2QAcAl6SgHIcCwWi7Zt26Zhw4Zp2LBhatasmbJkySJJqlu3rnx8fDRlyhQ1aNBABQoUUGJiombMmKFChQq5uHIAAIC7wdOmTZvk6+urNm3aGKFTMovFIkny9Lz7cWzNmjXauHGjdu/era+//lqFCxc2vWYAcAVCKQAZis1mU2xsrJYuXar27durbdu2ioqK0pkzZ7RhwwZlz55dXbp0UY0aNRQSEqLcuXMrT548ypcvn6tLBwAAkCS1a9dO2bJl0+TJkyVJrVq1uu/V85KSkpQ3b1499thjmjdvngICAkysFABci1AKQIZisVjk4+MjLy8vhYWFKSwsTPPmzdPZs2cVGRmpK1euaN++fZo6daqeffZZV5cLAABgJyEhQZ6enmrQoIH+/PNPrVq1SjabTe3bt091VLeHh4eqVKmiChUqyNvb2wUVA4Dr0FMKgMslNzU/ceKEDhw4IEl6+umndfPmTTVs2FA3btxQy5YttXz5cvXp00fXr19XbGysK0sGAABIlaenp9asWaP69esrMjJSuXLl0pw5czR37lydO3fuvtsRSAHIjBgpBcClUjY1Hz9+vNq2basiRYqoVatWqlmzpsLDw+2afZ44cUI5c+Y0ejEAAABkJCdPntSYMWM0aNAgvfbaa/L29tbChQv12WefSZI6d+5MH0wA+P8IpQC4lMVi0a+//qq3335bQ4YM0WuvvSZfX19JUpEiRVSkSBHZbDadOXNGixcv1tq1a/Xtt9/ybSIAAHC5hQsXKm/evHr55ZeNZfHx8fL29laZMmWM85V27dpJkkaPHq0sWbKoadOmKl68uEtqBoCMhOl7AFzGZrMpJiZGixYtUrt27dSuXTtZLBadPn1a33zzjebNmydJOnTokGbMmKHt27drwYIFKl26tIsrBwAAmd21a9cUEhKiMmXK2C2PjY1VdHS00Wrgzp07ku4GU0WKFNH8+fO1evVqxcfHm14zAGQ0FltyMxcASEdJSUny8PBI9fbAgQOVPXt2derUSd99951OnTqlc+fOKSEhQc8995w++ugj7dy5U0888YTy58/vqkMAAAAwWg9IdwMnHx8fHThwQOfOnVODBg0kSQMGDNC+ffu0cuVK5cmTR5J0+/ZtjRkzRoULF1bDhg3vezU+AMhMGCkFwBQeHh4KCwvTZ599pgsXLtj1hCpevLhOnDih119/XdeuXVOLFi30ww8/qHnz5rp27ZokqVq1agRSAADApZKSkmSxWHTt2jX99ddf8vHxUVRUlKZMmaL58+drzZo1kqRhw4apcOHCaty4sbZu3aodO3ZoxowZ2r9/vzp16kQgBQD/HyOlAJgiPj5ebdq00aFDh1SsWDHVqVNHQUFBql+/viTp1KlTunTpkqpXr26Monrvvfd08+ZNffrpp/Ly8qK5OQAAcLkzZ86oa9eueu655zRw4EDlyZNHJ06c0IQJExQTE6O2bdvqlVdeUXh4uD755BOFhITIx8dHFotFU6ZMUfny5V19CACQYRBKATDNrFmz5OnpqZIlS+r333/XggULVLNmTVWtWlUtW7Y0Qqfz589r4cKFWrZsmRYuXKhSpUq5uHIAAIC7I6WmTZumL774Qs8884xKlCihHj16KF++fAoLC9O4ceMUFxentm3bql69epKkY8eOKWvWrMqaNav8/f1dfAQAkLEQSgEwzc6dO9W7d2/NnTtXQUFBunLlipYsWaKZM2eqdOnSat68uaKjoxUeHq5ff/1VEyZMUNmyZV1dNgAAgCE0NFQdO3ZU+fLl5eHhoeLFi6tHjx7KmzevEUzFx8erVatWxohwAEDq6CkFwDTVqlVTy5YtNW/ePMXGxhrfKj7++OMqXry41q1bpwkTJuj27duaP38+gRQAAHCplN/f22w2JSYmqmzZsmrfvr3KlCmj4OBg7d27VzNnzlRERIQCAgL0zjvvKGvWrJo9e7Y2btzowuoBIOPzdHUBADKXChUqaO7cufLy8tK7776rXbt2ae7cuSpZsqTCwsK0Y8cOVa1aVX5+fq4uFQAAZGLJPS5v3LihxMRE+fn5GVcOfvzxx7V06VLNmTNHuXLl0qpVqyRJ3bt3V0BAgAYPHqzPP/9cgYGBrjwEAMjwmL4HwHTt27fX3r175e/vr5kzZ6pMmTKuLgkAAOAep0+fVvfu3ZUlSxYNGjRITz75pIoXLy5J6tixo4KCgjR06FB98cUX2rJli55++ml17txZ+fPnV3x8vLy8vFx8BACQsTFSCoBpbDabLBaLunfvroiICA0dOlRlypQxlgMAAGQUSUlJWrFiha5evars2bNr2rRpKlq0qHLnzq0hQ4aocePG2rt3r+Li4tS7d29ZLBatXLlSXl5eGjBggDw9+agFAP+Gd0oApkkOngIDA2Wz2XTo0CHVrVuXQAoAAGQ4Hh4eateunaKjo3Xx4kXlzJlTDRs21MSJEzV06FDFxMRox44dqly5spo1a6ZevXrJ29tbr7zyiqxWq6vLB4BHAo3OAZjO399fffr00bx583TgwAFXlwMAAJCqfPny6Y033lD+/Pl18uRJnTlzRsuXL1fr1q2NC7Jkz57dWL9bt24qXLiwq8oFgEcOPaUAuER4eLiGDBmiTz/9VAUKFHB1OQAAAPd15coVzZgxQ/v27VPjxo3VuXNnSdK5c+dUpEgR1xYHAI8wQikALhMbG6ssWbK4ugwAAIB/FRERoa+++koHDhzQiy++qJ49e0qSEhMTma4HAA+IUAoAAAAA0iA5mDpy5IieffZZ9e/f39UlAcAjjZ5SAAAAAJAGefPmVc+ePfXEE09o3759un79uqtLAoBHGiOlAAAAAMABV69elXT34i0AgAdHKAUAAAAAAADTMX0PAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGC6/wdEuCgpBFxuLgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------\n",
        "# GOAL 5: CORRELATION ANALYSIS - SKILL COUNT VS. JOB LEVEL\n",
        "# ----------------------------------------------------\n",
        "# Skill count distribution\n",
        "print(\"\\nüìä Skill Count Distribution\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Get statistics on skill counts\n",
        "skill_stats = df_final.select('skill_count').describe().toPandas()\n",
        "print(\"\\nSkill Count Statistics:\")\n",
        "print(skill_stats)\n",
        "\n",
        "# Distribution by job level\n",
        "print(\"\\nüìà Average Skills by Job Level:\")\n",
        "skills_by_level = df_final.groupBy('job_level') \\\n",
        "    .agg(\n",
        "        avg('skill_count').alias('avg_skills'),\n",
        "        count('*').alias('job_count')\n",
        "    ) \\\n",
        "    .orderBy(desc('avg_skills')) \\\n",
        "    .toPandas()\n",
        "\n",
        "print(skills_by_level.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "skills_by_level_top = skills_by_level.head(10)\n",
        "plt.bar(range(len(skills_by_level_top)), skills_by_level_top['avg_skills'])\n",
        "plt.xticks(range(len(skills_by_level_top)), skills_by_level_top['job_level'], rotation=45, ha='right')\n",
        "plt.ylabel('Average Number of Skills')\n",
        "plt.title('Average Skills Required by Job Level', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 6:"
      ],
      "metadata": {
        "id": "VA4NqgooVrPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# GOAL 6: PREDICTIVE MODELING - PREDICTING JOB LEVEL (LOGISTIC REGRESSION)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOAL 6: PREDICTIVE MODELING - PREDICTING JOB LEVEL (LOGISTIC REGRESSION)\")\n",
        "print(\"=\"*80)\n",
        "#\n",
        "\n",
        "# 1. Define Features (X) and Target (y)\n",
        "# Features: Skill count + the entire TF-IDF vector\n",
        "X = df_ml[['skill_count'] + TFIDF_COLS]\n",
        "y = df_ml['level_index']\n",
        "\n",
        "# 2. Split Data (70% Training, 30% Testing)\n",
        "train_start = time.time()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y # Important for imbalanced classes\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining data split: {len(X_train):,} (70%) vs {len(X_test):,} (30%)\")\n",
        "\n",
        "# 3. Train Logistic Regression Model (Multinomial for multiple classes)\n",
        "print(\"Training Scikit-learn Logistic Regression Model...\")\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000, # Increased for convergence\n",
        "    multi_class='multinomial', # Handles multiple job levels\n",
        "    solver='lbfgs',\n",
        "    random_state=42\n",
        ")\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Model training complete.\")\n",
        "\n",
        "# 4. Evaluate Model\n",
        "predictions = lr_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "report = classification_report(y_test, predictions, target_names=le.classes_, zero_division=0)\n",
        "\n",
        "print(f\"\\nModel Evaluation (Logistic Regression):\")\n",
        "print(f\"Accuracy on Test Data: **{accuracy:.4f}**\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "print(f\"‚úÖ Goal 6 completed in {time.time()-train_start:.1f}s\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# SUMMARY\n",
        "# -----------------------------------------------------------------------------\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ ALL ADVANCED ANALYTICS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"‚è±Ô∏è Total execution time (Goals 2-6): {total_time:.1f}s\")\n",
        "print(f\"üìä Processed {len(df_pandas):,} job postings\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "mDLORODJVq_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LwOTn3731yA"
      },
      "source": [
        "## Section 7: Machine Learning - Classification (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vtwSJzT31yA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "dcb740f1-c2fd-4529-846e-6e6ce9042205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MACHINE LEARNING - CLASSIFICATION\n",
            "======================================================================\n",
            "\n",
            "üéØ Problem: Predict if a job is 'High Demand' based on features\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üîß Feature engineering...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `applies` cannot be resolved. Did you mean one of the following? [`job_link`, `job_level`, `job_title`, `location`, `first_seen`].;\n'Filter isnotnull('applies)\n+- Project [job_link#0, job_title#1130, company_name#1131, location#1132, job_level#12, employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8, skills_list#3087, coalesce(skill_count#3089L, cast(0 as bigint)) AS skill_count#4027L]\n   +- Project [job_link#0, job_title#1130, company_name#1131, location#1132, job_level#12, employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8, skills_list#3087, skill_count#3089L]\n      +- Join LeftOuter, (job_link#0 = job_link#64)\n         :- Filter (isnotnull(job_title#1130) AND isnotnull(job_link#0))\n         :  +- Project [job_link#0, trim(lower(job_title#5), None) AS job_title#1130, company#6 AS company_name#1131, job_location#7 AS location#1132, job_level#12, job_type#13 AS employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8]\n         :     +- Repartition 100, true\n         :        +- Deduplicate [job_link#0]\n         :           +- Repartition 100, true\n         :              +- Relation [job_link#0,last_processed_time#1,got_summary#2,got_ner#3,is_being_worked#4,job_title#5,company#6,job_location#7,first_seen#8,search_city#9,search_country#10,search_position#11,job_level#12,job_type#13] csv\n         +- Aggregate [job_link#64], [job_link#64, collect_list(skill#3080, 0, 0) AS skills_list#3087, count(skill#3080) AS skill_count#3089L]\n            +- Filter isnotnull(skill#3080)\n               +- Project [job_link#64, trim(lower(job_skills#65), None) AS skill#3080]\n                  +- Repartition 100, true\n                     +- Relation [job_link#64,job_skills#65] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1157058276.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get median applies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmedian_applies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'applies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'applies'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3327\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m             raise PySparkTypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `applies` cannot be resolved. Did you mean one of the following? [`job_link`, `job_level`, `job_title`, `location`, `first_seen`].;\n'Filter isnotnull('applies)\n+- Project [job_link#0, job_title#1130, company_name#1131, location#1132, job_level#12, employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8, skills_list#3087, coalesce(skill_count#3089L, cast(0 as bigint)) AS skill_count#4027L]\n   +- Project [job_link#0, job_title#1130, company_name#1131, location#1132, job_level#12, employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8, skills_list#3087, skill_count#3089L]\n      +- Join LeftOuter, (job_link#0 = job_link#64)\n         :- Filter (isnotnull(job_title#1130) AND isnotnull(job_link#0))\n         :  +- Project [job_link#0, trim(lower(job_title#5), None) AS job_title#1130, company#6 AS company_name#1131, job_location#7 AS location#1132, job_level#12, job_type#13 AS employment_type#1133, search_city#9, search_country#10, search_position#11, first_seen#8]\n         :     +- Repartition 100, true\n         :        +- Deduplicate [job_link#0]\n         :           +- Repartition 100, true\n         :              +- Relation [job_link#0,last_processed_time#1,got_summary#2,got_ner#3,is_being_worked#4,job_title#5,company#6,job_location#7,first_seen#8,search_city#9,search_country#10,search_position#11,job_level#12,job_type#13] csv\n         +- Aggregate [job_link#64], [job_link#64, collect_list(skill#3080, 0, 0) AS skills_list#3087, count(skill#3080) AS skill_count#3089L]\n            +- Filter isnotnull(skill#3080)\n               +- Project [job_link#64, trim(lower(job_skills#65), None) AS skill#3080]\n                  +- Repartition 100, true\n                     +- Relation [job_link#64,job_skills#65] csv\n"
          ]
        }
      ],
      "source": [
        "# ML Problem 1: Classification - Predict High Demand Jobs\n",
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - CLASSIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Predict if a job is 'High Demand' based on features\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Create target variable: High demand if applies > median\n",
        "print(\"\\nüîß Feature engineering...\")\n",
        "\n",
        "# Get median applies\n",
        "median_applies = df_final.filter(col('applies').isNotNull()) \\\n",
        "    .approxQuantile('applies', [0.5], 0.01)[0]\n",
        "\n",
        "print(f\"   Median applies: {median_applies}\")\n",
        "\n",
        "# Create features\n",
        "df_ml = df_final.filter(\n",
        "    col('applies').isNotNull() &\n",
        "    col('skill_count').isNotNull()\n",
        ").withColumn(\n",
        "    'high_demand',\n",
        "    when(col('applies') > median_applies, 1).otherwise(0)\n",
        ").select(\n",
        "    'skill_count',\n",
        "    'views',\n",
        "    'high_demand'\n",
        ").fillna(0)\n",
        "\n",
        "# Sample if dataset too large\n",
        "ml_count = df_ml.count()\n",
        "if ml_count > 100000:\n",
        "    print(f\"\\n‚ö†Ô∏è Dataset large ({ml_count:,} records), sampling 100k for efficiency\")\n",
        "    df_ml = df_ml.sample(False, 100000/ml_count, seed=42)\n",
        "\n",
        "# Split data\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"\\nüìä Dataset split:\")\n",
        "print(f\"   Training: {train_df.count():,}\")\n",
        "print(f\"   Testing: {test_df.count():,}\")\n",
        "\n",
        "# Build pipeline\n",
        "print(\"\\nüèóÔ∏è Building classification model...\")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['skill_count', 'views'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol='scaled_features',\n",
        "    labelCol='high_demand',\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "# Train\n",
        "print(\"\\nüîÑ Training model...\")\n",
        "start = time.time()\n",
        "model = pipeline.fit(train_df)\n",
        "print(f\"‚úÖ Model trained in {time.time()-start:.1f}s\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nüìà Evaluating model...\")\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Metrics\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol='high_demand',\n",
        "    metricName='areaUnderROC'\n",
        ")\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(\n",
        "    labelCol='high_demand',\n",
        "    predictionCol='prediction',\n",
        "    metricName='accuracy'\n",
        ")\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol='high_demand',\n",
        "    predictionCol='prediction',\n",
        "    metricName='f1'\n",
        ")\n",
        "\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "accuracy = evaluator_acc.evaluate(predictions)\n",
        "f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "print(\"\\nüéØ Classification Results:\")\n",
        "print(f\"   AUC-ROC: {auc:.4f}\")\n",
        "print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "print(f\"   F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Classification complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSWffs9Y31yA"
      },
      "source": [
        "## Section 8: Machine Learning - Regression (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RQZFtpO31yA"
      },
      "outputs": [],
      "source": [
        "# ML Problem 2: Regression - Predict Number of Skills\n",
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - REGRESSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Predict skill_count based on job characteristics\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Prepare regression dataset\n",
        "print(\"\\nüîß Preparing features...\")\n",
        "\n",
        "# Index categorical variables\n",
        "job_level_indexer = StringIndexer(\n",
        "    inputCol='job_level',\n",
        "    outputCol='job_level_idx',\n",
        "    handleInvalid='keep'\n",
        ")\n",
        "\n",
        "emp_type_indexer = StringIndexer(\n",
        "    inputCol='employment_type',\n",
        "    outputCol='emp_type_idx',\n",
        "    handleInvalid='keep'\n",
        ")\n",
        "\n",
        "# Create regression dataset\n",
        "df_reg = df_final.filter(\n",
        "    col('skill_count').isNotNull() &\n",
        "    col('job_level').isNotNull() &\n",
        "    col('employment_type').isNotNull()\n",
        ").select(\n",
        "    'job_level',\n",
        "    'employment_type',\n",
        "    'views',\n",
        "    'skill_count'\n",
        ").fillna({'views': 0})\n",
        "\n",
        "# Sample if needed\n",
        "reg_count = df_reg.count()\n",
        "if reg_count > 100000:\n",
        "    print(f\"\\n‚ö†Ô∏è Sampling for efficiency ({reg_count:,} -> 100k records)\")\n",
        "    df_reg = df_reg.sample(False, 100000/reg_count, seed=42)\n",
        "\n",
        "# Split data\n",
        "train_reg, test_reg = df_reg.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"\\nüìä Dataset split:\")\n",
        "print(f\"   Training: {train_reg.count():,}\")\n",
        "print(f\"   Testing: {test_reg.count():,}\")\n",
        "\n",
        "# Build regression pipeline\n",
        "print(\"\\nüèóÔ∏è Building regression model...\")\n",
        "\n",
        "assembler_reg = VectorAssembler(\n",
        "    inputCols=['job_level_idx', 'emp_type_idx', 'views'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "scaler_reg = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "lr_reg = LinearRegression(\n",
        "    featuresCol='scaled_features',\n",
        "    labelCol='skill_count',\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "pipeline_reg = Pipeline(stages=[\n",
        "    job_level_indexer,\n",
        "    emp_type_indexer,\n",
        "    assembler_reg,\n",
        "    scaler_reg,\n",
        "    lr_reg\n",
        "])\n",
        "\n",
        "# Train\n",
        "print(\"\\nüîÑ Training regression model...\")\n",
        "start = time.time()\n",
        "model_reg = pipeline_reg.fit(train_reg)\n",
        "print(f\"‚úÖ Model trained in {time.time()-start:.1f}s\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nüìà Evaluating model...\")\n",
        "predictions_reg = model_reg.transform(test_reg)\n",
        "\n",
        "# Metrics\n",
        "evaluator_rmse = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='rmse'\n",
        ")\n",
        "\n",
        "evaluator_r2 = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='r2'\n",
        ")\n",
        "\n",
        "evaluator_mae = RegressionEvaluator(\n",
        "    labelCol='skill_count',\n",
        "    predictionCol='prediction',\n",
        "    metricName='mae'\n",
        ")\n",
        "\n",
        "rmse = evaluator_rmse.evaluate(predictions_reg)\n",
        "r2 = evaluator_r2.evaluate(predictions_reg)\n",
        "mae = evaluator_mae.evaluate(predictions_reg)\n",
        "\n",
        "print(\"\\nüéØ Regression Results:\")\n",
        "print(f\"   RMSE: {rmse:.4f}\")\n",
        "print(f\"   R¬≤: {r2:.4f}\")\n",
        "print(f\"   MAE: {mae:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Regression complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRp0JEuF31yA"
      },
      "source": [
        "## Section 9: Machine Learning - Clustering (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9njwOB531yB"
      },
      "outputs": [],
      "source": [
        "# ML Problem 3: K-Means Clustering\n",
        "print(\"=\"*70)\n",
        "print(\"MACHINE LEARNING - K-MEANS CLUSTERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüéØ Problem: Cluster jobs based on skill patterns\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Prepare clustering dataset\n",
        "print(\"\\nüîß Preparing features for clustering...\")\n",
        "\n",
        "df_cluster = df_final.filter(\n",
        "    col('skill_count').isNotNull() &\n",
        "    col('views').isNotNull()\n",
        ").select(\n",
        "    'job_link',\n",
        "    'job_title',\n",
        "    'skill_count',\n",
        "    'views'\n",
        ").fillna(0)\n",
        "\n",
        "# Sample for efficiency\n",
        "cluster_count = df_cluster.count()\n",
        "if cluster_count > 50000:\n",
        "    print(f\"\\n‚ö†Ô∏è Sampling for clustering ({cluster_count:,} -> 50k records)\")\n",
        "    df_cluster = df_cluster.sample(False, 50000/cluster_count, seed=42)\n",
        "\n",
        "# Build clustering pipeline\n",
        "print(\"\\nüèóÔ∏è Building K-Means model...\")\n",
        "\n",
        "assembler_cluster = VectorAssembler(\n",
        "    inputCols=['skill_count', 'views'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "scaler_cluster = StandardScaler(\n",
        "    inputCol='features',\n",
        "    outputCol='scaled_features'\n",
        ")\n",
        "\n",
        "# Try different K values\n",
        "print(\"\\nüîç Finding optimal K...\")\n",
        "silhouette_scores = []\n",
        "k_values = [3, 5, 7, 10]\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(k=k, featuresCol='scaled_features', seed=42)\n",
        "    pipeline_cluster = Pipeline(stages=[assembler_cluster, scaler_cluster, kmeans])\n",
        "\n",
        "    model_cluster = pipeline_cluster.fit(df_cluster)\n",
        "    predictions_cluster = model_cluster.transform(df_cluster)\n",
        "\n",
        "    evaluator = ClusteringEvaluator(featuresCol='scaled_features')\n",
        "    score = evaluator.evaluate(predictions_cluster)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "    print(f\"   K={k}: Silhouette Score = {score:.4f}\")\n",
        "\n",
        "# Find optimal K\n",
        "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
        "print(f\"\\n‚úÖ Optimal K: {optimal_k}\")\n",
        "\n",
        "# Final clustering with optimal K\n",
        "print(f\"\\nüîÑ Training final K-Means with K={optimal_k}...\")\n",
        "kmeans_final = KMeans(k=optimal_k, featuresCol='scaled_features', seed=42)\n",
        "pipeline_final = Pipeline(stages=[assembler_cluster, scaler_cluster, kmeans_final])\n",
        "\n",
        "model_final = pipeline_final.fit(df_cluster)\n",
        "predictions_final = model_final.transform(df_cluster)\n",
        "\n",
        "# Cluster distribution\n",
        "print(\"\\nüìä Cluster distribution:\")\n",
        "cluster_dist = predictions_final.groupBy('prediction').count().orderBy('prediction').toPandas()\n",
        "print(cluster_dist.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(cluster_dist['prediction'], cluster_dist['count'])\n",
        "plt.xlabel('Cluster ID')\n",
        "plt.ylabel('Number of Jobs')\n",
        "plt.title(f'K-Means Clustering Results (K={optimal_k})', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ K-Means clustering complete\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSyXgets31yB"
      },
      "source": [
        "## Section 10: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUF6DrDi31yB"
      },
      "outputs": [],
      "source": [
        "# Save key results\n",
        "print(\"=\"*70)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save top skills\n",
        "print(\"\\nüíæ Saving top skills...\")\n",
        "top_skills_pd.to_csv('top_skills_2024.csv', index=False)\n",
        "print(\"   ‚úÖ Saved: top_skills_2024.csv\")\n",
        "\n",
        "# Save ML results summary\n",
        "ml_results = pd.DataFrame({\n",
        "    'Metric': ['AUC-ROC', 'Accuracy', 'F1-Score', 'RMSE', 'R¬≤', 'MAE', 'Optimal_K'],\n",
        "    'Value': [auc, accuracy, f1, rmse, r2, mae, optimal_k]\n",
        "})\n",
        "\n",
        "print(\"\\nüíæ Saving ML results...\")\n",
        "ml_results.to_csv('ml_results_summary.csv', index=False)\n",
        "print(\"   ‚úÖ Saved: ml_results_summary.csv\")\n",
        "\n",
        "print(\"\\nüìä Results Summary:\")\n",
        "print(ml_results.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL RESULTS SAVED\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E273BDc31yB"
      },
      "source": [
        "## Section 11: Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llen7hiL31yB"
      },
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "print(\"=\"*70)\n",
        "print(\"CLEANUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Unpersist cached DataFrames\n",
        "print(\"\\nüßπ Clearing cached data...\")\n",
        "try:\n",
        "    df_postings_clean.unpersist()\n",
        "    df_work.unpersist()\n",
        "    df_skills_agg.unpersist()\n",
        "    df_final.unpersist()\n",
        "    print(\"‚úÖ Cache cleared\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úÖ Data loaded and cleaned\")\n",
        "print(\"‚úÖ EDA completed\")\n",
        "print(\"‚úÖ Machine learning models trained\")\n",
        "print(\"‚úÖ Results saved\")\n",
        "print(\"\\nReady for Phase 2 report!\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}