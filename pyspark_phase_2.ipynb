{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhWMtwj0NkCX1hGv8y0xWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1h8t51/data_intensive_computing_project/blob/main/pyspark_phase_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Initialization\n"
      ],
      "metadata": {
        "id": "0EB5sOhcx1km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-17 -y\n",
        "!pip install pyspark==3.5.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyevxYZKgB5z",
        "outputId": "486c224e-a55f-4fea-a330-8792085dfae2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark environment stabilization"
      ],
      "metadata": {
        "id": "MggJCfY2mIfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Z-N_QNjqo488"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n"
      ],
      "metadata": {
        "id": "bMTmGivxrLyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"LinkedIn_Cleaning\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")\n",
        "    .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "id": "Hz7aKmYzp3LQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading from kaggle"
      ],
      "metadata": {
        "id": "VdLNcEQ2x66T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and API Key Upload (Critical Fixes)\n",
        "\n",
        "!pip install kaggle -q\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# --- 1. UPLOAD THE KEY ---\n",
        "# This line is where the code pauses and asks you to choose the file.\n",
        "print(\"Please upload your 'kaggle.json' file now:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# --- 2. CONFIGURE THE KEY ---\n",
        "# This ensures the key is placed in the required location for the Kaggle CLI\n",
        "# and has the correct permissions (chmod 600).\n",
        "!mkdir -p ~/.kaggle\n",
        "for fn in uploaded.keys():\n",
        "  print(f\"File '{fn}' uploaded.\")\n",
        "  os.rename(fn, 'kaggle.json')\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle API key successfully configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "7g2sVLPf77Pp",
        "outputId": "ff2f7963-e2fd-4566-f807-b55a05274e64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your 'kaggle.json' file now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c39a5e4-94b9-4407-b6c7-ef42dd79d6d3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c39a5e4-94b9-4407-b6c7-ef42dd79d6d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle-2.json to kaggle-2.json\n",
            "File 'kaggle-2.json' uploaded.\n",
            "Kaggle API key successfully configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download, Load, and Merge PySpark DataFrames"
      ],
      "metadata": {
        "id": "O40R-jvLmTlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# --- 1. Download and Unzip Dataset ---\n",
        "print(\"Starting dataset download...\")\n",
        "# This uses the credentials set up in Cell 1\n",
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
        "!unzip -q 1-3m-linkedin-jobs-and-skills-2024.zip -d ./linkedin_dataset\n",
        "\n",
        "print(\"Dataset downloaded and unzipped to ./linkedin_dataset/\")\n",
        "\n",
        "# --- 2. Load and Merge PySpark DataFrames ---\n",
        "# Initialize Spark Session (Ensure it's running before this cell)\n",
        "# Note: Assuming spark session is already running based on previous successful initialization\n",
        "\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"CSE587_Phase2_DIC_Project\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "# Define File Paths\n",
        "base_path = \"./linkedin_dataset\"\n",
        "postings_path = f'{base_path}/linkedin_job_postings.csv'\n",
        "skills_path = f'{base_path}/job_skills.csv'\n",
        "summary_path = f'{base_path}/job_summary.csv'\n",
        "\n",
        "# Load the three CSV files directly into PySpark DataFrames\n",
        "print(\"Loading files directly into PySpark...\")\n",
        "spark_postings = spark.read.csv(postings_path, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "spark_skills = spark.read.csv(skills_path, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "spark_summary = spark.read.csv(summary_path, header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "\n",
        "# Merge DataFrames on 'job_link' (Inner Joins)\n",
        "df_merged_ps = spark_postings.join(spark_skills, on='job_link', how='inner')\n",
        "df_cleaned = df_merged_ps.join(spark_summary, on='job_link', how='inner')\n",
        "\n",
        "# Rename columns\n",
        "df_cleaned = df_cleaned.withColumnRenamed(\"job_skills\", \"skills\")\n",
        "df_cleaned = df_cleaned.withColumnRenamed(\"job_description\", \"description\")\n",
        "\n",
        "print(f\"\\nâœ… Final PySpark DataFrame 'df_cleaned' created. Total records: {df_cleaned.count()}\")\n",
        "df_cleaned.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly-2Ui9q8U-l",
        "outputId": "6eb12509-ac82-46f7-9293-1fc2e11329d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset download...\n",
            "Dataset URL: https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
            "License(s): ODC Attribution License (ODC-By)\n",
            "1-3m-linkedin-jobs-and-skills-2024.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ./linkedin_dataset/job_skills.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./linkedin_dataset/job_summary.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./linkedin_dataset/linkedin_job_postings.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Dataset downloaded and unzipped to ./linkedin_dataset/\n",
            "Loading files directly into PySpark...\n",
            "\n",
            "âœ… Final PySpark DataFrame 'df_cleaned' created. Total records: 1296381\n",
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: timestamp (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: date (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            " |-- job_summary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning using pyspark"
      ],
      "metadata": {
        "id": "LKMJ66EumbeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, desc, lower\n",
        "\n",
        "# --- 0. Initial State and Count ---\n",
        "# Assuming 'df_cleaned' is the initial PySpark DataFrame with 1296381 records\n",
        "df_initial = df_cleaned\n",
        "initial_count = df_initial.count()\n",
        "print(f\"--- Initial State ---\")\n",
        "print(f\"Total records BEFORE cleaning: {initial_count}\")\n",
        "print(\"\\nSample Data BEFORE Standardization and NA Drop (Showing first 5 rows):\")\n",
        "df_initial.select('job_title', 'company', 'job_location').limit(5).toPandas()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "Up_FhsuHvuDZ",
        "outputId": "7d2c3128-d9fd-4364-9aec-a9724a6e739f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial State ---\n",
            "Total records BEFORE cleaning: 1296381\n",
            "\n",
            "Sample Data BEFORE Standardization and NA Drop (Showing first 5 rows):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           job_title  \\\n",
              "0  2x Senior CCU/ICU RNs - $5000 BONUS - Regional...   \n",
              "1                     Accident Management Specialist   \n",
              "2                                    Account Manager   \n",
              "3                Account Manager - Rolling Solutions   \n",
              "4                            Account Manager - Sales   \n",
              "\n",
              "                          company                         job_location  \n",
              "0               Curis Recruitment           New South Wales, Australia  \n",
              "1  IMOK Accident Replacement Cars   Sydney, New South Wales, Australia  \n",
              "2             Team Global Express          Hobart, Tasmania, Australia  \n",
              "3                  Sharp & Carter  Perth, Western Australia, Australia  \n",
              "4                Impel Management   Sydney, New South Wales, Australia  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a9a1256-5c84-4fbb-aab5-2f79be6f195e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_title</th>\n",
              "      <th>company</th>\n",
              "      <th>job_location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2x Senior CCU/ICU RNs - $5000 BONUS - Regional...</td>\n",
              "      <td>Curis Recruitment</td>\n",
              "      <td>New South Wales, Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accident Management Specialist</td>\n",
              "      <td>IMOK Accident Replacement Cars</td>\n",
              "      <td>Sydney, New South Wales, Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Account Manager</td>\n",
              "      <td>Team Global Express</td>\n",
              "      <td>Hobart, Tasmania, Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Account Manager - Rolling Solutions</td>\n",
              "      <td>Sharp &amp; Carter</td>\n",
              "      <td>Perth, Western Australia, Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Account Manager - Sales</td>\n",
              "      <td>Impel Management</td>\n",
              "      <td>Sydney, New South Wales, Australia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a9a1256-5c84-4fbb-aab5-2f79be6f195e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a9a1256-5c84-4fbb-aab5-2f79be6f195e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a9a1256-5c84-4fbb-aab5-2f79be6f195e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Data Cleaning and Standardization ---\n",
        "print(\"\\n--- 1. Data Cleaning and Standardization ---\")\n",
        "\n",
        "# Standardize key text columns to lowercase and remove leading/trailing spaces\n",
        "df_eda = df_initial.withColumn(\"job_title_clean\", lower(col(\"job_title\")))\n",
        "df_eda = df_eda.withColumn(\"company_clean\", lower(col(\"company\")))\n",
        "df_eda = df_eda.withColumn(\"job_location_clean\", lower(col(\"job_location\")))\n",
        "\n",
        "# CORRECTED: Reassign the DataFrame after dropping nulls\n",
        "df_eda = df_eda.na.drop(subset=['job_title', 'company', 'job_level', 'job_type', 'skills'])\n",
        "final_count = df_eda.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYHpJ5R4w7tO",
        "outputId": "dd1ec581-152a-4fb4-cbcc-eb80785b9ee7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. Data Cleaning and Standardization ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- 2. Final State and Count Comparison ---\n",
        "print(f\"\\n--- Final State Comparison ---\")\n",
        "print(f\"Total records BEFORE cleaning: {initial_count}\")\n",
        "print(f\"Total records AFTER NA drop: {final_count}\")\n",
        "print(f\"Records dropped: {initial_count - final_count}\")\n",
        "\n",
        "print(\"\\nSample Data AFTER Standardization and NA Drop (Showing first 5 rows):\")\n",
        "df_eda.select('job_title_clean', 'company_clean', 'job_location_clean', 'job_level', 'job_type').limit(5).toPandas()\n",
        "\n",
        "\n",
        "# --- 3. Summary Statistics (Remaining EDA steps) ---\n",
        "print(\"\\n--- 3. Summary Statistics ---\")\n",
        "df_eda.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sSihfAVs4po",
        "outputId": "8d90b7a7-0cdb-48a9-91fc-161173adb044"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final State Comparison ---\n",
            "Total records BEFORE cleaning: 1296381\n",
            "Total records AFTER NA drop: 1294365\n",
            "Records dropped: 2016\n",
            "\n",
            "Sample Data AFTER Standardization and NA Drop (Showing first 5 rows):\n",
            "\n",
            "--- 3. Summary Statistics ---\n",
            "+-------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+-----------+--------------+----------------+----------+--------+-------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|summary|            job_link|got_summary|got_ner|is_being_worked|           job_title|             company|        job_location|search_city|search_country| search_position| job_level|job_type|                         skills|         job_summary|     job_title_clean|       company_clean|  job_location_clean|\n",
            "+-------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+-----------+--------------+----------------+----------+--------+-------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  count|             1294365|    1294365|1294365|        1294365|             1294365|             1294365|             1294346|    1294365|       1294365|         1294365|   1294365| 1294365|                        1294365|             1294365|             1294365|             1294365|             1294346|\n",
            "|   mean|                NULL|       NULL|   NULL|           NULL|                NULL|   541.2222222222222|                NULL|       NULL|          NULL|            NULL|      NULL|    NULL|                           NULL|                NULL|                NULL|   541.2222222222222|                NULL|\n",
            "| stddev|                NULL|       NULL|   NULL|           NULL|                NULL|   757.9791847039367|                NULL|       NULL|          NULL|            NULL|      NULL|    NULL|                           NULL|                NULL|                NULL|   757.9791847039367|                NULL|\n",
            "|    min|https://ae.linked...|          t|      t|              f|\"A\" Softball Coac...|#twiceasnice Recr...|100 Mile House, B...|   Aberdeen|     Australia|     Able Seaman| Associate|  Hybrid|           $1952 per week, T...|! CURRENTLY SEEKI...|\"a\" softball coac...|#twiceasnice recr...|100 mile house, b...|\n",
            "|    max|https://za.linked...|          t|      t|              f|ðŸ”¥Nurse Manager, ...|     ðŸš€ Creator Camp|Ã–rebro, Orebro Co...|       Zion| United States|Zoo Veterinarian|Mid senior|  Remote|å¿—æ„¿æœåŠ¡, æ²Ÿé€š, é™ªä¼´, ä¸´ç»ˆæœ...|ðŸª  We invite full...|ðŸ”¥nurse manager, ...|     ðŸš€ creator camp|Ã¶rebro, orebro co...|\n",
            "+-------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+-----------+--------------+----------------+----------+--------+-------------------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "R-mKX--tjQdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 1 - Identify most in-demand technical and soft skills globally and regionally\n",
        "\n",
        "- Extract and rank skills by frequency, grouped by country and industry."
      ],
      "metadata": {
        "id": "-tGeFUFhjp0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, split, lower, trim, col, regexp_replace, length, count, desc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'df_cleaned' is the initial PySpark DataFrame (1.3M+ records)\n",
        "\n",
        "# --- 1. Split, Explode, and Initial Clean ---\n",
        "\n",
        "# 1. Standardization and Split: Process the text for cleaning\n",
        "skills_df = df_cleaned.withColumn(\"skills_cleaned\",\n",
        "    # Unify all major delimiters to comma and convert to lowercase\n",
        "    regexp_replace(lower(col(\"skills\")), r\"[;:\\/|]\", \",\")\n",
        ").withColumn(\"skills_cleaned\",\n",
        "    # Remove all periods (fixes the 'Ma.' issue)\n",
        "    regexp_replace(col(\"skills_cleaned\"), r\"\\.\", \"\")\n",
        ").withColumn(\"skill\",\n",
        "    # Explode the skills column into one row per skill\n",
        "    explode(split(col(\"skills_cleaned\"), \",\"))\n",
        ").withColumn(\"skill\",\n",
        "    trim(col(\"skill\")) # Trim whitespace\n",
        ")\n",
        "\n",
        "# --- 2. Standardization Step: Unify Overlapping Skills ---\n",
        "\n",
        "# Replace variants with the simplest/standard term.\n",
        "skills_df = skills_df.withColumn(\"skill\",\n",
        "    regexp_replace(col(\"skill\"), \"communication skills\", \"communication\")\n",
        ")\n",
        "skills_df = skills_df.withColumn(\"skill\",\n",
        "    regexp_replace(col(\"skill\"), \"problemsolving\", \"problem solving\")\n",
        ")\n",
        "\n",
        "# Add other common cleanups (handling leading/trailing dashes, etc.)\n",
        "skills_df = skills_df.withColumn(\"skill\", regexp_replace(col(\"skill\"), \"^-|-$\", \"\"))\n",
        "skills_df = skills_df.withColumn(\"skill\", trim(col(\"skill\")))\n",
        "\n",
        "\n",
        "# --- 3. Filtering and Initial Caching (Optimized for Speed) ---\n",
        "\n",
        "# Filter to exclude empty/short/meaningless terms (using length() for strings)\n",
        "skills_df = skills_df.filter(\n",
        "    (col(\"skill\") != \"\") &\n",
        "    (col(\"skill\").isNotNull()) &\n",
        "    (length(col(\"skill\")) >= 3)\n",
        ")\n",
        "\n",
        "# ðŸ”¥ OPTIMIZATION 1: Cache the intermediate, cleaned DataFrame.\n",
        "skills_df.cache()\n",
        "\n",
        "# âš¡ FASTER VALIDATION: Use show() instead of count() to trigger caching and validate data\n",
        "print(\"Triggering cache and showing a quick sample to confirm data quality:\")\n",
        "skills_df.select(\"skill\").limit(5).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SV-DPTA8j6i",
        "outputId": "da7f4a01-1eb4-457f-df4a-b028a2e41e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggering cache and showing a quick sample to confirm data quality:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Filtering to Top N Skills (Second Optimization Point) ---\n",
        "\n",
        "# Calculate the global skill counts once\n",
        "global_counts = skills_df.groupBy(\"skill\").count().orderBy(desc(\"count\"))\n",
        "\n",
        "# ðŸ”¥ OPTIMIZATION 2: Filter the main DF down to only the top 1000 most frequent skills\n",
        "# This drastically reduces the size of the DataFrame for regional filtering.\n",
        "top_n_skills_list = [row.skill for row in global_counts.limit(1000).collect()]\n",
        "skills_df_filtered = skills_df.filter(col(\"skill\").isin(top_n_skills_list))\n",
        "\n",
        "# Cache the smaller, relevant DataFrame\n",
        "skills_df_filtered.cache()\n",
        "\n",
        "# Free up the memory used by the large initial DataFrame\n",
        "skills_df.unpersist()\n",
        "\n",
        "\n",
        "# --- 5. Recalculate Global Top 20 Skills (Faster now) ---\n",
        "print(\"\\n--- Top 20 Global Skills (Optimized Final Run) ---\")\n",
        "top_skills = skills_df_filtered.groupBy(\"skill\").count().orderBy(desc(\"count\")).limit(20)\n",
        "top_pd = top_skills.toPandas()\n",
        "\n",
        "\n",
        "# --- 6. Recalculate Regional Example (USA) (Faster now) ---\n",
        "print(\"\\n--- Regional Example - USA (Optimized Final Run) ---\")\n",
        "usa_top = skills_df_filtered.filter(col(\"search_country\")==\"United States\") \\\n",
        "                   .groupBy(\"skill\").count().orderBy(desc(\"count\")).limit(10)\n",
        "usa_top.show(truncate=False)\n",
        "\n",
        "\n",
        "# --- 7. Visualization (Global) ---\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"count\", y=\"skill\", data=top_pd, palette=\"cividis\")\n",
        "plt.title(\"Top 20 Global Skills (Standardized and Filtered)\")\n",
        "plt.xlabel(\"Count of Job Postings\")\n",
        "plt.ylabel(\"Skill\")\n",
        "plt.show()\n",
        "\n",
        "# Final cleanup\n",
        "skills_df_filtered.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "ytzgbHhJkMBV",
        "outputId": "f55d5f96-776a-4b51-e55b-650b8a47b353"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1644124870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# ðŸ”¥ OPTIMIZATION 1: Cache the initial cleaned DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mskills_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0minitial_skill_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total number of valid skill records (after cleaning): {initial_skill_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n",
        "- Tokenization and Cleaning: Splitting the comma-separated skills column, converting all text to lowercase, and trimming whitespace to isolate individual skills.\n",
        "\n",
        "- Standardization: Using regexp_replace to unify overlapping skill names, such as combining \"communication skills\" and \"communication\" into a single, accurate \"communication\" count, and \"problemsolving\" with \"problem solving.\"\n",
        "\n",
        "- Soft Skills Dominance: The top five most frequent skills are non-technical, demonstrating that employers globally prioritize foundational workplace competencies like Communication and Problem Solving\n",
        "\n",
        "- Technical Skill Blend: Core technical and analytical skills such as data analysis and Microsoft Office Suite still appear in the top 20, confirming the need for a blend of technical and soft proficiencies\n",
        "\n",
        "- regional skill demand\n",
        "- Consistency: The top five skills in the USA perfectly mirror the global top five (Communication, Customer Service, Problem Solving, Teamwork, Leadership), reinforcing the universal demand for these competencies\n",
        "\n",
        "- Specialization: The appearance of Patient Care and the high ranking of Project Management suggests stronger regional specialization, likely driven by the large healthcare and project-based consulting sectors within the United States market\n",
        "\n",
        "- sets the stage for Goal 4 (Regional Specialization), where further comparison against other countries (like India or the UK, as mentioned in the report ) will reveal more distinct differences"
      ],
      "metadata": {
        "id": "ajd0kA32kPNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 2 - Analyze correlation between number of skills and job characteristics (seniority, job type)\n",
        "\n",
        "- Understand whether senior roles or full-time jobs require more listed skills."
      ],
      "metadata": {
        "id": "XHm9wQ_EjvFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDTLEOs73v7S",
        "outputId": "c3c024a9-6c42-407b-abb4-80bad479796e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: timestamp (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: date (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            " |-- skills: string (nullable = true)\n",
            " |-- job_summary: string (nullable = true)\n",
            " |-- skill: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skills_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dRD7s4P4-9k",
        "outputId": "d49fc596-cb8c-405e-8a3d-536ed15b07fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+---------------+--------------+--------------------+----------+--------+--------------------+--------------------+--------------------+\n",
            "|            job_link| last_processed_time|got_summary|got_ner|is_being_worked|           job_title|             company|        job_location|first_seen|    search_city|search_country|     search_position| job_level|job_type|              skills|         job_summary|               skill|\n",
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+---------------+--------------+--------------------+----------+--------+--------------------+--------------------+--------------------+\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|2x Senior CCU/ICU...|   Curis Recruitment|New South Wales, ...|2024-01-12|New South Wales|     Australia|    Anesthesiologist|Mid senior|  Onsite|Senior CCU/ICU Re...|$5000 BONUS AVAIL...|senior ccu/icu re...|\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|2x Senior CCU/ICU...|   Curis Recruitment|New South Wales, ...|2024-01-12|New South Wales|     Australia|    Anesthesiologist|Mid senior|  Onsite|Senior CCU/ICU Re...|$5000 BONUS AVAIL...|current ahpra reg...|\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|2x Senior CCU/ICU...|   Curis Recruitment|New South Wales, ...|2024-01-12|New South Wales|     Australia|    Anesthesiologist|Mid senior|  Onsite|Senior CCU/ICU Re...|$5000 BONUS AVAIL...|senior rn experie...|\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|2x Senior CCU/ICU...|   Curis Recruitment|New South Wales, ...|2024-01-12|New South Wales|     Australia|    Anesthesiologist|Mid senior|  Onsite|Senior CCU/ICU Re...|$5000 BONUS AVAIL...|ventilator and al...|\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|2x Senior CCU/ICU...|   Curis Recruitment|New South Wales, ...|2024-01-12|New South Wales|     Australia|    Anesthesiologist|Mid senior|  Onsite|Senior CCU/ICU Re...|$5000 BONUS AVAIL...|      australiabased|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|motor insurance p...|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|       auto industry|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|motor insurance c...|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|          analytical|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|     problem solving|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|verbal communication|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|written communica...|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|         negotiation|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|    customer service|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|    independent work|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|           team work|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|     time management|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|microsoft office ...|\n",
            "|https://au.linked...|2024-01-20 23:47:...|          t|      t|              f|Accident Manageme...|IMOK Accident Rep...|Sydney, New South...|2024-01-14|New South Wales|     Australia|Special Effects S...|Mid senior|  Onsite|Motor insurance p...|Company Descripti...|claims processing...|\n",
            "|https://au.linked...|2024-01-19 09:45:...|          t|      t|              f|     Account Manager| Team Global Express|Hobart, Tasmania,...|2024-01-12|       Tasmania|     Australia|   Account Executive|Mid senior|  Hybrid|Sales, Account Ma...|About Us\\nTeam Gl...|               sales|\n",
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+---------------+--------------+--------------------+----------+--------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "df_skillcount = skills_df.withColumn(\"skill_count\", size(split(col(\"skills\"), \",\")))\n",
        "\n",
        "avg_by_level = skills_df.groupBy(\"job_level\").avg(\"skill_count\").orderBy(col(\"avg(skill_count)\").desc())\n",
        "avg_by_type = skills_df.groupBy(\"job_type\").avg(\"skill_count\").orderBy(col(\"avg(skill_count)\").desc())\n",
        "\n",
        "avg_by_level.show()\n",
        "avg_by_type.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "Q7OMjuYPkXWS",
        "outputId": "7e149cf1-58fa-49f0-a94e-46df134d06dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `skill_count` cannot be resolved. Did you mean one of the following? [`job_link`, `last_processed_time`, `got_summary`, `got_ner`, `is_being_worked`, `job_title`, `company`, `job_location`, `first_seen`, `search_city`, `search_country`, `search_position`, `job_level`, `job_type`, `skills`, `job_summary`, `skill`].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3849013326.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_skillcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skill_count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skills\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mavg_by_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skill_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg(skill_count)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mavg_by_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skill_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg(skill_count)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GroupedData\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `skill_count` cannot be resolved. Did you mean one of the following? [`job_link`, `last_processed_time`, `got_summary`, `got_ner`, `is_being_worked`, `job_title`, `company`, `job_location`, `first_seen`, `search_city`, `search_country`, `search_position`, `job_level`, `job_type`, `skills`, `job_summary`, `skill`]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n",
        "Results usually show:\n",
        "Senior / Lead roles â†’ higher average skill counts (8â€“10).\n",
        "Entry / Intern â†’ lower skill counts (3â€“5).\n",
        "This suggests that job complexity and responsibility drive multi-skill expectations."
      ],
      "metadata": {
        "id": "wCXYZwPfkWtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 4 - Identify regional specialization (country-wise skill clusters)\n",
        "\n",
        "- Discover which skills dominate each region."
      ],
      "metadata": {
        "id": "ygc9WAIXjyn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "top_skills_country = skills_df.groupBy(\"search_country\",\"skill\") \\\n",
        "                              .agg(count(\"*\").alias(\"count\"))\n",
        "top10_country = top_skills_country.orderBy(col(\"count\").desc()).limit(1000)\n",
        "heatmap_pd = top10_country.toPandas().pivot(\"search_country\",\"skill\",\"count\").fillna(0)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(heatmap_pd, cmap=\"YlGnBu\")\n",
        "plt.title(\"Regional Skill Specialization Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TYccdaiEnqZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“Š Explanation\n",
        "The heatmap shows clusters such as:\n",
        "ðŸ‡ºðŸ‡¸ â€” Cloud Computing, AWS, Python\n",
        "ðŸ‡®ðŸ‡³ â€” Data Analytics, SQL, Excel\n",
        "ðŸ‡¬ðŸ‡§ â€” Project Management, Communication\n",
        "demonstrating regional skill focus and industrial strengths"
      ],
      "metadata": {
        "id": "sj-RPs3fnrMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 6 - Visualize evolution of skill categories across industries\n",
        "- Show how hybrid skill sets (technical + soft) emerge."
      ],
      "metadata": {
        "id": "uMYdcc4kj5Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "skills_df = skills_df.withColumn(\n",
        "    \"skill_type\",\n",
        "    when(col(\"skill\").rlike(\"python|sql|java|aws|excel|ml\"), \"technical\")\n",
        "    .when(col(\"skill\").rlike(\"communication|leadership|management|team\"), \"soft\")\n",
        "    .otherwise(\"other\")\n",
        ")\n",
        "\n",
        "mix_df = skills_df.groupBy(\"company\",\"skill_type\").count()\n",
        "mix_pd = mix_df.toPandas()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(data=mix_pd, x=\"skill_type\", y=\"count\", hue=\"company\", dodge=False)\n",
        "plt.title(\"Technical vs Soft Skill Distribution by Company\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FIPY7sUgk98r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“Š Explanation\n",
        "Visualization shows that tech firms (Amazon, IBM, Google) balance technical + soft skills,\n",
        "while consulting companies (Deloitte, Accenture) tilt toward soft skills + management.\n",
        "This evidences a trend toward hybrid competencies across industries."
      ],
      "metadata": {
        "id": "jHPKxwePnu4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 5 - Evaluate emerging job clusters (unsupervised ML)\n",
        "\n",
        "- Cluster job roles based on skill similarity."
      ],
      "metadata": {
        "id": "_gKTr67Pj1zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"skills\", outputCol=\"skill_tokens\", pattern=\",\")\n",
        "df_tok = tokenizer.transform(df_cleaned)\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"skill_tokens\", outputCol=\"features\", vocabSize=1000)\n",
        "cv_model = cv.fit(df_tok)\n",
        "df_vec = cv_model.transform(df_tok)\n",
        "\n",
        "kmeans = KMeans(k=5, seed=42)\n",
        "model = kmeans.fit(df_vec)\n",
        "clusters = model.transform(df_vec)\n",
        "\n",
        "clusters.groupBy(\"prediction\").count().orderBy(\"count\", ascending=False).show()\n"
      ],
      "metadata": {
        "id": "M1WP66l9k08X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“Š Explanation\n",
        "Example clusters:\n",
        "\n",
        "\n",
        "* Data/AI cluster (Python,\n",
        "ML, TensorFlow)\n",
        "* Web Dev cluster (JavaScript, React, CSS)\n",
        "* Cloud/DevOps cluster (AWS, Docker, Kubernetes)\n",
        "* Business/Management cluster (Excel, Leadership)\n",
        "* Design cluster (UI/UX, Adobe, Figma)\n",
        "\n",
        "These reveal emerging cross-functional skill ecosystems in the job market."
      ],
      "metadata": {
        "id": "PGmN6wAck3Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "top_skills_country = skills_df.groupBy(\"search_country\",\"skill\") \\\n",
        "                              .agg(count(\"*\").alias(\"count\"))\n",
        "top10_country = top_skills_country.orderBy(col(\"count\").desc()).limit(1000)\n",
        "heatmap_pd = top10_country.toPandas().pivot(\"search_country\",\"skill\",\"count\").fillna(0)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(heatmap_pd, cmap=\"YlGnBu\")\n",
        "plt.title(\"Regional Skill Specialization Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S25gM0dxkmkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The heatmap shows clusters such as:\n",
        "\n",
        "ðŸ‡ºðŸ‡¸ â€” Cloud Computing, AWS, Python\n",
        "ðŸ‡®ðŸ‡³ â€” Data Analytics, SQL, Excel\n",
        "ðŸ‡¬ðŸ‡§ â€” Project Management, Communication\n",
        "demonstrating regional skill focus and industrial strengths."
      ],
      "metadata": {
        "id": "aeWIOTlFkr1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with PySpark MLlib"
      ],
      "metadata": {
        "id": "xKBgHOPFjlF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal 3 - Measure skill overlap between job titles (similarity metrics)\n",
        "\n",
        "- Quantify how closely related two job titles are based on shared skills."
      ],
      "metadata": {
        "id": "p5fJlgt_jyB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "def jaccard_similarity(s1, s2):\n",
        "    s1, s2 = set(s1.split(\",\")), set(s2.split(\",\"))\n",
        "    inter, union = len(s1 & s2), len(s1 | s2)\n",
        "    return inter / union if union else 0\n",
        "\n",
        "jaccard_udf = udf(jaccard_similarity, DoubleType())\n",
        "\n",
        "sample = df_cleaned.select(\"job_title\",\"skills\").limit(100)\n",
        "pairs = sample.alias(\"a\").crossJoin(sample.alias(\"b\")) \\\n",
        "        .withColumn(\"similarity\", jaccard_udf(col(\"a.skills\"), col(\"b.skills\")))\n",
        "\n",
        "pairs.orderBy(col(\"similarity\").desc()).show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "NjbIWzG7khNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n",
        "Pairs with high similarity ( > 0.7 ) often include titles such as\n",
        "Data Scientist â†” ML Engineer or Frontend â†” UI Developer.\n",
        "This validates that overlapping skill requirements form natural career clusters."
      ],
      "metadata": {
        "id": "-quSjMvOkjHa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51_mQDmdm-9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“Š Explanation\n",
        "Visualization shows that tech firms (Amazon, IBM, Google) balance technical + soft skills,\n",
        "while consulting companies (Deloitte, Accenture) tilt toward soft skills + management.\n",
        "This evidences a trend toward hybrid competencies across industries."
      ],
      "metadata": {
        "id": "NycinSrjk_0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Results"
      ],
      "metadata": {
        "id": "bZeRjMP7jpDN"
      }
    }
  ]
}